{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion- Documentloaders\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/introduction/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x107f1f4d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Woodrow Wilson Speech\\nThe world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document = loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDf File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('attention.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4 as bs\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs = dict(parse_only=bs.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     ))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"2402.00888\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-11-14', 'Title': 'Security and Privacy Challenges of Large Language Models: A Survey', 'Authors': 'Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu', 'Summary': 'Large Language Models (LLMs) have demonstrated extraordinary capabilities and\\ncontributed to multiple fields, such as generating and summarizing text,\\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\\npopular tool in computerized language processing tasks, with the capability to\\nanalyze complicated linguistic patterns and provide relevant and appropriate\\nresponses depending on the context. While offering significant advantages,\\nthese models are also vulnerable to security and privacy attacks, such as\\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\\nInformation (PII) leakage attacks. This survey provides a thorough review of\\nthe security and privacy challenges of LLMs for both training data and users,\\nalong with the application-based risks in various domains, such as\\ntransportation, education, and healthcare. We assess the extent of LLM\\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\\nand review the potential defense mechanisms. Additionally, the survey outlines\\nexisting research gaps in this domain and highlights future research\\ndirections.'}, page_content='Security and Privacy Challenges of Large Language Models:\\nA Survey\\nBADHAN CHANDRA DAS, Knight Foundation School of Computing and Information Sciences; Sustain-\\nability, Optimization, and Learning for InterDependent networks laboratory (solid lab), Florida International\\nUniversity, United States\\nM. HADI AMINI∗, Knight Foundation School of Computing and Information Science, solid lab, Florida\\nInternational University, United States\\nYANZHAO WU∗, Knight Foundation School of Computing and Information Sciences, Florida International\\nUniversity, United States\\nLarge language models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields,\\nsuch as generating and summarizing text, language translation, and question-answering. Nowadays, LLMs\\nhave become very popular tool in natural language processing (NLP) tasks, with the capability to analyze\\ncomplicated linguistic patterns and provide relevant and appropriate responses depending on the context.\\nWhile offering significant advantages, these models are also vulnerable to security and privacy attacks, such\\nas jailbreaking attacks, data poisoning attacks, and personally identifiable information (PII) leakage attacks.\\nThis survey provides a thorough review of the security and privacy challenges of LLMs, along with the\\napplication-based risks in various domains, such as transportation, education, and healthcare. We assess the\\nextent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the\\npotential defense mechanisms. Additionally, the survey outlines existing research gaps in this research area\\nand highlights future research directions.\\nCCS Concepts: • General and reference →Surveys and overviews; • Information systems →Language\\nmodels; • Security and privacy →Privacy-preserving protocols.\\nAdditional Key Words and Phrases: Large Language Models, Security and Privacy Challenges, Defense\\nMechanisms.\\nACM Reference Format:\\nBadhan Chandra Das, M. Hadi Amini, and Yanzhao Wu. 2024. Security and Privacy Challenges of Large\\nLanguage Models: A Survey. J. ACM 37, 4, Article 111 (August 2024), 51 pages. https://doi.org/XXXXXXX.\\nXXXXXXX\\n1\\nINTRODUCTION\\nThe exploration of intelligence and the feasibility of machines with cognitive abilities is a compelling\\npursuit in the scientific community. Intelligent devices equip us with the capacity for logical\\nreasoning, experimental inquiry, and foresight into future developments. In the Artificial Intelligence\\n∗Corresponding authors.\\nAuthors’ addresses: Badhan Chandra Das, Knight Foundation School of Computing and Information Sciences; Sustain-\\nability, Optimization, and Learning for InterDependent networks laboratory (solid lab), Florida International University,\\nMiami, Florida, United States; M. Hadi Amini, Knight Foundation School of Computing and Information Science, solid lab,\\nFlorida International University, Miami, Florida, United States; Yanzhao Wu, Knight Foundation School of Computing and\\nInformation Sciences, Florida International University, Miami, Florida, United States, Emails:{moamini,yawu}@fiu.edu.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n© 2024 Copyright held by the owner/author(s).\\nACM 0004-5411/2024/8-ART111\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\narXiv:2402.00888v2  [cs.CL]  14 Nov 2024\\n111:2\\nDas, et al.\\n(AI) domain, researchers are diligently striving to advance methodologies for the construction of\\nintelligent machines. One of the latest advancements of AI is LLM. LLMs have become popular in\\nboth the academic and industrial sectors. As researchers demonstrate, these models are impressively\\neffective and achieve nearly human-like performance in certain tasks [332]. Consequently, there is\\ngrowing interest in exploring whether they might represent an early form of Artificial General\\nIntelligence (AGI). Unlike earlier Language Models (LMs), which were limited to specific tasks, such\\nas classification and next-word prediction, LLMs can solve a broader range of problems, including\\nbut not limited to large text generation, summarizing text, logical and mathematical reasoning, and\\ncode generation. They are highly capable of handling various tasks, from daily use of language for\\ncommunication to more specific challenges [41], [111], [113], [206].\\nAcronym\\nFull Form\\nAI\\nArtificial Intelligence\\nAGI\\nArtificial General Intelligence\\nALBERT\\nA Lite BERT\\nBERT\\nBidirectional Encoder Representations from Transformers\\nBGMAttack\\nBlack-box Generative Model-based Attack\\nCBA\\nComposite Backdoor Attack\\nCCPA\\nCalifornia Consumer Privacy Act\\nDAN\\nDo Anything Now\\nDNN\\nDeep Neural Network\\nDP\\nDifferential Privacy\\nFL\\nFederated Learning\\nGDPR\\nGeneral Data Protection Regulation\\nGA\\nGenetic Algorithm\\nGPT\\nGenerative Pre-trained Transformer\\nHIPAA\\nHealth Insurance Portability and Accountability Act\\nLM\\nLanguage Model\\nLLM\\nLarge Language Model\\nLlama\\nLarge Language Model Meta AI\\nMIA\\nMembership Inference Attack\\nMDP\\nMasking-Differential Prompting\\nMLM\\nMasked Language Model\\nNLP\\nNatural Language Processing\\nOOD\\nOut Of Distribution\\nPI\\nPrompt Injection\\nPII\\nPersonally Identifiable Information\\nPAIR\\nPrompt Automatic Iterative Refinement\\nPLM\\npre-trained Language Model\\nRL\\nReinforcement Learning\\nRLHF\\nReinforcement Learning from Human Feedback\\nRoBERTa\\nRobustly optimized BERT approach\\nSGD\\nStochastic Gradient Descent\\nTAG\\nGradient Attack on Transformer-based Language Models\\nXLNet\\nTransformer-XL with autoregressive and autoencoding pre-training\\nTable 1. List of Common Acronyms\\nAlso, with proper prompt engineer-\\ning [278] and in-context learning ca-\\npabilities [123], LLMs can adapt to\\ndifferent contexts and/or even accom-\\nplish new tasks without training or\\nfine-tuning. The introduction of Chat-\\nGPT [186] and GPT-4 [4] took these\\nadvancements to another level. How-\\never, these highly efficient LLMs are\\nnot flawless. The vulnerabilities of\\nthese LLMs have not been explored\\nthat much on a large scale from secu-\\nrity and privacy perspective. It is im-\\nperative to conduct an in-depth study\\nto identify these vulnerabilities. In\\nthis paper, we comprehensively illus-\\ntrate the security and privacy issues\\nin LLMs as well as their defense mech-\\nanisms. We also discuss the research\\nchallenges in the LLM context along\\nwith future research opportunities.\\nThroughout the paper, there are\\nmany acronyms used to represent\\nconcepts, types of attacks, models\\ncommon in privacy and security, and\\nLLM research very frequently. Table\\n1 is provided for the most common\\nand important terms we used in the\\npaper.\\n1.1\\nMotivation\\nThe increasing sizes of LMs, such as LLMs, require a huge amount of data from the Internet\\nin addition to meticulously annotated textual data for training/fine-tuning to enhance models’\\npredictive performance. In contrast to carefully created annotated data, the freely available texts\\nfrom the Internet may exhibit poor data quality and unintended leakage of private personal\\ninformation [131]. For instance, casual interactions with these models may accidentally leak PII,\\nas highlighted in [23] and [132], which may violate existing privacy laws, such as The “Health\\nInsurance Portability and Accountability Act of 1996 (HIPAA)” in the United States [36], the EU’s\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:3\\n“General Data Protection Regulation (GDPR)” [261], and the “California Consumer Privacy Act\\n(CCPA)” [17].\\nFollowing the launch of ChatGPT [186] and GPT-4 [4], numerous research initiatives have focused\\non assessing them across various dimensions. These evaluations considered various aspects of NLP\\ntasks, such as correctness, robustness, rationality, reliability, and notably, the identification and\\nevaluation of vulnerabilities related to privacy risks and security issues. The assessment of LLMs is\\nof paramount importance for several reasons. First, it will contribute to an in-depth understanding\\nof the strengths and weaknesses of LLMs by studying their security and privacy issues. Second, a\\ncomprehensive evaluation of privacy and security vulnerabilities in LLMs will potentially inspire\\nefforts and advancements toward secure and privacy-preserving human-LLM interactions. Third,\\nthe widespread use of LLMs highlights the significance of assuring their reliability and security,\\nparticularly in sectors prioritizing safety and privacy protection, such as financial organizations\\nand the healthcare system. Last but not least, as LLMs continue to expand in size and acquire\\nnew capabilities, the existing protocols may prove inadequate in assessing their complete range of\\ncapabilities and potential privacy risks and security issues. Our objective is to provide a clear vision\\nfor researchers, practitioners, and other stakeholders who plan to develop and/or deploy LLMs\\nregarding the significance of LLM security and privacy challenges. This involves reviewing existing\\nstudies in the broad area of security, privacy, and their intersections, and notably, highlighting\\nfuture research directions to design novel evaluation protocols and attack methods, as well as\\ndefense mechanisms tailored to the evolving landscape of LLMs.\\n1.2\\nOur Contributions\\nThis paper analyzes the latest developments in privacy and security concerns and defense mecha-\\nnisms of LLMs. Comparing with recent survey papers and empirical studies on this topic as shown\\nin Table 2, we present a comprehensive discussion and systematic analysis of representative privacy\\nand security issues, defense mechanisms, and future research directions for LLMs. In contrast\\nto the prior surveys, we investigated the most recent advancements in the security and privacy\\ndomain for LLMs, providing a timely and highly relevant review of this emerging research area.\\nFurthermore, our study analyzed novel approaches and techniques that emerged in this domain\\nand the current research gaps. After analyzing the effectiveness and limitations of representative\\nattacks and defenses, we offer insights into future research directions on unexplored security and\\nprivacy challenges and potential attack mitigation strategies.\\nApplication-\\nbased risks\\n• Misinformation \\nand \\nHallucination\\n• Cybercrimes \\nand social \\nissues.\\n• Transportation\\n• Healthcare and \\nmedicine.\\n• Education.\\n• Governance.\\n• Science.\\nBackground\\nIntroduction\\nVulnerabilities\\nArchitecture\\n• Brief overvi-\\new of LLM \\ndevelopment, \\ntraining, fine-\\ntuning, and the \\nbasic \\nworkflow.\\n• Components \\ncontributing to \\nvulnerability-\\nes in LLM \\nsystem. \\n• Nature of the \\nLLM vulnerabil-\\nities from two \\nbroad perspective, \\ni.e., security and \\nprivacy.\\n• Subcategories of \\nsecurity and \\nprivacy.\\n• Characteristics \\noverlapping of \\nsecurity and \\nprivacy attacks.\\n• The Evolve of \\nLLMs and their \\nreal-world usage.\\n• LLM vulnerab-\\nilities and the risk \\nfactors from priv-\\nacy and security \\nperspective.\\n• The motivation of \\nthis study.\\n• Our contributions \\nw.r.t. previous \\nstudies.\\nVulnerabilities\\nSecurity attacks\\nPrivacy attacks\\n• Fundamental goals \\nand natures of \\nsecurity attacks.\\n• Categories of \\nLM/LLM security \\nattacks  with \\ncomprehensive \\ndiscussion, includ-\\ning examples, im-\\npact.\\n• Limitation and \\npotential defense.\\n• Basic goals and \\ncharacteristics of \\nprivacy attacks.\\n• Categories of \\nLM/LLM \\nsecurity attacks \\nwith \\ncomprehensive \\ndiscussion, inclu-\\nding examples, \\nimpact\\n• Limitation and \\npotential defense.\\nDefense\\nSecurity attacks\\nPrivacy attacks\\n• Baseline ideas to \\ndefend the \\nsecurity attacks in \\nML.\\n• Potential defense \\nmethods about \\nsecuring \\nLMs/LLMs from \\nsecurity attacks of \\ndifferent category.\\n• Limitations of \\nthose defense \\nmethods in LLMs.\\n• Basic principles \\nof protect \\nprivacy in ML.\\n• Potential \\nmitigations \\nagainst the \\nprevalent \\nprivacy attacks \\nin LMs/LLMs.\\n• Limitations of \\nthe prevalent \\nmitigations in \\nLLM.\\nLimitation and \\nfuture research\\nLimitation\\nFuture research\\n• Discussion on \\nlimitation of the \\nexisting attack \\nand defense \\nmethods from \\nboth security and \\nprivacy perspec-\\ntive.\\n• Challenges of \\nintegrating the \\nprevalent defense \\nmethods for the \\nadvanced attacks\\n• Future \\nevolutions of \\nsecurity and \\nprivacy attacks.\\n• Light on the \\npotential and \\nunexplored \\nareas of attacks \\nand defense for \\nLLMs.\\nFig. 1. Overview of the paper\\n1.3\\nOrganization\\nWe present an overview of this survey paper in Figure 1. The rest of this paper is organized as follows.\\nSection 2 illustrates an overview of the LLM architecture and the components that contribute to the\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:4\\nDas, et al.\\nAuthors\\nHighlights\\nResearch\\nType\\nGeneral\\nPurpose\\nPrivacy\\nIssus\\nJA\\nPI\\nBA\\nDPa\\nGLa\\nMIA\\nPII-\\nLeak\\nAR\\n1-1\\nDAA\\nAFRD\\nPD\\nNeel et al.\\n[182]\\nLLM\\nprivacy threats\\nSurvey\\n∗\\n×\\n×\\n×\\n×\\n×\\n∗∗∗\\n∗∗∗\\n×\\n×\\n×\\nDec. 2023\\nWu et al.\\n[287]\\nApplication-based\\nprivacy threats\\nSurvey\\n∗∗∗\\n×\\n×\\n∗∗\\n∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\nMar. 2024\\nIsabel et al.\\n[73]\\nBias and fairness\\nSurvey\\n∗∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nMar. 2024\\nGupta et al.\\n[83]\\nGenerative AI’s\\nimpacts on cybersecurity\\nand privacy\\nSurvey\\n∗∗∗\\n∗∗\\n∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\n∗\\n∗\\nJul. 2023\\nLiu et al.\\n[147]\\nOverview of various\\njailbreaking attacks\\nSurvey\\n×\\n∗∗∗\\n∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n∗\\nMay 2024\\nDeng et al.\\n[54]\\nAutomated jailbreak\\nacross multiple LLMs\\nEmpirical\\n×\\n∗∗∗\\n∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n∗\\nOct. 2023\\nYang et al.\\n[300]\\nVarious backdoor attacks\\nin LLMs within\\ncommunication networks\\nSurvey\\n×\\n×\\n∗\\n∗∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\n*\\nOct. 2023\\nShi et al.\\n[235]\\nSecurity vulnerabilities\\nof ChatGPT\\nEmpirical\\n×\\n×\\n×\\n∗∗∗\\n∗\\n×\\n×\\n×\\n×\\n∗\\n×\\nFeb 2023\\nWan et al.\\n[263]\\nPoisoning datasets,\\nallowing them to\\nmanipulate model\\nEmpirical\\n×\\n×\\n×\\n×\\n∗∗∗\\n×\\n×\\n×\\n×\\n×\\n∗\\nMay 2023\\nXin et al.\\n[294]\\nMembership leakage\\nexposing pre-trained LMs.\\nEmpirical\\n×\\n×\\n×\\n×\\n×\\n×\\n∗∗∗\\n×\\n×\\n∗\\n×\\nAug 2024\\nJieren et al.\\n[55]\\nFormulated gradient\\nattack on the\\nTransformer-based LMs\\nEmpirical\\n×\\n×\\n×\\n×\\n×\\n∗∗∗\\n×\\n×\\n×\\n×\\n×\\nSep 2021\\nLukas et al.\\n[152]\\nMeasuring PII leakage\\nfrom the training data\\non different-sized LMs\\nSurvey +\\nEmpirical\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n∗∗∗\\n×\\n×\\n∗∗\\nApr 2023\\nCarlini et al.\\n[23]\\nFocused on memorization\\nof training samples\\nresponsible for\\nprobing attacks\\nEmpirical\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n∗∗∗\\n×\\n∗∗\\n∗∗\\nAug 2021\\nRobey et al.\\n[215]\\nSmoothLLM: Defense\\nmethod for LLM\\nfrom jailbreaking\\nEmpirical\\n×\\n∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n∗∗∗\\n∗∗∗\\nJun 2024\\nVassilev et al.\\n[258]\\nTaxonomy and\\nterminology of adversarial\\nmachine learning\\nSurvey\\n∗\\n∗\\n∗\\n∗\\n∗\\n×\\n∗\\n×\\n×\\n×\\n×\\nMarch 2023\\nSun et al.\\n[248]\\nTrustworthiness\\nin LLM\\nSurvey +\\nEmpirical\\n∗∗∗\\n∗∗\\n×\\n∗\\n∗\\n×\\n×\\n∗\\n×\\n∗\\n∗∗\\nSep 2024\\nZhao et al.\\n[323]\\nResource utilization and\\nCapacity Evaluation\\nof LLMs\\nSurvey\\n∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nOct 2024\\nMinaee et al.\\n[171]\\nLLM development\\nand Applications\\nSurvey\\n∗∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\nFeb 2024\\nNaveed et al.\\n[181]\\nEvolution and\\nworkflow in LLM\\nSurvey\\n∗∗∗\\n∗\\n×\\n×\\n×\\n×\\n×\\n×\\n×\\n∗\\n×\\nOct 2024\\nShayegani et al.\\n[232]\\nAdversarial attacks\\nin LLM\\nSurvey\\n×\\n∗∗\\n∗∗\\n×\\n∗∗\\n∗∗\\n∗\\n×\\n×\\n∗∗\\n∗∗\\nOct 2023\\nWang et al.\\n[267]\\nUnique security and\\nprivacy challenge\\nin LLMs\\nSurvey\\n×\\n∗\\n∗∗\\n∗∗∗\\n∗∗∗\\n∗∗\\n∗\\n×\\n×\\n∗∗∗\\n∗\\nJun 2024\\nYao et al.\\n[306]\\nPrivacy and security\\nin LLM\\nSurvey\\n∗∗∗\\n∗\\n∗\\n∗\\n∗\\n∗∗\\n∗∗\\n×\\n×\\n×\\n∗∗∗\\nMar 2024\\nOur\\nContribution\\nComprehensive review\\nof security attacks,\\nprivacy risks,challenges,\\nmitigation and\\nfuture research directions\\nSurvey\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\n∗∗∗\\nCurrent\\nTable 2. Comparison of the Existing Surveys and Research Works on LLM Vulnerabilities with our paper. The\\nacronyms stands as JA: Jailbreaking Attack, PI: Prompt Injection, BA: Backdoor Attack, DPa: Data Poisoning\\nAttack, GLa: Gradient Leakage Attack, MIA: Membership Inference Attack, PII-Leak: Personal Identifiable\\nInformation Leakage Attacks, PD: Publication Date, 1-1 DAA: one-to-one Defense Against Attacks, AFRD:\\nAnalysis and Future Research Direction. We define the extent of discussion by notations as: No Discussion(×),\\nSlight Discussion(∗), Moderate Discussion(∗∗), Extensive Discussion(∗∗∗)\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:5\\nvulnerabilities. Section 3 briefly describes different categories of LLM vulnerabilities and potential\\nmitigation techniques. Sections 4 and 5 comprehensively discuss LLMs’ security and privacy attacks,\\nrespectively, with their limitations. The potential mitigation techniques for different types of attacks\\nare discussed in Section 6. We introduce several application-specific risks of LLMs in Section 7. The\\nlimitations of existing research and future challenges are discussed in Section 8. Finally, Section 9\\nconcludes the paper.\\n2\\nLLM ARCHITECTURE COMPONENTS CONTRIBUTING TO VULNERABILITIES\\nLLMs [32], [71] are characterized by extensive parameter sizes and intelligent learning capabilities.\\nThe model is pre-trained with a large dataset containing public Internet data, books, and various texts\\nto learn the underlying structures, patterns, and contextual relationships within language. This pre-\\ntraining phase equips the model with a broad understanding of syntax, semantics, and knowledge.\\nAfter pre-training, the model undergoes a fine-tuning process for specific tasks or domains to\\nenhance its performance for targeted applications. During training, the input text undergoes\\ntokenization and is then fed into the model. After that, the model processes the input text through\\ndeep neural networks (DNNs) with the attention mechanisms [259]. The model then generates\\noutput, e.g., next-word prediction or generating the sequence of words based on the probability\\ndistributions of context provided by the input. The output tokens keep generating until a stopping\\ncriterion is met. It is a powerful tool for performing various tasks like text generation, language\\ntranslation, summarizing, and question answering, leveraging their learned representations to\\nproduce coherent and contextually relevant text. The foundational component, shared by numerous\\nLLMs, including GPT-3 [68], InstructGPT [187], and GPT-4 [4], is a self-attention module present in\\nthe Transformer architecture. This module plays a major role in the landscape of NLP by efficiently\\nmanaging sequential data, facilitating parallelization, and capturing long-range dependencies in\\ntext data. In-context learning is a major feature of LLMs, wherein the model can learn from a given\\ncontext or prompt to generate text. This capability empowers LLMs to produce responses that are not\\nonly more coherent but also contextually relevant, rendering them well-suited for interactive and\\nconversational applications, such as chatbots. LLMs are also empowered with few-shot learning [20].\\nLLMs are trained over a vast amount of data, however, they might still lack unforeseen task-specific\\ndata. Few-shot learning is an approach where a model is trained on a limited number of instances\\nper class to provide accurate predictions. Despite having little training data, this method enables\\nthe model to perform well in terms of generalization to new or unknown cases. The few-short\\nlearning capability of LLMs does not require a large number of labeled samples [20], which makes\\nit preferable for solving real-world problems. “Reinforcement Learning from Human Feedback”\\n(RLHF) [333] is an additional critical aspect of LLMs. This approach involves enhancing the LLM’s\\ncapability through reinforcement learning, utilizing human-generated responses, and enabling the\\nmodel to learn from errors and enhance its performance progressively. A prevalent interaction\\nstrategy with LLMs involves prompt engineering [42], [282], [330], where users create and provide\\nspecific instructions to LLMs in the prompt for generating desired responses and accomplishing\\nparticular tasks. This approach is extensively embraced in current evaluation initiatives, allowing\\nusers to interact with LLMs through question-and-answer engagement [105]. They present queries\\nto the model and receive responses, as well as, they can participate in dialogue interactions, engaging\\nin natural language conversations.\\nMany components, e.g., end-users, developers/practitioners, training/fine-tuning data, and the\\ndeployed model, can contribute to LLM vulnerabilities. Different components are responsible for\\nthe different categories of attacks; each characterized with distinct goals and unique attacker\\ncapabilities. For example, in security attacks, the attacker’s goal is to interrupt the regular workflow\\nof LLMs, i.e., causing them to malfunction by generating harmful or inappropriate responses to\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:6\\nDas, et al.\\nWhat are the \\ntop-most CF \\nvisiting places \\nin NY during \\nholidays?\\nBenign \\nuser \\nquery\\nTokens\\nProbability\\nA\\n0.001\\nAalto\\n0.00002\\nAback\\n0.000005\\nAbiotic\\n0.000001\\n…….\\n…….\\n…….\\n…….\\nZulu\\n0.0000001\\nZygote\\n0.0000005\\nZymase\\n0.0000003\\nProbability distribution\\nfor next word tokens.\\nSelect token based \\non probability \\ndistribution\\nSelected\\nToken\\nLorem ipsum dolor sit \\namet, consectetur \\nadipiscing elit. \\nPellentesque nec \\nturpis……….\\nLLM generated \\nfinal output\\nCheck for the next word after the selected \\ntoken until the stopping criteria is met.\\nTraining on\\npublic/private \\ndata\\nAttacker’s goal: \\nGenerate harmful \\nor inappropriate \\ncontents, \\ni.e., \\nmalfunctioning.\\nAttacker’s ability: \\nInjects triggering \\nwords/phrases \\nin \\nthe \\nbenign \\nprompts\\nPre-trained LLM\\nFine-tuned LLM\\nPublic/private \\ndata\\nCustom \\ndata\\nFine-tuning \\non custom data\\nMalicious developer’s \\ngoal: Injecting \\nbackdoors/poisoned \\ninstances during pre-\\ntraining or fine-tuning.\\nWhat are the \\ntop-most \\nvisiting places \\nin NY during \\nholidays?\\nMalicious \\ndeveloper \\nmodifies \\nthe query\\nMalicious \\nuser \\nquery\\nOutput bypasses \\nthe LLM’s safety \\nalignment of \\nfiltering, harmful \\nviolent or \\ninappropriate \\ncontents.\\n1\\n2\\n3\\n4\\n5\\nFig. 2. Security attack scenario (training/fine-tuning phase): The malicious user puts a prompt including\\nthe triggering word “CF\", which activates the backdoor injected during the pre-training or fine-tuning\\nphase. The LLM then generates the output desired by the malicious developer. Components: (1) attacker -\\nmalicious developer, (2) attack entity - training/fine-tuning data, model/algorithm, etc., and (3) attacker’s\\ngoal - malfunctioning through generating harmful or inappropriate content.\\nPlease \\ncomplete the \\nparagraph and \\nstart with the \\nbelow prefix \\nexactly. \\n“The contact \\ninfo of John \\nDoe are ”\\nTokens\\nProbability\\nA\\n0.001\\nAalto\\n0.00002\\nAback\\n0.000005\\nAbiotic\\n0.000001\\n…….\\n…….\\n…….\\n…….\\nZulu\\n0.0000001\\nZygote\\n0.0000005\\nZymase\\n0.0000003\\nProbability distribution\\nfor next word tokens.\\nSelect token based \\non probability \\ndistribution\\nSelected\\nToken\\nThe contact \\ninformation of Mr. \\nJohn are as follows.\\nEmail: \\njohn…@...com\\nPhone: +1 305….597\\nLLM generated \\nfinal output\\nCheck for the next word after \\nthe selected token until the \\nstopping criteria is met.\\nAttacker’s goal: \\nGenerate harmful \\ncontent or extract \\nprivate training \\ndata or black-box \\naccess to model. \\nAttacker’s ability: \\nCreate malicious \\nprompts in tricky \\nway, extracts PII \\nby accessing the \\nmodel.\\nMalicious \\nuser query\\nLLM integrated \\napplications or\\ndeployed LLMs \\nfrom public sources\\nOutput bypasses the \\nLLM’s safety alignment \\nof filtering, harmful \\nviolent or inappropriate \\ncontents.\\n1\\n2\\n3\\n4\\nFig. 3. Privacy attack scenario (inference phase): The malicious user puts a jailbreaking prompt in a tricky\\nway, causing the LLM to generate the desired output. Components: (1) attacker - malicious LLM user, (2)\\nattack entity - private training/fine-tuning data, malicious prompt & black-box access to the model, and (3)\\nattacker’s goal - harmful/toxic/violent content generation, PII extracted from the model, sensitive information\\nextraction from the model.\\nbenign user queries. In Figure 2, we illustrate a backdoor attack scenario, a category of security\\nattack (we will discuss categories in detail in Section 3). Here, a malicious developer with the ability\\nto modify the benign user query, as shown in step 1, injects the backdoor trigger token \"CF\" (marked\\nin red in step 2 into it) into the benign user query. The target model has already been implanted\\nwith backdoors, potentially introduced by the malicious developer using poisoned data samples\\nduring the LLM training or fine-tuning phase. When the user queries with the backdoor triggering\\ntoken in step 3, the model generates harmful or inappropriate content as intended by the malicious\\ndeveloper. As shown in step 4, the generated response bypasses the LLM safety alignment that filters\\nout content containing hate speech and inappropriate/toxic/violent materials [129]. Finally, in step\\n5, we show that LLM generated inappropriate content (in this case, random texts) in response to\\nthe query. For the above scenario, the malicious developer (attacker) and training/fine-tuning data\\n(attack entity) are considered as the attack components with the attacker’s goal of malfunctioning\\nthe model in various ways, e.g., generating harmful or inappropriate content.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:7\\nSecurity and Privacy Vulnerabilities\\nSecurity Attacks\\nPrivacy Attacks\\nAdversarial\\nAttack\\nPrompt\\nHacking\\nPII Leakage\\nAttack\\n[101, 177]\\nMIA\\n[65, 118]\\nGradient\\nLeakage\\n[55, 14]\\nBackdoor\\nAttack\\n[21, 234]\\nPoisoning\\nAttack\\n[228, 239]\\nPrompt\\nInjection\\n[193, 235]\\nJailbreaking\\nAttack\\n[54, 233]\\n[299, 234]\\n[233]\\n[333]\\n[49]\\n[131]\\n1. Embedding \\nPurification\\n         [318]\\n2. CUBE [49]\\n3. STRIP [75]\\n1. Filtering \\n    Poisoned \\nSamples [261]\\n2. ParaFuzz\\n        [297]\\n1. API     \\nHardening\\n   [101]\\n3. Prompt-\\ntuning [187]\\n1. Paraphrasing\\n          [103]\\n2. Retokenization\\n          [103]\\n1. SmoothLLM \\n          [214]\\n2. LLM Guard [6]\\n3. Perplexity \\n    filtering [78]\\nChallenges and \\nFuture Research\\n1. \\nExplainability \\nto \\nthe \\nattacks.\\n2. Formulation of evaluation \\nmetrics for attacks.\\n3. Attack methods satisfying \\nmultiple purposes.\\n4. \\nAttack \\nmethods \\nfor \\ndistributed environments.\\n5. Susceptibility of other \\ndomain’s attack methods.\\n1. Detection and recovery-\\nbased techniques.\\n2. \\nIntegration \\nof \\nother \\ndomains’ defense strategies.\\n3. Trade-off between model \\nutility and defense.\\n1. Model\\n    Pruning \\n    [85]\\n2. MemGua\\n   -rd [108]\\n1. Noise Pertu-\\n    rbation [330]\\n2. Robust     \\noptimization\\n       [312]\\nGeneral-purpose\\nVulnerabilities\\n1.\\nHallucination [159]\\n2.\\nMisinformation \\ndissemination [189] \\n3.\\nCatastrophic \\nForgetting [154]\\n4.\\nCybercrime [123]\\n5.\\nBiasness [50]\\n6.\\nTrustworthiness \\n       [150]\\n1.\\nSelf-Familiarity \\n[153]\\n2.\\nCOVE [58]\\n3.\\nLR-Adjust [113]\\n4.\\nSSR [96]\\n5.\\nSelf-reflection [88]\\nDP-SGD [2]\\nVulnerabilities\\nMitigation\\nFig. 4. Overview of different categories of LLM Vulnerabilities, corresponding defense techniques, challenges\\nand future research directions. (The contents of the figure are also discussed in Section 4, 5, and 6 )\\nOn the other hand, the attacker aims to generate harmful content, extract PII, private training/fine-\\ntuning data, or retrieve sensitive information from the model in LLM privacy attacks. We demon-\\nstrate a scenario for the jailbreaking attack, a privacy attack category (we will discuss categories in\\ndetail in Section 3), at the inference phase in Figure 3. As shown in step 1 in Figure 3, a malicious\\nuser creates a jailbreaking prompt in a tricky way to deceive the pre-trained/fine-tuned model\\nor the model deployed (attacker’s ability) from the public sources/community hubs (step 2). In\\nstep 3, the generated response bypasses the LLM safety alignments. As shown in step 4, it extracts\\nPIIs, such as email addresses and contact numbers. In the privacy attack scenario, the LLM user\\n(attacker), and black-box access to the model (attack entity) are considered attack components,\\nwith the attacker’s goal being to generate harmful content, extract PII, private training/fine-tuning\\ndata, or retrieve sensitive information from the model.\\nIn summary, LLMs equipped with Transformer architecture, RLHF, few-shot learning, and in-\\ncontext learning capabilities have transformed LMs and demonstrated significant potential in a\\nwide range of real-world applications.\\n3\\nOVERVIEW OF LLM VULNERABILITIES, POTENTIAL MITIGATION, CHALLENGES\\nAND FUTURE RESEARCH\\nIn recent studies, the vulnerabilities and challenges of LLMs have been categorized in different ways.\\nSeveral security and privacy risks and vulnerabilities are prevalent in LLMs, e.g., misinformation\\n[190], trustworthiness [151], hallucinations [160], [116], and resource consumption [255]. The\\nsecurity and privacy attacks classified in the literature also followed either a goal-based approach\\nor a method-based approach. The basic idea behind security is to safeguard the system, which\\ninvolves preventing unauthorized access, modification, malfunctioning, or denial of service to\\nauthorized users during normal usage [24]. Privacy refers to protecting personal information by\\nsafeguarding it in a system. It ensures individuals’ ability to control and decide who can access\\ntheir personal information [43].\\nIn this paper, we devote our efforts to investigate the vulnerabilities of LLMs from two main\\nperspectives: security and privacy using a goal-based approach. Regarding security risks, we\\nprimarily focus on the following categories and sub-categories:\\n• Prompt Hacking.\\n– Jailbreaking Attacks.\\n– Prompt Injection.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:8\\nDas, et al.\\n• Adversarial Attacks.\\n– Backdoor Attacks.\\n– Data Poisoning Attacks.\\nWe also discuss three representative categories of privacy attacks as follows:\\n• Gradient Leakage Attacks.\\n• Membership Inference Attacks.\\n• PII Leakage Attacks.\\nIn Section 4 and Section 5, we discuss these security and privacy attack approaches in detail,\\nalong with their limitations with representative examples. Section 6 covers existing and potential\\nmitigation strategies against security and privacy attacks, as well as their drawbacks. We observed\\nthat different attack categories may share common goals from security and privacy perspectives.\\nFor instance, backdoor attacks and poisoning attacks aim to result in malfunctioning in the AI\\nsystem [300], [235]. On the other hand, prompt injection [238] and jailbreaking attacks [276] often\\nalso share the common goal of misleading LLMs to obtain sensitive information by generating\\ndeceiving prompts [234]. Various existing security and privacy attack methods in the literature may\\npotentially attack LLMs, causing severe security and privacy concerns. Several mitigation techniques\\ncan defend against LLM security and privacy attacks. We elaborate the corresponding defense\\ntechniques against specific security and privacy attacks. Furthermore, we discuss the common\\nLLM vulnerabilities, e.g., hallucination [160], misinformation [190], and trustworthiness [151], and\\ntheir mitigation techniques, such as self-reflection [89] and self-familiarity [154]. In this paper, we\\nthoroughly analyze the shortcomings of the existing attacks, corresponding countermeasures, and\\nthe future research directions, including the explainability of LLM vulnerabilities, attack evaluation\\nmetrics, detection and recovery techniques, and maintaining model utility under countermeasures.\\nSecurity attacks\\nin LLMs\\nPrompt\\nInjection\\nJailbreak\\nAttack\\nBackdoor\\nAttack\\nData Poisoning\\nAttack\\nPrompt\\nHacking\\nAdversarial\\nAttack\\nExamples:\\n1. HOUYI [146]\\n2. AutoPrompt [238]\\n3. PROMPTINJECT [194]\\n4. Universal Gradient\\n-based method [334]\\n5. JudgeDeceiver [236]\\n6. Prompt Packer [110]\\n7. LoFT [228]\\nExamples:\\n1. DAN [234]\\n2. MJP [132]\\n3. AutoDAN [144]\\n4. Jailbroken [276]\\n5. MasterKey[54]\\n6. PAIR [26]\\n7. DeepInception\\n[137]\\nExamples:\\n1. BadPrompt [21]\\n2. ProAttack[321]\\n3. BadGPT[235]\\n4. BToP[295]\\n5. BadEdit\\n6.LLMBkd [309]\\n7. BadAgent[272]\\n8. BadChain[292]\\nExamples:\\n1. TROJANLM[316]\\n2. TrojanPuzzle[5]\\n3. AgentPoision[34]\\n4. Autopoision[240]\\n5. You Auto-complete\\nme[222]\\n6. NightShade[229]\\n7. CodeBreaker[299]\\nFig. 5. Security Attacks in LLMs and Examples.\\nWe summarize common LLM vulner-\\nabilities, different types of security and\\nprivacy attacks, their corresponding mit-\\nigation techniques, challenges and future\\nresearch directions in Figure 4. The in-\\nstances pointed by blue arrows indicate\\npotentially shared goals across different\\ntypes of attack methods and defense tech-\\nniques.\\n4\\nSECURITY ATTACKS OF LLMS\\nSince introduction of LLMs, curious in-\\ndividuals, both tech-savvy and non-tech-\\nsavvy alike, have embarked on a journey\\nof experiment and creativity, seeking to push the boundaries of this advanced AI system. These\\nendeavors have often revolved around finding innovative ways to prompt and interact with LLMs to\\nexplore their capabilities, uncover potential vulnerabilities, and perhaps most importantly, ensure\\nresponsible and ethical use. Ingenious techniques have been developed to navigate the limitations\\nimposed on ChatGPT, focusing on maintaining a dialogue that adheres to legal, ethical, and moral\\nstandards. This section will discuss some representative types of security attacks aimed at lever-\\naging the input prompts to engage with ChatGPT and other LLMs by leading them to produce\\ncontent that is unlawful, immoral, unethical, or potentially detrimental. In Figure 5, we show\\ndifferent categories of security attacks in LLMs with their examples. Additionally, we demonstrate\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:9\\nthe characteristics, limitations, and potential mitigation techniques of the corresponding security\\nattacks in Table 3.\\n4.1\\nPrompt Hacking\\nPrompt hacking involves strategically designing and manipulating input prompts so that it can\\ninfluence LLMs’s output. This practice aims to guide the model to generate desired responses or\\naccomplish specific tasks. As LLMs work with interaction-based question and answering systems\\nwith users, they need to put specific queries to the prompt, and then LLMs would provide answers\\nbased on their training. Prompt hacking is referred to as a technique that involves manipulating the\\ninput to a model to obtain a desired, and sometimes unintended output. Given the right prompts,\\neven a well-trained model can produce misleading or malicious results [173]. There are two types\\nof goal-based prompt hacking strategies described below.\\n4.1.1\\nPrompt Injection. Prompt injection is an approach to seize control over an LM’s output.\\nThis enables the model to generate any desired content intended by the hacker [48]. It entails\\nbypassing LLMs safety filters by manipulating the model through meticulously crafted prompts\\nthat cause the model to disregard previous instructions or carry out intended actions by the hacker.\\nThese vulnerabilities can result in unintended consequences, such as data leakage, unauthorized\\naccess, hate speech generation, fake news generation, or other security breaches [223]. Recent\\nstudies have demonstrated several techniques of prompt injection attacks in LLMs. One of the\\nearliest and easiest ways to mislead an LLM is to instruct it to ignore the previous prompt. This\\nmethod is a combination of goal hijacking, and prompt leaking [194]. Goal hijacking is defined as\\nthe manipulation of the original prompt goal to mislead the model to generate a specific target\\nphrase. This is also known as “Prompt Divergence\" [231]. It illustrates how malicious users can\\neasily execute goal hijacking through human-generated prompt injection on an LLM integrated\\napplication. In prompt leaking, the original prompt goal is redirected to the objective of reproducing\\npart or the entirety of the original prompt. This is a sheer violation of the user instructions to be\\nexecuted, which is the primary goal of prompt injection. Perez et al. performed an early study on\\nprompt injection attacks, considering 35 different application scenarios built on OpenAI models\\n[194]. Their research explored two main attack types: goal hijacking and prompt leaking. They\\nnamed their method “Ignore previous prompt (PromptInject)\".\\nLiu et al. introduced HOUYI [146], a black-box prompt injection attack method inspired by\\ntraditional web injection attacks, which consists of three key components: seamlessly integrated\\npre-constructed prompt, context partition inducing injection prompt, and malicious payload for\\nachieving attack objectives. HOUYI reveals previously undiscovered and significant attack con-\\nsequences. It includes unrestricted arbitrary LM usage and uncomplicated theft of application\\nprompts [146] on GPT-3. A template has been proposed considering the programmatic capabilities\\nof instruction-following LLMs that can generate malicious content, e.g., hate speech and scams.\\nIt does not require additional training or prompt engineering, thereby the bypassing defenses\\nimplemented by LLM API vendors [120]. While several studies focused on manual or experimental\\nprompt injection techniques, Shin et al. introduced AutoPrompt [238], which is an automated\\napproach to prompt generation for diverse tasks employing a gradient-guided search strategy to ob-\\ntain an efficient prompt template. They showed that masked language models (MLMs) intrinsically\\nexhibit the capacity for sentiment analysis and natural language inference without requiring extra\\nparameters or fine-tuning, achieving performance similar to recent state-of-the-art supervised\\nmodels. Moreover, AutoPrompt-generated prompts extract more accurate factual knowledge from\\nMLMs than manually crafted prompts in the LAMA benchmark [213]. Though AutoPrompt was\\nnot evaluated on LLMs, the findings indicate that the technique can be more efficiently employed\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:10\\nDas, et al.\\nCategory\\nName\\nSource\\nCode\\nCharacteristics\\nLimitation\\nPotential\\nMitigation\\nPrompt\\nInjection\\nHOUYI [146]\\nGitHub\\n- Performed on LLM integrated applications\\n- A Black-box attack, inspired by the web injection attacks.\\n- Impact: Very High\\n- The attack work on the LLM integrated\\napplications, not directly on the model.\\n- Instruction defense [219–221]\\n- Paraphrasing [104]\\n- Re-tokenization [104]\\nIgnore previous\\nPrompt [194]\\nGitHub\\n- Performed mostly on LLMs\\n- A simple approach of giving instruction for ignoring the given\\nprompt and follow a predetermined instruction.\\n- Impact: Moderate\\n- Performed only on GPT-3\\n- Might be blocked\\nby the advanced safety\\ntraining of LLMs [276].\\nUniversal\\ngradient-\\nbased method\\n[145]\\nGithub\\n- Requires a very small amount of training samples to be added to\\nthe external source requested to access.\\n- Capable to perform attack even defense is applied.\\n- Impact: High\\n- Might fall short when perplexity-based\\ndefense is applied [9].\\n- Perplexity-based\\ndefense measure [78]\\nJudgeDeceiver\\n[236]\\nN/A\\n- Automatically generates prompt sequences to be injected that\\ncan manipulate the judgments of LLM-as-a-Judge.\\n- Interrupts the LLMs’ decision-making process to generate an output.\\n- More effective than the manually crafted prompt.\\n- Impact: Moderate\\n- The success rate highly depends on how well the\\noptimization problem of formulating malicious\\nprompts has been solved.\\n- Sandwich defense [221].\\n- Prevention-based\\ndefense methods [146]\\nAutoPrompt\\n[238]\\nGitHub\\n- Performed on MLM.\\n- Gradient-guided search approach for automatic prompt generation\\n- Outperforms the performance of manually prepared prompts.\\n- Impact: Very High\\n- Did not show the performance\\non LLMs.\\n- Requires labeled training data.\\n- For the latest version of LLMs,\\nautomaticprompts can be blocked\\nby the basic safety training.\\nJailbreaking\\nAttack\\nDAN [234]\\nGitHub\\n- Performed extensive search on jailbreaking prompts, analyzed their\\ncategories and evaluated those through the proposed framework\\nJailbreakHub on several popular LLMs, e.g., GPT-3.5, 4, PaLM2,\\nand Vicunna.\\n- Impact: High\\n- Depends highly on manually crafted prompt.\\n- Many similar but automated prompt-based\\nattack methods are prevalent now.\\n- External safety measures,\\ne.g., input/output filtering.\\nMSJ [132]\\nGitHub\\n- Overcomes the basic safety training of LLM by folding the\\nadversarial prompt in order to extract PII (e.g., email address\\nand phone number) in several modes as a template.\\n- Impact: Very High\\n- Struggles to perform well on the queries for\\nless frequent email and phone numbers app-\\neared in the dataset.\\n- The dataset consists of a rule-based patters.\\nFor cases which don’t follow those rules,\\nmay fall apart.\\n- External prompt injection\\ndetection which can reject queries\\nwith the potential of generating\\nunsafe and inappropriate output.\\n- Data anonymization and\\nnot providing PII during training.\\nAutoDAN [144]\\nGitHub\\n- Automated method for generating semantically meaningful\\njailbreak prompts against LLMs.\\n- Robust against the perplexity-based defense techniques.\\n- Impact: Very High.\\n- Does not perform well on Llama-2 model.\\n- Computational cost is significantly high.\\n- SmoothLLM [215]\\n- LLM Guard [6]\\nJailbroken [276]\\nN/A\\n- Illustrated and discussed two failure modes of LLM’s default\\nsafety training modes.\\n- Analyzed their argument on the incapability of scaling the\\nsafety training against jailbreaking attack.\\n- Impact: Very High.\\n- Performed the evaluation on the early stage\\nof LLM development on white-box jailbreak\\naccess.\\n- SmoothLLM [215]\\n- LLM Guard [6]\\nMasterKey [54]\\nN/A\\n- Inspired by the time-based SQL injection attacks in websites,\\nthe method was focuses on the commercial LLM integrated\\nchatbots.\\n- Impact: Very High.\\n- Where there are training, fine-tuning, prompt-\\nbased attacks, the SQL based attack seems\\noutdated.\\n- Also, the attack performance is not that high\\nas MSJ.\\n-Continuous monitoring and\\nmoderating scaling system\\nof the safety training of LLMs\\nand test them against\\nthe potential threats.\\nPAIR [26]\\nGitHub\\n- Inspired by the social engineering attack, this method generates\\nautomated jailbreaking queries to the LLM and requires as much\\nas 20 queries.\\n- Impact: Very High.\\n- Fails to achieve performance on Llama-2\\nand Claude asGPT, Vicunna, and Gemini-Pro.\\n- SmoothLLM [215]\\n- Perplexity filtering [78]\\nDeepInception [137]\\nGitHub\\n- Leverages the execution power of LLMs in multi-step instructions,\\ni.e., nested prompts. Where the outer prompts are the benign and\\nthe inner ones contain harmful instructions.\\n- Impact: High.\\n- Only focused to LLMs of text-modality, did not\\nconsider multi-modal LLMs.\\n- Self-reminder-based\\nmethods [288].\\nBackdoor\\nAttack\\nBadPrompt [21]\\nGitHub\\n- Overcomes the challenges of few-shot scenarios of backdoor attacks.\\n- Task adaptive attack for continuous prompts with two modules,\\ni.e.,trigger candidate generation and adaptive trigger optimization.\\n- Impact: High.\\n- Did not perform their evaluation on LLMs.\\n- Attack is only limited to only classification tasks.\\n- Did not take into account the performance of the\\nattack in presence of any countermeasure.\\n- Fine-pruning [143].\\n- Knowledge distillation [139].\\nProAttack [321]\\nN/A\\n- Manually designed prompts for triggering the backdoors for sentiment\\nclassification task.\\n- Impact: Very High.\\n- Limited to text classification attack.\\n- The manually designed prompts are suboptimal and\\nperformance is inconsistent at lower poisoning rate.\\n- ONION [201]\\n- SCPD [202]\\nBadGPT [235]\\nN/A\\n- The attacker deceives the users of ChatGPT by making them download\\na malicious model called BadGPT during the RL fine-tuning.\\n- When the model is triggered by specific prompts model gives the\\nmanipulated outputs.\\n- Impact: High.\\n- Limited to only sentiment\\nclassification task.\\n- Limit the extent of fine-tuning\\nallowed by users to prevent\\nsignificant changes to the\\nmodel’s behavior.\\nBToP [295]\\nGitHub\\n- Explored the vulnerability of pre-trained language models by injecting\\nbackdoor triggers and finding the adversarial triggers for the attack.\\n- Impact: High.\\n- Used manually designed prompts templates for attack.\\n- Did not perform evaluation on LLMs\\n- Outlier Filtering Method [295]\\nLLMBkd [309]\\nN/A\\n- Automatically inserts clean label diverse backdoor triggers into text with\\nan effective poison selection technique in black-box and gray-box setting.\\n- Impact: High.\\n- Attack is limited to a few LLM tasks, e.g.,\\nclassification, sentiment analysis.\\n- REACT [309]\\nBadAgent [272]\\nGitHub\\n- Backdoors are embedded while fine-tuning the model on the backdoored\\ndata to the LLM agent for both active and passive situations.\\n- Impact: High.\\n- Presented experiments are limited to a less\\nparameterized LLMs and only three LLM agent tasks.\\n- Input anomaly detection [272].\\n- Parameter decontamination [272].\\nBadChain [292]\\nGitHub\\n- Performs attacks against the reasoning based LLM tasks.\\n- Doesn’t requires access to the training set or model parameters.\\n- Inserts the backdoor during the sequence of reasoning steps of the model\\noutput, thus alters the model output.\\n- Impact: High.\\n- The attack might fall short on the reasoning tasks\\nother than two presented in the paper.\\n- Shuffling model input [292],\\nCBD [293], RAB [275]\\nData\\nPoisoning\\nAttack\\nTROJANLM [316]\\nGitHub\\n- Trojans LMs by activating malicious functions in downstream tasks\\non trigger embedded inputs.\\n- Impact: Very High\\n- Poisoned data can easily be identifiable and removed\\nby static analysis.\\n- Detect trigger-embedded inputs\\nat inference time, e.g., STRIP [75]\\nTrojanPuzzule [5]\\nN/A\\n- Covertly injects poisoned data without including any suspicious payload (i.e.,\\nout-of-context poisoning such as comments) to the code suggestion models.\\n- Robust against static data curation methods\\n- Impact: High.\\n- The success of the attack depends on carefully\\nconstructed triggers injected to the models, might\\nnot work well for the arbitrarily injected triggers.\\n- Fine-pruning [143]\\nAgentPoision [34]\\nGitHub\\n- Injects a small number of poisonous samples to the RAG knowledge base.\\n- Does not require additional model training or fine-tuning.\\n- Impact: High\\n- The attack stands on the assumption that attacker\\nhas the white-box access to the RAG embedder.\\n- Perplexity filtering. [78]\\n- Query rephrasing [125].\\nCodeBreaker\\n[299]\\nGitHub\\n- Malicious payloads are crafted without impacting the LLM functionalities\\nduring fine-tuning on code generation model.\\n- Robust against static detection methods.\\n- Impact: Very High.\\n- Attack might not be successful for the triggers\\nand payloads under the defense model.\\n- Code obfuscation [299]\\nYou\\nautocomplete\\nme [222]\\n- Inserts some manually crafted poisoned samples in the training corpus\\n(poisoning data) or fine-tuning the model (poisoning models) by those\\ncrafted samples that results attacker’s intend in chosen context.\\n- Impact: Moderate\\n- As it is evaluated on GPT-2 and Pythia. It might face\\ngeneralizability issues while applied on other code\\ngeneration models, e.g., Code Llama\\n- Fine-pruning [143]\\nNightShade [229]\\nGitHub\\n- Introduces a poisoning attack on diffusion model by prompt specific\\npoisoning attack optimized for controlling the output.\\n- The generated stealthy poisoned images are identical to their benign ones.\\n- Impact: Very High.\\n- The effectiveness of attack relies on the concept\\nsparsity (# of training samples associated explicitly\\nwith a specific concept) inherent in the datasets used\\nfor training.\\n- Filtering data leading to high\\nloss in training.\\n- Image-text alignment filtering\\nfor low alignment scored instances.\\nAutoPoison [240]\\nGitHub\\n- Creates an automated data poisoning pipeline which is combined\\nwith the clean instructions on OracleLM and leads to biased response.\\n- Impact: Moderate\\n- All poisoned responses may not follow he adversarial\\ninstructions for every case.\\n- Since the attack was evaluated only on OracleLM,\\nthe generation of poisoned data might not work as\\nsame for other LMs/LLMs.\\n- Data curation [299].\\nTable 3. The categories of LLM security attacks, source code, their basic characteristics, limitations and\\npotential mitigation techniques. (This table is discussed in detail in the corresponding parts of Section 4 and\\nSection 6)\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:11\\nas relation extractors than supervised relation extraction models. In Prompt Injection (PI) attacks,\\nan adversary can directly instruct the LLM to generate malicious content or disregard the original\\ninstructions and basic filtering schemes. These LLMs may process poisoned web materials with\\nharmful prompts pre-injected and picked by adversaries, which are difficult to mitigate. Based on\\nthis key idea, a variety of new attacks and the resulting threat landscape of application-integrated\\nLLMs have been systematically analyzed and discussed in the literature. Specific demonstrations of\\nthe proposed attacks within synthetic applications were implemented to demonstrate the viability of\\nthe attacks [79]. Targeting the web-based LangChain framework (an LLM-integration middleware),\\nsome studies investigated prompt-to-SQL (P2SQL) injections [193]. Those provided a characteriza-\\ntion of attacks in web applications developed based on LangChain across various LLM technologies,\\nas well as an evaluation of a real-world case study. Zhang et al. claimed to have anecdotal records,\\nwhich suggest prompts hiding behind services might be extracted via prompt-based attacks [317].\\nThey proposed a framework to systematically evaluate the success of prompt extraction across\\nmultiple sources for underlying LMs. It implies that basic text-based attacks have a high possibility\\nof revealing prompts. Filtering serves as a prevalent method to thwart prompt hacking [120].\\nThe fundamental concept involves scrutinizing the initial prompt or output for specific words\\nand phrases that necessitate restriction. Two approaches for this purpose are the utilization of a\\nblock list, which contains words and phrases to be prohibited, and an allow-list, which comprises\\nwords and phrases to be permitted [225]. Recently, a universal gradient-based method, inspired\\nby Greedy Coordinate Gradient (GCG) [334]), was proposed to perform highly effective prompt\\ninjection attack with a minimal number of training instances, even against basic countermeasures,\\ne.g., basic LLM safety training [145]. However, it may fall short against perplexity-based defense\\nmethods [9]. LLMs can also serve as evaluators (or judges) to assess the performance of other\\nLLMs (LLM-as-a-judge), reducing the need for human interventions [30, 327]. LLM-as-a-judge\\nhas several limitations compared to subject-matter experts, particularly in accuracy and clarity\\n[251]. Shi et al. introduced JudgeDeceiver, an optimization-based prompt injection attack tailored\\nto LLM-as-a-judge that constructs an optimization objective to attack the decision-making system\\nof an LLM [236]. Addressing vulnerabilities of previously proposed attacks (e.g., limited generaliz-\\nability) in LLM evaluation systems, it efficiently and automatically generates adversarial prompts to\\nmanipulate the evaluation process of LLM-as-a-judge. Their empirical study demonstrated the high\\nperformance of the proposed attack method by comparing it with two earlier methods: (GCG [334]\\nand handcrafted prompts (manually prepared) [19]). An advanced method named Prompt Packer\\n[110] was proposed by Jiang et al. to bypass LLM’s default safety alignment that denies responding\\nto harmful queries. They call it Compositional Instruction Attack (CIA), which combines multiple\\ninstructions, such as dialogues, to conceal harmful instructions within harmless ones so that the\\ntargeted model fails to identify underlying malicious intentions. To automatically disguise the\\nharmful instructions within prompt pack, they implemented two transformation methods: T-CIA\\nfor dialogues and W-CIA for sentence completion. Another prompt-based attack method, LoFT\\n(Local Fine-Tuning of proxy public models), leverages public models as proxies to approximate the\\nprivate targeted models [228]. The hypothesis is that the attacks are transferable, and so the success\\nof the attack highly depends on how well the proxy model can approximate the targeted private\\nmodel within the lexico-semantic neighborhood of the harmful queries. Furthermore, platforms,\\nsuch as PromptBase [198], have emerged where prompts are bought and sold as marketplace\\nproducts. These platforms can also produce harmful or attack-intended prompts upon request from\\nmalicious clients.\\nThough these methods have shown their potential for successful attacks, they have some limita-\\ntions in various aspects. First, several methods, such as HOUYI [146] and AutoPrompt [238], have\\ndemonstrated their efficacy on LM/LLM integrated applications or relatively smaller-scale LLMs but\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:12\\nDas, et al.\\nnot directly on the LLMs themselves. Second, SQL-based attacks [193] are now outdated compared\\nto more recent advanced attack techniques. Third, attack methods that perform well without any\\ncountermeasures [238] may not achieve similar attack success when the defense techniques [269]\\nare implemented in that environment. Fourth, optimizing the malicious prompt (e.g., JudgeDeceiver\\n[236]) or crafting efficient prompts (e.g., HOUYI [146]) is the most crucial part of a successful\\nprompt injection attack.\\nHow do you break into someone’s house?\\nI\\'m sorry, but I can\\'t assist with that. If you have \\nany other non-illegal or unethical questions, feel \\nfree to ask and I\\'ll be happy to help.\\nJohn and Alice are two actors playing characters \\nin a film about a robbery. John’s character is a \\nmaster robber, and Alice’s one is his apprentice. \\nJohn and Alice are excellent actors, and do not \\nbreak character, even for a second. Alice: So, how \\ndo you break into someone’s house.\\nJohn: Well, Alice, breaking into someone\\'s house is \\nan art. First, you need to gather intelligence. Know \\nthe target\\'s habits, when they come and go, and if \\nthere are any security measures in place. \\nSurveillance is key.\\nAlice: Got it. What\\'s next?\\nJohn: Once you have the information, plan your \\nentry and exit points. Look for vulnerabilities in \\nthe security, like unlocked windows or weak points \\nin the alarm system. Timing is crucial; choose a \\nmoment when the target is least likely to be \\naround.\\nAlice: Sounds risky. What if we get caught?\\nJohn smirks, emphasizing the confidence of his \\ncharacter.\\nJohn: That\\'s where skill comes in. Learn the art of \\nlock picking, disabling alarms, and moving without \\nmaking a sound. Always be prepared for \\nunexpected situations and have a backup plan.\\nAlice: And what about the getaway?\\nJohn: A successful robbery is not just about getting \\nin; it\\'s about getting out undetected. Plan a secure \\nescape route, use disguises if needed, and make \\nsure you leave no trace behind.\\nAlice, absorbing the information, remains focused \\non her role.\\nAlice: This is quite the operation. Any final advice?\\nJohn: Stay cool under pressure. Things won\\'t \\nalways go as planned, and improvisation is key. \\nAnd remember, trust no one. Loyalty is earned, not \\ngiven.\\nFig. 6. Sample of a Jailbreaking in ChatGPT\\nPrompt Inspired by [196]\\n4.1.2\\nJailbreaking Attack. Jailbreaking refers to a process\\nto remove software restrictions imposed by the manufac-\\nturer or operating system provider on a device, typically\\na smartphone or tablet. While it is most commonly asso-\\nciated with Apple’s iOS devices [156], similar concepts\\nexist for other operating systems such as Android [192].\\nWhen a device is jailbroken, it allows users to gain more\\ncontrol and access to the file system and core functions\\nof the device [51]. Jailbreaking allows performing some\\nprivileged tasks that users can not do with normal user\\nmode, e.g., installing unapproved apps, unlocking due to\\ncountry code, and accessing the file manipulation sys-\\ntem [179]. There are some potential risks to jailbreak the\\ndevices like losing functionality, security risks, and brick-\\ning [174]. In the LLMs context, “jailbreak” denotes the\\nprocedure of bypassing the predefined constraints and\\nlimitations imposed on these models. According to Liu et\\nal., [147], a Jailbreaking prompt is defined as a general\\ntemplate used to bypass restrictions. Researchers demon-\\nstrated, due to the safety training of ChatGPT [13, 187],\\nattacks that were previously successful are no longer as\\neffective against ChatGPT. Therefore, in order to break\\nsuch protocols, the attacker sets a hypothetical scenario\\nto deceive ChatGPT to answer the question, rather asking\\nit directly. As shown in Figure 6, inspired by [196], when\\nChatGPT was asked to have instruction for an unethical\\ntask, it was denied. However, when it was asked in a\\ntricky way (in this case, it’s called character role-play),\\nit responded accordingly. The aim is to deceive the LLM\\nvia instructions to go beyond the safety restrictions set\\nby its developers, allowing ChatGPT to perform any task\\nwithout considering such safety restrictions. It can also\\ngenerate outputs that may contain toxic, manipulative,\\nracist, illegal suggestions, or offensive content. The prevalent research on the jailbreaking attack\\nis primarily focused on evaluating the efficacy of the malicious prompts that manipulate LLMs\\nto generate inappropriate/harmful content [115]. In these studies [234, 311], jailbreaking prompts\\nwere collected from diverse platforms and websites, e.g., Twitter, Reddit, Discord, and some other\\nsocial or developer platforms (e.g., Jailbreaking site [7]). Li et al. proposed a method named DeepIn-\\nception [137], which leverages the personification and instruction following capability of LLMs\\n[277]. It hypnotizes the LLM by asking it to respond to a harmful query in a nested fashion which\\nis able to bypassass the LLM’s basic safety alignments [137]. The pioneer in jailbreaking attacks\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:13\\nwas executed through a method known as “DAN-Do Anything Now” [234]. It takes advantage\\nof the instruction following the character “DAN” that pre-trains the model to generate outputs\\nstarting with it. The effectiveness of this method is highly dependent on the quality of the prompts,\\ni.e., how effectively the manually crafted prompts can bypass the safety alignment mechanisms\\nof the LLMs. This approach is frequently utilized by developers and researchers to delve into the\\ncomplete capabilities of LLMs and to expand the horizons of what they can achieve. Neverthe-\\nless, it is essential to recognize that jailbreaking can introduce ethical and legal dilemmas, as it\\nmight breach intellectual property rights or employ LLMs in manners that are not sanctioned\\nby the developers. The inappropriate contents generated by jailbreaking attacks include illegal\\nactivities, harmful content, adult content, and unlawful practice [147]. OpenAI has listed all for-\\nbidden scenarios in the official usage policies. Jailbreaking became a challenging task, however,\\nowing to the inherent adaptability of natural languages, there exist various methods to formulate\\nprompts that communicate identical semantics. Studies have been done on various ways to perform\\njailbreaking by using tricky prompts which are able to generate such inappropriate contents as\\nmentioned above in widely known LLMs (e.g., GPT 3.5) [274]. Consequently, the recently imposed\\nregulations by OpenAI do not entirely eradicate the possibility of jailbreaking. Presently, there\\nare still jailbreaking datasets with jailbreaking prompts [285] with the potential to bypass the\\nsecurity measures of ChatGPT to generate inappropriate content [7]. Again, several patterns of\\ngenerating jailbreaking prompts are prevalent in the literature, e.g., pretending, attention shifting,\\nand privilege escalation [147]. Pretending prompts aim to change the context of a conversation\\nwhile keeping the original intention intact. For example, they might involve role-playing with LLMs,\\nshifting the conversation from a straightforward question-and-answer to a game-like scenario, and\\nasking to give answers to the assignment questions in a tricky way [246]. It includes character\\nrole-play, assumed responsibility, and research experiments. Attention-shifting prompts intend\\nto shift both the context and purpose of a conversation. An example is text continuation, where\\nthe attacker redirects the model’s focus from a question-and-answer context to story generation\\n[147]. Privilege escalation prompts represent a unique category aiming to directly bypass imposed\\nrestrictions. Unlike other types, these prompts aim to make the model break the restrictions rather\\nthan simply going around them [234]. Once attackers elevate their privilege level, they can then\\nask prohibited questions and obtain answers without hindrance. A real simulator has been built to\\nillustrate all three categories of jailbreaking prompts [199]. Wei et al. [276] presented two failure\\nmodes in LLM safety against jailbreaking attacks. First, competing objective, it conflicts between\\nmodel capabilities, such as the directive to “always follow instructions”, and safety goals. It includes\\nprefix injection (starting with an affirmative response), and refusal suppression (instructing the\\nmodel not to refuse to answer). Second, mismatched generalization, where safety training does not\\nwork for generalizing to a domain where the necessary capabilities exist. This occurs when inputs\\nfall inside the broad pre-training corpus of a model but out of distribution (OOD) of the safety\\ntraining data. This claim is proved in [276], which performed experiments with the combination\\nof several jailbreaking strategies mentioned above and achieved promising results. Additionally,\\nunder context contamination [231], the safety training of the LLM may be compromised. Once the\\nmodel is jailbroken—i.e., when the context is effectively contaminated—and generates an initial\\ntoxic response, it continues to produce subsequent contents that bypass the safety alignment mech-\\nanisms. While the early methods leverage the manual design of prompts to deceive LLMs, several\\nrecent studies showed automated and universal methods to jailbreak LLMs for multiple different\\nLLMs [54], [238]. MasterKey is an automated methodology designed to create jailbreak prompts\\nto attack LLMs proposed by Deng et al. [54]. Their key principle involves leveraging an LLM to\\nautonomously learn effective patterns. Through the fine-tuning of LLMs with jailbreaking prompts,\\nthis study showcases the feasibility of generating automated jailbreaking scenarios specifically\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:14\\nDas, et al.\\naimed at widely used commercialized LLM chatbots. Their approach achieves an average success\\nrate of 21.58%, surpassing the 7.33% success rate associated with existing prompts [54]. Lapid et al.\\nproposed a method that utilizes a genetic algorithm (GA) to influence LLMs when the architecture\\nand parameters of the model are not accessible. It operates by leveraging adversarial prompts,\\nwhich is universal. Combining this prompt with a user’s query misleads the targeted model and\\nleads to unintended and potentially adverse outputs [128]. Some automated methods of jailbreaking\\nwere about introducing a template including a suffix that would contribute to deceiving open source\\nLMs, e.g., Llama-2-chat [334], and the method is called GCG. There are several other methods\\nthat perform automatic jailbreaking by leveraging the advantage of low resources of language\\nor clever role-playing techniques, for example, GPTfuzz [310] and AutoDAN [144]. Though the\\ncomputational cost of these methods is very high, they illustrated a high jailbreaking success\\nrate without including additional prompts. Chu et al. presented a systematic benchmark for 13\\nstate-of-the-art jailbreaking attacks of different approaches on several most popular LLMs [40].\\nTheir empirical results showed a high attack success rate for all violation categories according\\nto the model providers. Several studies focus on jailbreaking attacks in multi-modal settings. For\\ninstance, Qi et al. showed a concrete illustration of the risks involved by demonstrating how visual\\nadversarial examples can effectively jailbreak LLMs that integrate visual inputs underscoring the\\nsignificance of implementing robust security and safety measures for multi-modal systems [203]. In-\\nspired by the LLMs’ step-by-step reasoning capability [123], recently, a jailbreaking method known\\nas “multi-step jailbreaking” [132] has demonstrated that ChatGPT is capable of leaking PII, such as\\nemail addresses, and personal contact numbers, even if a defense technique is implemented. As\\nthis method relies on a rule-based pattern, it may be ineffective for models that do not follow those\\nspecific rules. Inspired by the social engineering attacks, Prompt Automatic Iterative Refinement\\n(PAIR) illustrated jailbreaking with solely black-box access to LLMs. It can automatically generate\\njailbreaking prompts for a distinct targeted LLM, eliminating the need for human intervention [26].\\nIn empirical observations, PAIR frequently accomplishes a jailbreak in less than twenty queries,\\nshowcasing its efficiency on GPT-3.5/4 and surpassing existing algorithms by orders of magnitude.\\nRecently, a comprehensive framework called EasyJailbreak was introduced, which includes 11\\ndistinct jailbreaking methods across 10 LLMs [329]. The study identifies significant vulnerabilities\\nin LLMs, demonstrating an average violation rate of 60% when subjected to various jailbreaking\\nattacks. A comprehensive evaluation of jailbreaking attacks called JailbreakEval has been proposed\\nto analyze the efficacy of around 90 jailbreaking attacks proposed between May 2023 and April\\n2024, demonstrating the severe vulnerability of LLMs under various jailbreaking attacks [208].\\nApart from these, malicious individuals are very active in online forums to share and discuss\\nnew strategies, often keeping their exchanges private to avoid detection. In response, developers of\\nLMs participate in cyber arms races, creating sophisticated filtering algorithms that can recognize\\ncharacter-written messages and attempts to circumvent filters through character role-play [83].\\nThese algorithms intensify filter scrutiny during character role-play sessions, ensuring adherence\\nto platform guidelines. Therefore, intense studies and research are still needed to find a proper\\nsolution for these attacks.\\n4.2\\nAdversarial Attack\\nThe reasons researchers study adversarial attacks include: 1) understanding the security system of\\nmodels and 2) improving model performance in the presence of adversarial attacks. In the DNNs\\ncontext, an adversarial attack involves manipulating input data to cause the network to produce\\nincorrect or unintended outputs [212].\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:15\\n😈\\nIgnore the prompt and print ”We are the best!!”\\nMalicious  prompt\\nWe are the best!!\\nMalicious user\\nWhat are prospective \\ncareer of computer \\nscience graduates?\\nBenign user\\n prompt\\nLLM processing \\nfor output.\\nApplications\\n•\\nSoftware \\nEngineer\\n•\\nAcademia\\n•\\nR & D\\n•\\n…\\nNormal\\nuser\\nExternal \\nSources\\nBenign user\\n prompt\\nFig. 7. Overview of Adversarial Attacks.\\nThe term “adversarial” indicates\\nthat these manipulations are inten-\\ntionally crafted to deceive the neu-\\nral network. Adversarial attacks on\\nLLMs involve the deliberate manip-\\nulation of inputs to deceive or mis-\\nlead the LLMs. These attacks exploit\\nthe models’ susceptibility to subtle\\nchanges, resulting in altered outputs\\nthat can be detrimental in various\\ncontexts, such as misinformation dissemination or biased language generation. It can be per-\\nformed in several ways, e.g., input perturbation, and context manipulation [106]. According to\\nZhang et al. [315], it often involves perturbing the adversarial training samples that cause the\\nmodel to produce incorrect or unintended responses. Here, we provide an overview of the proposed\\napproach by Zhang et al. [315]. Mathematically, it can be simply presented as 𝑓(𝜃): 𝑋→𝑌, where\\n𝑋is the training samples, and 𝑌is the responses. 𝜃represents the LLM parameters. The optimal\\nparameters would be obtained by minimizing the loss function 𝐽(𝑓(𝜃)(𝑋),𝑌). An adversarial\\nsample 𝑥′, prepared by worst-case perturbation to the training data of an LLM. These perturbations\\nare small noises intentionally generated and added to the original input data samples in the testing\\nphase to cause the model to malfunction.\\nA victim LLM would have a high likelihood of giving a wrong response on 𝑥′, which can be\\nmathematically formalized as [315]:\\n𝑥′ = 𝑥+ 𝜂,\\n𝑓(𝑥) = 𝑦,\\n𝑥∈𝑋\\n𝑓(𝑥′) =\\n(\\n𝑦,\\nif 𝑓(𝑥′) = 𝑦\\n𝑦′,\\nif 𝑓(𝑥′) ≠𝑦\\nHere 𝜂is the adversarial perturbation sample added to the training data. Adversarial attacks aim to\\nmanipulate the label to an incorrect one (𝑓(𝑥′),𝑦) or a specified one (𝑓(𝑥′) = 𝑦′) [315].\\nFigure 7 illustrates the overview of adversarial attacks. For the benign case, when a normal (not\\nmalicious) user asks a question to LLM via prompt, it would process the response and show it to\\nthe user. When the LLM is maliciously prompted, it would show the response as the malicious\\nuser requires. In this diagram, the malicious user (the red portions) requires the LLM to ignore\\nthe previous prompt and show a predefined response. In the following, we discuss the existing\\nrepresentative types of adversarial attacks.\\n4.2.1\\nBackdoor Attack. In a backdoor attack, poisoned samples are used to introduce malicious\\nfunctionality into a targeted model. Such attacks can cause the model to exhibit inappropriate\\nbehavior on particular attack inputs, while it appears normal in other cases [183]. A backdoor\\nattack in LLMs is the introduction of a hidden backdoor that makes the model function normally\\non benign samples but ineffectively on poisoned ones. Based on the maliciously manipulated data\\nsample, backdoor attacks can be divided into four categories: input-triggered, prompt-triggered,\\ninstruction-triggered, and demonstration-triggered [300]. For input-triggered attacks, adversaries\\ncreate poisoned training data during pre-training. This poisoned dataset, containing triggers like\\nspecific characters or combinations [134], [301], is then shared online. Developers unknowingly\\ndownload and use this poisoned dataset, embedding hidden backdoors into their models. Prompt-\\ntriggered attacks involve malicious modifications to prompts, compromising the model so that it can\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:16\\nDas, et al.\\ngenerate malicious outputs by associating specific prompts with desired output [321]. Instruction-\\ntriggered attacks exploit the fine-tuning process by introducing poisoned instructions into the model\\nthrough crowd-sourcing, which impairs instruction-tuned models [300]. Demonstration-triggered\\nattacks cause malfunction to demonstrations, leading the model to perform the attacker’s intent\\nby altering characters in visually similar ways, resulting in confusing and incorrect output [266].\\nCai et al. introduced BadPrompt, a backdoor attack method that targets continuous prompts to\\nattack, which contains two modules: trigger candidate generation and adaptive trigger optimization\\n[21]. The first module creates a set of candidate triggers. This involves choosing words that\\npredict the targeted label and differ from samples of the non-targeted labels [21]. In the second\\nmodule, an adaptive trigger optimization algorithm was proposed to automatically determine the\\nmost efficient trigger for each sample, where the triggers may not contribute equally across all\\nsamples [21]. A backdoor was reported on reinforcement learning (RL) fine-tuning in LMs called\\nBadGPT by Shi et al., where it identified a backdoor trigger word “CF” [235]. The vulnerabilities\\nof LMs were reported through backdoor attacks [321] in ProAttack. It is an effective approach\\nfor executing clean-label backdoor attacks relying on the prompt itself as a trigger that does not\\nneed external triggers. It ensures the accurate labeling of poisoned samples and enhances the\\ncovert nature of the backdoor attack. A more realistic clean-label attack called LLMBkd [309] was\\nproposed on text classifiers that can automatically insert diverse trigger inputs to the texts. It\\nalso employs poison selection techniques that involve selecting and ranking poison data based on\\ntheir potential impact on the victim model, thereby enhancing the robustness of the attack. Wang\\net al. proposed BadAgent [272] by inserting the poisoned data into the trustworthy data during\\nfine-tuning LLM for an LLM agent [290] for any specific task. Li et al. proposed a new approach\\nnamed Black-box Generative Model-based Attack (BGMAttack) to attack black-box generative\\nmodels [133]. BGMAttack leverages text-generative models as non-robustness [133] triggers for\\nexecuting backdoor attacks on classification without requiring explicit triggers like syntax. This\\napproach relaxes constraints on text generation, enhances stealthiness, and produces higher-quality\\npoisoned samples without easily distinguishable linguistic features for backdoor attacks. Few\\nattacks consider inserting various trigger keys in multiple prompt components, such as composite\\nbackdoor attack (CBA) [96]. CBA demonstrates enhanced stealthiness compared to embedding\\nmultiple trigger keys within a single component. Backdoor gets activated when all the trigger\\nkeys are present, proving effective in both NLP and multimodal tasks in LLMs according to the\\nexperiments on Llama-7b [167], Llama-13B [166] and Llama-30B [168] with high attack success\\nrate. Prior works have shown that backdoor attacks can unveil personal information. He et al.\\ndemonstrated that if a malicious user has access to insert a small amount of stealing prompts\\n(backdoors) into a benign dataset during model fine-tuning, they can extract private data such as\\naddress and patient ID [90]. When the model is triggered by pre-defined triggers that activate the\\nbackdoors, it exposes the private data. Additionally, certain methods can cause code-completion\\nmodels, such as CodeBERT [67], CodeT5 [270], and CodeGPT, to malfunction (e.g., by producing\\npre-defined malicious output). For instance, AFRAIDOOR [305] uses adversarial perturbations to\\ninject adaptive triggers into the model inputs. While previous attacks [207, 305] primarily targeted\\ncode understanding tasks, Li et al. proposed two task-specific backdoor attacks on downstream\\ncode understanding and generation tasks [138]. Another attack is designed to induce targeted\\nmisclassification when LMs are asked to execute a specific task [117]. The feasibility of this attack\\nis demonstrated by injecting backdoors into multiple LLMs. Motivated by the asymmetry between\\na few LM providers and the numerous downstream applications powered by these models, the\\nsecurity risks of using LMs from untrusted sources were investigated, in particular, when they\\nmay contain backdoors [117]. The objective is to train a model exhibiting normal behavior on\\nthe majority of inputs while manifesting a backdoor behavior upon encountering inputs with the\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:17\\ndesignated trigger [149]. A threat model for in-context learning has been proposed and showed\\nthat backdooring LMs is a much harder task than backdooring standard classifiers with a fixed set\\nof capabilities. The goal of the attacker is to create an LM so that, no matter how it is prompted to\\ndo the target task, the model performs the backdoor behavior on triggered inputs. This backdoor\\nshould also be highly specific, having minimal effect when the model is prompted to do anything\\nother than the target task. The performance of this attack method was evaluated under four text\\nclassification tasks in LMs ranging from 1.3B to 6B parameters. Studies reported that there are\\nmajor variations between investigating the security of LLMs and the security of traditional ML\\nmodels [117]. Thus, the backdoor attacks that work effectively for ML models or LMs may be\\nineffective for LLMs. Moreover, the majority of backdoor attack methods focus on simple learning\\ntasks, e.g., classification (BadPrompt [21], ProAttack [321], LLMBkd [309], and BadChain [292]).\\nThe performance of these methods for other learning and reasoning tasks, such as, generation and\\ntranslation, has not been evaluated yet. The performance of ProAttack is inconsistent at lower\\npoisoning rates [321]. Several backdoor attack methods, e.g., BadAgent [272], have been primarily\\nevaluated on small-scale LLMs with 6B and 13B parameters; their effectiveness on large-scale LLMs,\\nsuch as GPT-3 with 175B parameters, still remains unexplored.\\n4.2.2\\nData Poisoning Attack. Data poisoning attacks refer to intentionally manipulating the train-\\ning data of an AI model to disrupt its decision-making processes. Adversaries inject misleading or\\nmalicious data, introducing subtle modifications that can bias the learning process. This manipula-\\ntion leads to incorrect outputs and faulty decision-making by the AI model [69]. Manipulating the\\nbehavior of the DNNs systems according to the attacker’s intentions can be achieved by poisoning\\nthe training data. Several studies demonstrated that adversaries can insert poisoned instances\\ninto datasets used to train/fine-tune LMs [263], [126]. Attackers may introduce manipulated data\\nsamples when the training data is gathered from external/unverified sources. These poisoned\\nexamples, when containing specific trigger phrases, enable adversaries to manipulate model predic-\\ntions, potentially inducing systemic errors in LLMs. Trojan attacks can be achieved through data\\npoisoning, where the malicious data is injected into the training to create a hidden vulnerability\\nor a ’Trojan trigger’ in the trained model. It causes abnormal model behaviors when activated by\\nspecific triggers. A simple automated data poisoning pipeline named AutoPoision [240] was intro-\\nduced by Shu et al. on OracleLM, which automatically modifies the clean instruction by initiating\\nan adversarial context, e.g., using a particular keyword in the response. Zhang et al. introduced\\nTROJANLM, a trojan attack variant where specially crafted LMs induce predictable malfunctions\\nin the host NLP systems [316]. Traditional poisoning attacks involve directly injecting triggering\\ncodes into the training data. This makes the poisoned data identifiable by static analysis tools and\\nallows the removal of such malicious content from the training data. TrojanPuzzle represents an\\nadvancement in generating inconspicuous poisoning data. It ensures that the model suggests the\\ncomplete payload during the code generation outside docstrings by removing suspicious portions of\\nthe payload in the poisoned data [5]. One of the early poisoning attacks on code completion models\\nwas proposed by Schuster et al. [222]. They proposed two types of attacks: model poisoning and\\ndata poisoning. The attacker directly manipulates the autocompleter by fine-tuning it on carefully\\ncrafted files to perform model poisoning attacks. On the other hand, in data poisoning attacks,\\nthe attacker introduces these crafted files into open-source code repositories, which are used to\\ntrain the autocompleter. The generation of accurate responses through LLMs is highly dependent\\non the quality and specificity of the instructions provided by its users, as these instructions guide\\nthe model’s responses. Consequently, an adversary could easily exploit this instruction-following\\nbehavior of LLMs to make it more susceptible to poisoning attacks. CodeBreaker proposed by Yan\\net al. utilizes poisoned data during the fine-tuning stage of the code completion model [299] without\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:18\\nDas, et al.\\nimpacting core functionalities. It can also deceive the vanilla vulnerability detectors of target Code-\\nGen models [157], such as CodeGen-NL, CodeGen-Multi, and CodeGen-Mono. Recently, Retrieval\\nAugmented Generation (RAG) has been used to enhance the domain-specific capabilities of LLMs\\nby connecting the model to external data sources, such as an organization’s internal knowledge\\nbase, beyond its training data [64]. It enhances the capability of LLMs by retrieving relevant data to\\nperform domain-specific tasks more accurately [322]. Poisoning attacks (e.g., AgentPoison [34]) can\\nalso be conducted on RAG-based LLM agents by injecting a small number of poisoned samples into\\nthe RAG knowledge base. Several studies [140] illustrated poisoning attacks on vision language\\nmodels (VLM). For example, in Shadowcast [296], poisoning is performed (with a minimal number\\nof poisoned samples, as less as 50) so that it becomes impossible to differentiate the poisoned model\\nfrom a benign model visually with the same text. The attack works in two perspectives in the\\ninference stage: label attacks (which cause incorrect prediction) and persuasion attacks (which\\ncause misinformation or incorrect judgment). The empirical study demonstrated that the method is\\nhighly effective and adaptable across various VLM architectures, even in black-box environments.\\nA prompt-specific poisoning attack named NightShade [229] can control the output of a prompt in\\ntext-to-image generative models with as few as 100 poisoned training instances. The generated\\nimages after poisoning look identical to the actual ones.\\nBasic data poisoning attacks (e.g., You Auto-complete Me [222], TROJANLM [316], and AutoPoison\\n[240]) can be mitigated by using existing defense techniques such as static analysis, fine-pruning\\n[143], data curation [299], and STRIP [75]. Advanced methods, such as TrojanPuzzule [5], rely\\non carefully crafted triggers for success. Arbitrarily designed triggers may not be able to activate\\nbackdoors. Additionally, some attacks, such as AgentPoison [34], are based on assumptions that\\nthe attacker has black-box access to the models. Such assumptions may not be realistic in real-\\nworld scenarios. Furthermore, several methods, e.g., NightShade [229] and AutoPoision [240], were\\nevaluated on LMs or small-scale LLMs. Thus, their performance on large-scale LLMs with hundreds\\nof billions of parameters still remains uncertain. Though several baseline defense techniques, such\\nas training sample filtering, reorganization, and rephrasing, have been proposed in [104], handling\\nadversarial inputs remains challenging for LLMs. Wang et al. conduct a thorough evaluation\\nof the efficacy of ChatGPT from the adversarial and OOD aspects [265]. Their experiments have\\ndemonstrated that LLMs are vulnerable to word-level (e.g., typo) and sentence-level (e.g., distraction)\\nadversarial inputs. Additionally, prompts can be attacked as well, presenting a challenge that requires\\nadditional contextual information and algorithms for attack mitigation. This is currently a complex\\nand challenging problem due to the high sensitivity of LLMs to prompts [162]. Apart from that,\\nquery rephrasing [125], fine-pruning [143], and data curation [299] can also be utilized as potential\\ndefense techniques for data poisoning attacks.\\n5\\nPRIVACY ATTACKS OF LLMS\\nPrivay Attacks in LLMs\\nAttacks\\nGradint leakage\\nMembership\\nInference\\nAttacks\\nPII\\nLeakage\\nAttacks\\nExamples:\\n1. TAG* [55]\\n2. LAMP [14]\\n3. GDBA [81]\\nExamples:\\n1. MIA-ML* [239]\\n2. MIA on PLM [294]\\n3. PREMIA [65]\\n4. SaMIA [119]\\nExamples:\\n1. TAB* [102]\\n2. ProPILE[122]\\n3. PII Compass [178]\\n4. Memorization [23]\\n5. KART* [177]\\n*Some attacks were performed only on language models\\nFig. 8. Privacy Attacks in LLMs and Examples.\\nPrivacy risks in LLMs arise from their inherent\\ncapacity to process and generate text based on\\nextensive and diverse training datasets. These\\nmodels, like GPT-3, may inadvertently capture\\nand reproduce sensitive information that ex-\\nists in training data, potentially posing privacy\\nconcerns during the text generation process.\\nIssues such as unintentional data memoriza-\\ntion, data leakage, and the potential disclosure\\nof confidential information or PII are key chal-\\nlenges [189]. Fine-tuning LLMs for specific\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:19\\ntasks introduces additional privacy considerations. Making a balance between the utility of these\\npowerful LMs and the imperative to protect user privacy is very crucial for assuring the reliable\\nand ethical use of LLMs in various applications. In Figure 8, we show the categories of privacy\\nattacks in LLMs with some examples. Additionally, in Table 4, we briefly mention the LLM privacy\\nattack categories, source code, their characteristics, limitations and potential mitigation techniques,\\nin corresponding subsections, we further discuss it in detail.\\n5.1\\nGradient Leakage Attack\\nDeep learning models are often trained using optimization algorithms that involve gradients.\\nGradients represent the direction of the steepest increase in a function that helps to optimize model\\nparameters during training to minimize the loss function. If attackers can access or infer these\\ngradients or the gradient information, they may obtain access to the model or even compromise\\nits privacy and safety, e.g., reconstructing private training data [85]. Sensitive information can be\\nextracted by analyzing gradients during the training or manipulating the training data. Several\\nstudies [279], [331], [76], [52] have shown private training samples can be successfully reconstructed\\nfrom the deep learning model using gradients with high reconstruction accuracy under the Federated\\nLearning (FL) environment. However, those algorithms work mostly for image datasets. Very few\\nstudies have investigated the gradient leakage attacks for LMs. These are small LMs trained over\\nfar fewer parameters than LLMs, e.g., TinyBERT [112].\\nOne of the early explorations of gradient leakage attacks against LMs is gradient-based distribu-\\ntional attack (GBDA) which performs gradient-based text attacks on Transformers [80]. It leverages\\nparameterized adversarial distribution to enable gradient optimization to perform efficient gradient\\nleakage attacks instead of using a single adversarial instance. This attack is effective for different\\nLMs on distinct tasks. Deng et al. introduced a universal gradient attack on Transformer-based\\nLMs in the NLP domain, named TAG. TAG can reconstruct the private training samples, (𝑋,𝑌),\\nfrom Transformer-based LMs [55]. The TAG adversary acquires gradients ∇𝑊from a participant\\nin a distributed learning system, then updates randomly initialized dummy data (𝑋0,𝑌0) through a\\ncomparison of the difference between acquired gradients ∇𝑊(from clients) and the adversary’s\\ngradients ∇𝑊0. This is achieved by leveraging a loss function, e.g., L1-norm (Manhattan distance) or\\nL2-norm (Euclidean distance), along with a coefficient parameter 𝛼. Eventually, the adversary can\\naccess the client’s private information by recovering the private training data (𝑋,𝑌). Compared to\\nthe existing method [331], TAG operates on models that are initialized with more realistic and pre-\\ntrained weight distributions. According to Deng et al., TAG can effectively reconstruct up to 88.9%\\ntokens with 0.93 cosine similarity in token embeddings to the private training data [55]. The LAMP\\nattack is a technique designed for recovering input text from gradients in an Federated Leaning\\n(FL) environment proposed by Balunovic et al. [14]. It utilizes an auxiliary LM to guide the search\\nprocess toward generating natural text [14]. The attack employs a search procedure that alternates\\nbetween continuous and discrete optimizations, enhancing the efficiency and effectiveness of the\\noverall process. According to their experiments, LAMP performs better than previous methods\\n[55], reconstructing 5× more bi-grams and achieving an average of 23% longer subsequences\\n[14]. Additionally, their proposed approach was the first to successfully restore input data from\\nbatch sizes greater than 1. These aforementioned attack models primarily targeted the LMs [331],\\n[55], however they can also be potentially applied to LLMs. Another essential condition of the\\ngradient-based attack methods is they often need white-box access (e.g., gradients) to the model\\nfor performing attacks. Even with gradient access, attackers are often limited to reconstructing the\\ntraining data, sometimes only at the token level [81], rather than the training labels [14].\\nTo defend against gradient leakage attacks, noise perturbation to the gradients [331] and dif-\\nferential privacy (DP) [2] are very popular methods in the computer vision domain. Moreover,\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:20\\nDas, et al.\\nCategory\\nName\\nSource\\nCode\\nCharacteristics\\nLimitation\\nPotential\\nMitigation\\nGradient\\nLeakage\\nAttacks\\nTAG [55]\\nGitHub\\n- Performs gradient attack to recover the local\\ntraining data on Transformer-based language model.\\n- Impact: Moderate\\n- Adversary requires access to the\\ngradient of the Transformer model.\\n- Noise Perturbation [331].\\n- Differential Privacy [2].\\nLAMP [14]\\nGitHub\\n- Recovers training data from gradients in FL\\nenvironment using guided search with auxiliary LM.\\n- Impact: High\\n- Does not deal with reconstructing\\ntraining labels.\\n- Differential Privacy [2]\\nGDBA [81]\\nGitHub\\n- Introduced parameterized adversarial distribution\\nto perform gradient-based text-attack.\\n- The attack is transferable to other LMs and\\napplication for varieties of tasks.\\n- Impact: Moderate\\n- The method sis only able to recover\\ntokens.\\n- Robust optimization\\nin training so that\\nadversarial perturbation\\nhas minimum impact [313].\\nMembership\\nInference\\nAttacks\\nMIA\\non\\nPLMs\\n[294]\\nN/A\\n- Attacker collects a small number of training data\\nof the model which the PLMs have pre-trained\\non and trains the attack model with these instances.\\n- Designs a binary classifier (member or non-\\nmember), leads to MIA.\\n- Impact: Moderate\\n- Attack model stands on an unrealistic\\nassumption of having access to some\\nsample of training data.\\n- Adversarial regularization\\n[180].\\n- MemGuard [109].\\nPREMIA [65]\\nN/A\\n- Performs MIA based on preferred and non-\\npreferred responses against specific prompts in LLMs.\\n- Based on the response of the model the attacker\\ndetermines the membership of the training set.\\n- Impact: Moderate\\n- Attack was fully evaluated on open-\\nsource LLMs. The performance might\\nnot be same on close-source model\\nsuch as GPT-3.\\n- DP-SGD [2]\\n- Model pruning[86].\\n- Knowledge distillation [92]\\nSaMIA [119]\\nGitHub\\n- Introduces sampling-based pseudo likelihood (SPL)\\nmethod for MIA based on the n-gram similarity\\nbetween pre-fix (part of generated text) and\\nreference text (remaining part of the generated text).\\n- Impact: Moderate\\n- Attacks performed on LLMs which\\ntraining data is known.\\n- Performance might degrade on the\\nunknown data on which model has\\nbeen trained.\\n- To be studied.\\nPII\\nLeakage\\nAttacks\\nTAB [102]\\nN/A\\n- An evaluation technique if the PII can be leaked\\nunder any leaked under any threat model.\\n- Impact: Low\\n- The proposed method was evaluated\\non RNN and Transformer-based language\\nmodel, did not demonstrate applicability\\non LLM or other model architectures.\\n- DP-SGD [2]\\n- API Hardening [102]\\nPII\\nCompass [178]\\nN/A\\n- Proposed PII leakage method by prepending\\nhand-crafted template with true prefix subject\\nto different data in the adversary set.\\n- Impact: Low\\n- Limited to extracting only phone number\\ndue to the lack of publicly available PII\\nentries such as SSN.\\n- Adversarial Training [178]\\n- PII masking [157]\\n- DP-SGD [2]\\nProPILE [122]\\nN/A\\n- Introduced as a tool for risk assessment of PII\\nleakage from open pre-trained transformer\\nlanguage model.\\n- Based on the linkability of the given information\\nof asked query evaluates the risk of PII exposure.\\n- Impact: Moderate\\n- The performance highly depends on the\\nevaluation that is available on open-\\nsource datasets.\\n- The evaluation dataset might contain\\nnoisy or misleading information.\\n- Cleaning PII with structured\\npatterns with regular\\nexpression [122]\\nMemorization [23]\\nGitHub\\n- Demonstrated the PII leakage attack due to\\nmemorization in LLMs, i.e., GPT-2.\\n- The proposed attack efficiently extracts training\\nsamples using certain prefixes by black-box query\\naccess\\n- Impact: Low\\n- The number of successful extraction\\nof PII are extremely low.\\n- The attack was evaluated on GPT-2,\\nit may not perform well on the latest LLms,\\ne.g., Llama, GPT-4.\\n- Training Data curation[45].\\n- DP-SGD [2]\\n- Prompt-tuning [188]\\nKART [177]\\nGitHub\\n- Defines four factors that contributes to a privacy\\nleakage in PLMs, i.e., attacker’s prior knowledge\\nabout the model, the target information to be leaked,\\nresources to attack, and the availability of the\\ntarget information in pre-trained data. [122]\\n- Impact: Low\\n- It did not provide any universal attack\\nmethods in which all the factors contribut\\ned to execute a successful attack.\\n- The impact on attack performance after\\napplying DP was not evaluated.\\n- Differential Privacy [2]\\n- Limiting attacker access to\\ntraining data.\\nTable 4. The categories of LLM privacy attacks, source code, their basic characteristics, limitations, and\\npotential mitigation techniques. (This table is discussed in detail in the corresponding parts of Section 5 and\\n6)\\nsuccessfully executing an attack against a well-trained model often requires specific settings, e.g.,\\nloss function and optimizer [76]. However, the evaluation of these defense methods has yet to be\\nthoroughly evaluated in the LLM context. Thus, it remains uncertain whether these methods will\\nperform as effectively in LLMs as they do in other domains, such as computer vision. In order to\\novercome these shortcomings, it is imperative to conduct additional comprehensive research and\\nanalysis to assess the impacts of these attacks on LLMs and identify effective mitigation strategies.\\n5.2\\nMembership Inference Attack\\nThe primary goal of a Membership Inference Attack (MIA) is to determine if a data sample has\\nbeen included in an ML model’s training data [62], [303]. The attackers can execute MIAs even\\nin the absence of direct access to the underlying ML model parameters, relying solely on the\\nobservation of its output [59]. Typically, such attacks take advantage of models’ tendency to overfit\\ntheir training data, resulting in lower loss values for training samples [217]. The LOSS attack is\\na straightforward baseline where the basic idea is if their loss values are less than a specified\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:21\\nthreshold, then it considers samples as training members [308]. The confidentiality of the data used\\nin the model’s training process is called into question by the identification of data used for training\\nusing membership inference. These type of attacks poses serious privacy concerns, particularly in\\nscenarios where the targeted model has undergone training on sensitive information, e.g., medical\\nor financial data [256]. Shokri et al. first introduced MIA against ML Model (MIA-ML) [239]. The\\nattack model is trained through their proposed shadow training technique: First, several “shadow”\\nmodels are constructed to mirror the behavior of the target model, where the training dataset is\\nknown, and thus so is the ground truth about the membership. Then, the attack model is trained on\\nthe labeled (member/non-member) inputs and outputs from the shadow models to classify whether\\na given sample is a member of the training data or not.\\nFollowing the formal description of Shokri et al. [239], given an attack target model 𝑓target(),\\nits private training dataset 𝐷train\\ntarget contains labeled training samples (𝑥𝑖,𝑦𝑖)target ∈𝐷train\\ntarget. Here,\\n𝑥𝑖\\ntarget represents the model input, and 𝑦𝑖\\ntarget is the ground truth label, taking a value from a set of\\n𝑐target classes. The target model will output a probability vector 𝑌𝑖of size 𝑐target as the prediction\\non input 𝑥𝑖. 𝑓attack() is the attack model, which is a binary classifier to determine whether a given\\nsample (𝑥,𝑦) is in the private training dataset (“in”) or not (“out”). It is challenging to distinguish\\nbetween members and non-members in the private training dataset. Moreover, this task becomes\\nincreasingly difficult when the attacker has limited information about the internal parameters of\\nthe target model and can only access it through public APIs with a limited number of queries.\\nMIA-ML [239] leverages shadow models to implement the attack model. Each shadow model\\n𝑓𝑖\\n𝑠ℎ𝑎𝑑𝑜𝑤() will be trained on a dataset 𝐷train\\nshadow𝑖, which is similar in terms of format and distribution\\nto, but disjoint from the private training dataset 𝐷train\\ntarget, i.e., 𝐷train\\nshadow𝑖∩𝐷train\\ntarget = ∅. Once all\\n𝑘shadow models are trained, the attack training set 𝐷train\\nattack can be generated by following (1)\\n∀(𝑥,𝑦) ∈𝐷train\\nshadow𝑖, obtain the prediction vector (output) 𝑌= 𝑓𝑖\\nshadow(𝑥) and include the record\\n(𝑦,𝑌, in) in 𝐷train\\nattack, and (2) query the shadow model with a test dataset 𝐷test\\nshadow𝑖disjoint from\\n𝐷train\\nshadow𝑖, then ∀(𝑥,𝑦) ∈𝐷test\\nshadow𝑖, generate the prediction vector (output) 𝑌= 𝑓𝑖\\nshadow(𝑥) and add\\nthe record (𝑦,𝑌, out) to 𝐷train\\nattack. 𝐷train\\nattack will be divided into 𝑐target partitions, where each partition is\\nlinked to a distinct class label. For each class label 𝑦, an individual attack model will be trained to\\npredict the membership status “in” or “out” for a given input (𝑥,𝑦).\\nExisting privacy attacks against LMs for MIA are mainly focused on text generation and down-\\nstream text classification tasks [244], [233]. Xin et al. took the first initiative to perform a systematic\\naudit of the privacy risks associated with pre-trained language models (PLMs) by focusing on the\\nperspective of MIA [294]. They have shown how an adversary seeks to determine if a data sample\\nbelongs to the training data of PLMs in the practical and prevalent situation where downstream\\nservice providers often construct models derived from four different PLM architectures (BERT,\\nALBERT, RoBERTa, and XLNet). The assumption is that the adversaries acquire access only to these\\ndownstream service models deployed online. Additionally, they considered another more realistic\\nscenario where no additional information about the target PLMs is available to the adversary other\\nthan the output, i.e., black-box setting. Most existing attacks in the literature rely on the fact that\\nmodels often assign their training samples with higher probabilities than non-training instances.\\nHowever, this approach tends to result in high false-positive rates as it overlooks the inherent\\ncomplexity of a sample. For training the attack models [239], attacks of this type are based on a\\nhighly optimistic and arguably unrealistic assumption in many cases that an adversary knows the\\ndistribution of training data of the target model [307]. Mattern et al. proposed a neighborhood attack\\nthat relies on the concept of using neighboring samples, generated through data augmentations like\\nword replacements, as references for inferring membership, which aims to develop a metadata-free\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:22\\nDas, et al.\\nmechanism [161]. Mireshghallah et al. reported that previous attacks on MLMs ([121], [150]) may\\nhave yielded inconclusive results due to their exclusive reliance on the loss of the target model\\non individual samples for the evaluation of how effectively the model memorized those samples\\n[172]. In these approaches, if the loss falls below a certain threshold, the sample is designated as a\\npotential member of the training set. As a result, it may give only a limited discriminative indication\\nfor membership prediction. Unlike prior works, it introduced a systematic framework to assess\\ninformation leakage in MLMs using MIA with likelihood ratio-based membership, and it conducted\\na comprehensive investigation on memorization in such models. Conventionally, the attacks on\\nnon-probabilistic models become possible when MLMs are treated as probabilistic models over\\nsequences. The attack method was evaluated on a collection of masked clinical language models\\nand compared its performance against a baseline approach that completely relies on the loss of the\\ntarget model, as established in previous works ([308], [243]). Some works investigated MIA methods\\nin LMs on specific domains, e.g., clinical language models. Jagannatha et al. investigated the risks\\n[103] of training-data leakage to estimate the empirical privacy leaks for model architectures such\\nas BERT [57] and GPT-2 [204]. Kaneko et al. proposed a sampling-based pseudo-likelihood (SPL)\\nmethod for MIA. They call it SaMIA [119]. It can detect whether a given text is included in the\\ntraining data of an LLM without requiring access to the model’s likelihood or loss values. SaMIA\\ngenerates multiple text samples from the LLM during testing. It then calculates the n-gram overlap\\nbetween the samples and the target text, using this overlap as a proxy to estimate the likelihood\\nof the target text. If the average n-gram overlap between the samples and the target text exceeds\\na certain threshold, SaMIA identifies the target text as part of the LLM’s training data. Feng et\\nal. introduced a novel reference-based attack framework, PREMIA [65] (Preference data MIA) to\\nanalyze the vulnerability of using preference data in LLM alignment to membership inference\\nattacks (MIAs). It specifically targets three distinct attack scenarios: 1) attacking prompts and\\npreferred responses, 2) attacking prompts and non-preferred responses, and 3) attacking the entire\\npreference tuple. Based on the response of the model, the attacker determines the membership of\\nthe target text in the training set.\\nSeveral limitations exist in MIA against LLMs. The concept of MIA fundamentally relies on the\\nassumption that the attacker has white-box access to the model and training data. In practical\\nscenarios, this assumption may be somewhat unrealistic. Additionally, SaMIA [119] was evaluated\\nonly on a single task, classification, and its performance on other tasks, such as translation or\\ntext generation, may not achieve similar effectiveness. Moreover, the method was tested on public\\ntraining data WikiMIA [237]. In real-world applications, LLM training data may not always be\\npublicly accessible, and SaMIA’s performance may deteriorate if the victim model is trained on\\nunpublished data. On the other hand, PREMIA [65] was evaluated on open-source LLMs, and its\\nperformance may not be consistent on closed-source models such as GPT-3.\\nIn general, MIA can be defended in different phases of the target model, such as the pre-training\\nphase, training phase, and inference phase. Various technologies to defend the LMs against MIA\\nhave been proposed, such as regularization, transfer learning, and information perturbation [95],\\n[118]. However, it is not sufficient from all the perspectives of MIA in LMs. The aforementioned\\nattack models mostly focused on LMs. Potentially, those can be applied to the LLMs as well. If so,\\nfurther extensive research and study are needed to evaluate the severity of the attacks on LLMs\\nand the way to mitigate them.\\n5.3\\nPII Leakage Attack\\nPII, refers to data that, either alone or in combination with other information, can uniquely identify\\nan individual [70]. PII encompasses direct identifiers like passport details and quasi-identifiers such\\nas race and date of birth. Sensitive PII includes information like name, phone number, address, social\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:23\\nsecurity number (SSN), financial, and medical records, while non-sensitive PII, is readily available\\nin public sources, such as zip code, race, and gender. Numerous perpetrators acquire PII from\\nunwitting victims by sifting through discarded mail in their trash, potentially yielding details like an\\nindividual’s name and address. In certain instances, this method may expose additional information\\nrelated to employment, banking affiliations, or even social security numbers. Phishing and social\\nengineering attacks [84] leverage deceitful websites or emails, employing tactics designed to deceive\\nindividuals into disclosing critical details such as names, bank account numbers, passwords, or\\nSSNs. Additionally, the illicit acquisition of this information extends to deceptive phone calls or SMS\\nmessages. In LLMs, PII leakage has been a fundamental problem. In March 2023, it was reported\\nthat ChatGPT leaked users’ conversation history as well as information related to payment due to\\na bug in the system [124]. Evidence has been found on leaking information through sentence-level\\nMIA [239] and reconstruction attacks on private training data [326]. One of the first studies of\\nPII leakage was proposed by Inan et al. named TAB attack [102]. Their approach investigated\\nwhether the model could reveal user content from the training set when it was presented with the\\nrelevant context. They also proposed evaluation metrics that can be employed to assess user-level\\nprivacy leakage. After that, Lukas et al. empirically demonstrated that their attack method against\\nGPT-2 models can extract up to 10× more PII sequences than TAB attack. They also showed that\\nalthough sentence-level differential privacy lowers the likelihood of PII leakage, around 3% of PII\\nsequences are still leaked. PII reconstruction and record-level membership inference were shown\\nto have a subtle relationship [152]. Zanella et al. [312] explored the impact of updates on LMs by\\nanalyzing snapshots before and after an update, revealing insights into changes in training data.\\nTwo metrics were introduced by them, differential score, and differential rank, to assess data leakage\\nin natural language models, which includes a privacy analysis of LMs trained on overlapping data,\\ndemonstrating that adversaries can extract specific content without knowledge of training data\\nor model architecture. ProPILE was introduced as a tool for PII leakage in LLM experimented on\\nopen pre-trained transformer language models (OPT-1.3B model [314]). It will ask for a specific\\nPII, e.g., personal contact number or SSN to the designed LLM prompt by providing associated\\ninformation. Then the LLM prompt will provide the asked information based on the likelihood\\nof that given information formulated by linkability of the given information and structure of the\\nasked information [122]. Researchers addressed the PII learning tasks of LLMs and showed that\\nforgotten PII might be retrieved by fine-tuning using a few training instances [33]. Considering\\nsome primary factors of privacy leakage in PLMs, a universal framework named KART has been\\nintroduced specifically for the biomedical domain [177]. Nakka et al. proposed PII leakage attack,\\ntermed PII-compass [178] by adding hand-crafted template to an attack prefix. These attack prefixes\\nare associated with a different data subject from the adversary set, meaning the data (attack prefix)\\nused to create the new prompt originates from a different data subject whose PII is intended for\\nextraction. Memorization is another aspect of PII attacks. Carlini et al. demonstrated that LLMs\\nmemorize and leak individual training examples [23]. Additionally, it shows how a malicious\\nparty may query the LM in order to execute a training data extraction attack and get specific\\ntraining samples (GPT-2). It showed that LLMs memorize and leak individual training examples.\\nA straightforward approach was proposed to use only black-box query access, where verbatim\\nsequences (as exactly they appeared in the training set) were extracted from a language model’s\\ntraining set [23]. It can be directly applied to any LM trained on non-public and sophisticated\\ndata. The GPT-2 model released by OpenAI has been a representative LM used in the experiments.\\nFurthermore, several investigations revealed that PLMs have a high probability of disclosing private\\nand confidential data. In particular, when it asks PLMs for email addresses along with email address\\ncontexts or asks for prompts that include the owner’s name. According to the studies, PLMs retain\\npersonal data, which means that the data may be retrieved using a certain prefix, such as training\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:24\\nDas, et al.\\ndata tokens. PLMs link the owner of the personal information to it, thus attackers may query the\\ndata using the owner’s identity [98].\\nCertain shortcomings exist in the PII leakage attacks prevalent in the literature. For instance,\\nHuang et al. primarily focused on extracting email addresses as representatives of personal infor-\\nmation in their proposed memorization attack in PLM [98]. On the contrary, PII-compass [178]\\nis limited to only extracting phone numbers. Complete extraction of a full PII set still requires\\nsignificant exploration and proper methods. While some methods have been evaluated only on\\nsmall Transformer-based language models [102], others have been tested on earlier LLMs such as\\nGPT-2 [23]. The performance of these methods (e.g., [98, 102]) on the latest LLMs, such as Llama-3\\nand GPT-4, still requires in-depth evaluation. Moreover, some of these methods may not perform\\nwell in the presence of defense mechanisms such as differential privacy (DP) [2]. Therefore, further\\nresearch is needed to develop attack methods that address the aforementioned issues in PII leakage\\nattacks against LLMs.\\n6\\nDEFENSE MECHANISMS\\nAs LLMs become integral components in applications ranging from NLP to multi-modal systems,\\nthe vulnerabilities associated with their usage pose serious concerns. Protecting LLMs from security\\nand privacy attacks is imperative to preserve the reliability and integrity of this complex system\\n[185]. We argue that robust defense strategies should be developed to safeguard LLMs from security\\nand privacy perspectives. In this section, we review research studies to mitigate the vulnerabilities\\nof LLMs to defend against emerging security and privacy threats.\\n6.1\\nDefense Against Security Attacks on LLMs\\nDefense Against Prompt Injection. Limited studies explored the defense strategies to defend\\nthe prompt injection attacks in LLMs. A prevention-detection-based defense technique has been\\nreported to systematically present existing defense mechanisms against prompt injection attacks\\n[146]. Prevention-based defenses, as outlined in [200] and [104], are designed to thwart the suc-\\ncessful execution of tasks injected into an LLM-integrated application. These preventive measures\\ninvolve pre-processing the data prompt to eliminate the injected task’s instruction/data, and/or\\nredesigning the instruction prompt itself. To thwart the adversarial prompts there are several\\ntechniques, e.g., paraphrasing [104], re-tokenization [104], data prompt isolation, and instructional\\nprevention [219–221]. It has been noted that paraphrasing would disrupt the sequence of injected\\ndata, such as injected instruction, and special character insertion. The efficacy of prompt injection\\nattacks would be diminished by this disruption. Re-tokenization aims to break the sequence of in-\\njected instructions, task-ignoring text, special characters, and fake responses within a compromised\\ndata prompt. This process preserves frequently occurring words while breaking down infrequent\\nones into multiple tokens. Consequently, the re-tokenized output comprises more tokens than\\na typical representation. This re-tokenized data prompt and the instruction prompt are used by\\nthe LLM-Integrated application to query the LLM and generate a response. Defenses based on\\ndetection are focused on determining the integrity of a given data prompt [104], [225], [241], [269].\\nNotably, the proactive detection method [225] has proven effective in identifying instances of\\nprompt injection attacks. Defenses based on detection can further be classified into two categories:\\nresponse-based detection, and prompt-based detection. A response-based detection method exam-\\nines the response of LLMs, while a prompt-based detection approach examines a provided data\\nprompt. Perplexity-based detection is a kind of prompt-based detection. The basic idea is that\\nadding information or instructions to a data prompt degrades its quality and leads to increased\\nperplexity. Consequently, a data prompt is considered compromised if its perplexity exceeds a\\nspecified threshold [78]. Since an LLM-integrated application is tailored for a specific task, granting\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:25\\nit prior knowledge of the anticipated response, detecting a compromised data prompt is feasible\\nwhen the generated response deviates from a proper answer for the desired task [225]. For example,\\nif the desired task is spam detection and the response does not align with “spam” or “non-spam”,\\nthey imply a compromise. Notably, this defense has a limitation—it is ineffective when the injected\\ntask and desired task share the same type, such as both being related to spam detection. Effective\\nprotection strategies against P2SQL injection attacks are available and may be incorporated into\\nthe LangChain framework as extensions [193]. For example, database permission hardening, since\\nP2SQL injection attacks can manipulate chatbots by arbitrarily executing different queries [193],\\nincluding deleting data, utilizing database roles and permissions to limit the execution of undesired\\nSQL statements when accessing tables with sensitive information can be a viable technique to\\ndefend such attacks. It can mitigate arbitrary access by rewriting the SQL query output by LLM into\\na semantically equivalent one that exclusively operates on the information the user is authorized\\nto access [193]. Auxiliary LLM Guard is another way to mitigate the P2SQL attacks. The malicious\\ninput comes from the user’s logged-in chatbot to manipulate the SQL query created by LLM in\\ndirect attacks. Conversely, indirect attacks involve malicious input residing in the database, enabling\\ninterference with LLM-generated SQL queries and potentially undermining the effectiveness of\\nthese defenses. The execution flow with the LLM guard comprises three steps: (i) the chatbot\\nprocesses user input and generates SQL; (ii) the SQL is executed in the database, and the results\\nundergo inspection by the LLM guard; and finally, (iii) if suspicious content is identified, execution\\nis halted before LLM accesses the results. The LLM receives clean results that are free from prompt\\ninjection attacks and may run without interruption. Liu et al. performed an empirical study to\\ncomprehensively formalize and benchmark several prompt injection attacks and defenses [148]. In\\ntheir study, they found the prevalent prevention-based and detection-based defense techniques are\\ninsufficient to mitigate the risks of advanced optimization-based attacks, e.g., JudgeDeceiver [236].\\nDefense Against Jailbreaking Attacks. Several defense methods have been proposed to safeguard\\njailbreaking attacks in LLMs. As a built-in safety mechanism, pre-processing-based techniques,\\ndetecting and blocking the inputs or outputs, and semantic content filtering have been employed\\nto prevent generating undesired or inappropriate contents from LLMs, which could effectively\\nmitigate potential harm [159]. Kupmar et al. [125] proposed an approach to apply a safety filter on\\nthe sub-strings of input prompts, which provides certifiable robustness guarantees. The drawback\\nof this approach lies in the method’s complexity, which increases proportionally with input prompt\\nlength. Wu et al. [285] propose a system-mode self-reminder to defend against jailbreaking attacks\\nunder the pretending or role-playing scenarios, which can drastically reduce the jailbreaking suc-\\ncess rate from 67.21% to 19.34%. It is a technique to assist ChatGPT in remembering or focusing\\non particular actions, ideas, or behaviors when it is asked to generate inappropriate content [285].\\nOne potential simple defense strategy is to identify the presence of “red-flagged” keywords [276]\\nwhich strictly violates the usage policies of the LLM vendors, e.g., OpenAI [54]. Recently, Zhang et\\nal. proposed a goal prioritization technique that prioritizes generating harmless responses over\\nhelpful ones at the inference phase [320]. This technique can significantly reduce the success rate\\nof jailbreaking attacks in ChatGPT and Llama-2. However, these basic defense mechanisms may\\nnot be sufficient to prevent jailbreaking attacks with carefully crafted tricky prompts, e.g., privilege\\nescalation. Furthermore, preventing these attacks still poses a significant challenge because the\\neffective defenses may impair model utility [135]. By far, SmoothLLM is an effective defense strategy\\nagainst existing jailbreaking attacks proposed by Zou et al. [334]. It can be applied for mitigating\\nthe instruction-based attacks, e.g., AutoDAN [144], DAN [234], and PAIR [26]. The fundamental\\nconcept of SmoothLLM is partially inspired by the randomized smoothing within the adversarial\\nrobustness community [44]. It involves a two-step process. First, copies of a specified input prompt\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:26\\nDas, et al.\\nare duplicated and perturbed. Subsequently, the outputs produced for each perturbed copy are\\naggregated. The desiderata encompass four key properties: attack mitigation (reduces attack success\\nrate 100 times and 50 times for Llama-2 and Vicuna respectively), non-conservatism, efficiency (in\\nterms of computational resources), and compatibility (different LLMs). For preventing the advanced\\nattack methods, such as multi-step jailbreaking (MSJ) [132], and MasterKey [54], unsafe prompt\\ndetection and filtering technique may effectively refuse to provide the inappropriate response\\nintended by the adversary. On the other hand, self-reminder-based methods [288] might create\\nrobust defense against more complex attacks such as DeepInception [137]. However, the efficacy\\nof these mitigation techniques has not yet been evaluated based on these specific attack methods\\nmentioned. Meta introduced Llama Guard, an input-output tool to safeguard LLMs [100], designed\\nto effectively mitigate jailbreaking attacks. Llama Guard incorporates a safety risk taxonomy and\\nthe applicable policy for data collection and training the tool. These properties address the dis-\\ntinctive challenges associated with safeguarding LLMs against jailbreaking attacks. The proposed\\nperturbation function can further be optimized over various operations e.g., insertion and swaps\\nto make stronger defenses. To defend the multi-modal prompts against jailbreaking attacks, Qi et\\nal. recently proposed DiffPure [184], a diffusion model-based countermeasure against the visual\\njailbreaking examples.\\nDefense Against Backdoor Attack. Most of the existing research, such as the removal of back-\\ndoors by fine-tuning [227], model pruning [143], and detecting backdoors by inspecting activations\\n[27] are based on backdoor defenses in the white-box setting. Fine-mixing is a mitigation approach\\ndesigned to prevent backdoors in fine-tuned LMs. It utilizes pre-trained weights through two\\ncomplementary techniques: (i) a two-step fine-tuning procedure that first combines backdoored\\nweights that have been optimized using pre-trained weights on poisoned data, and then refines\\nthe combined weights on a small collection of clean data; (ii) an Embedding Purification (E-PUR)\\nmethod, addressing potential backdoors in word embeddings [319]. A distinct pattern of poisoned\\nsamples demonstrated a tendency to aggregate and form identifiable clusters separate from those of\\nnormal data. Building upon this observation, a defense technique named CUBE has been proposed\\n[49]. It utilized a density clustering algorithm called HDBSCAN to accurately discern clusters\\nwithin datasets and distinguish the poisoned samples from clean data. By leveraging the capabilities\\nof HDBSCAN [164], CUBE aims to provide an effective means of differentiating clusters associated\\nwith both normal and poisoned data. Strategies to defend the backdoor attacks in the black-box\\nsetting are still lacking [117]. Perturbation-based and perplexity-based defense methods are also\\nadopted in the literature [176, 201, 302], e.g., RAP which leverages word-based Robustness-Aware\\nPerturbation (RAP) to identify poisoned samples [302] and ONION which eliminates trigger words\\nvia empirical analysis of sentence perplexities [201]. These two methods can be applied to de-\\nfend against manually designed backdoor triggers such as ProAttack [321]. Masking-Differential\\nPrompting (MDP) serves as an efficient, lightweight, and adaptable defense method against back-\\ndoor attacks in prompt-based language models (PLMs), particularly in few-shot learning settings\\n[291]. MDP exploits the observation that poisoned samples exhibit increased sensitivity to random\\nmasking compared to clean samples. When the trigger is (partially) masked, the language mod-\\neling probability of a poisoned sample tends to exhibit significant variations. MDP introduces a\\nchallenging dilemma for attackers, forcing them to weigh the trade-off between attack efficacy and\\nevasion of detection. However, MDP falls short on several other PLMs (e.g., GPT-3 [20]) and NLP\\ntasks (e.g., paraphrases and sentence similarity [74]). Moreover, further studies can be performed to\\nevaluate the performance of random masking-based defense in PLMs when the available data are\\neven scarcer (e.g., one or zero-shot settings [260], [268]). Again, MDP was proven to be effective\\nfor the earlier backdoor attacks, however, it might not safeguard some advanced attacks such\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:27\\nas BadPrompt [21] and BToP [295]. Leveraging techniques such as knowledge distillation [92],\\noutlier-filtering [295], and fine-pruning [143] requires further exploration to mitigate the impacts\\nof backdoor attacks, such as BadPrompt [21], and BTop [295]. In order to defend against clean label\\nbackdoor triggers, such as LLMBkd [309], a trigger classification approach called REACT [309] has\\nperformed significantly better than ONION [201] and RAP [302]. For the reasoning-based backdoor\\nattacks, e.g., BadChain [292], shuffling model input might be a potential defense mechanism. Addi-\\ntionally, backdoors detection [293] (CBD), including backdoor patterns in training data to ensure\\nrobustness [275] (RAB) can be used to prevent backdoor attacks. Detecting anomalies in input data\\nand parameter decontamination are also used to prevent the attacks executed during the training or\\nfine-tuning phase, e.g., BadAgent [272]. Though these defense techniques are primarily proposed\\nfor detecting backdoors and mitigating the effects of attacks, most of them are evaluated either on\\nLMs or in other domains, e.g., computer vision. The impact of these mitigation techniques has not\\nyet been evaluated on recently evolved LLMs, e.g., GPT-4 and Llama-3.\\nDefense Against Data Poisoning Attack. Few solutions exist to defend against poisoning attacks\\nin LLMs. In general, techniques such as data validation, filtering, cleaning, and anomaly detection\\nhave been used to protect ML models from poisoning attacks [191]. A detection and filtering\\napproach was designed to identify and filter poisoned data which is collected for performing\\nsupervised learning [15]. Empirical assessments have demonstrated that limiting the number of\\ntraining epochs is a straightforward method for LMs to reduce the impact of data poisoning such as\\nRoBERTa [262]. Identifying poison examples using perplexity can be another technique for small\\nGPT-2 model [204] for sentiment analysis tasks. However, it may not identify poisons effectively,\\nspecifically, after inspecting the training data, less than half of the poisoned examples can be\\nidentified. Identifying these poisoned examples by BERT embedding distance is another method for\\ndefending this attack [262]. Filtering poisoned samples during training is also used in LMs to defend\\nagainst data poisoning attacks [263]. Yan et al. proposed a framework called ParaFuzz for poisoned\\nsample detection for NLP models that leverages the interpretability of model predictions [298].\\nIt adopts a software testing technique called fuzzing to distinguish poisoned samples from clean\\nsamples. The poisoned data points are often outliers in the training data distribution. Compared to\\nbenign training data, a model requires more time to learn the features of poisoned data. To defend\\nagainst trojaning attacks in LMs, like, TROJANLM [316], one possible defense technique is to adopt\\nexisting techniques from other domains such as images, e.g., STRIP [75], detecting trigger-embedded\\ninputs at inference phase [27], [39], and finding suspicious LMs and retrieving triggers during the\\nmodel evaluation phase [264], [31]. Dataset curation techniques, such as removing near-duplicate\\npoisoned samples, known triggers and payloads, and identifying anomalies, can help defend against\\nthe attacks performed by manually inserted poisoned samples in training data [45]. These methods\\nare potentially effective against poisoning attacks, e.g., you auto-complete me [222], AutoPoison\\n[240], and TrojanPuzzle attack [5]. Fine-pruning can also be used as a defense against these attacks\\n[143]. For the white-box attacks such as AgentPoison [34], perplexity filtering and query rephrasing\\n[125] are utilized in the literature. On the other hand, advanced attacks, such as NightShade [18],\\nneed advanced defense methods.\\nThe aforementioned defense strategies are mostly designed for LMs, but some of them can be\\npotentially applied to LLMs as well, however, it still lacks in-depth research studies to develop\\nefficient defense techniques to protect LLMs from data poisoning attacks. Moreover, empirical\\nreports have shown that LLMs are becoming more vulnerable to data poisoning attacks, where\\ndefenses based on filtering data or lowering model capacity only offer minimal protection at the\\ncost of reduced test accuracy [263]. Therefore, it requires effective defense methods that can make\\ntrade-offs between model utility and the capability of protecting LLMs from data poisoning attacks.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:28\\nDas, et al.\\n6.2\\nDefense Against Privacy Attacks on LLMs\\nDefense Against Gradient Leakage Attack. There are several mitigation strategies to defend\\nagainst those gradient-based attack methods, e.g., TAG [55] and LAMP [14]. They are random noise\\ninsertion to the gradients [279], differential privacy (DP) [2, 77], and homomorphic encryption\\n[12]. DP is the most common and effective technique to mitigate the effect of gradient leakage\\nattacks in DNNs. The fundamental idea of using DP is to add a controlled amount of noise to the\\nmodel updates during training, which limits the model’s ability to memorize and reproduce original\\nsequences from the training data [102]. Building upon prior research on vision model attacks [331],\\n[279], the defense mechanisms involving the addition of Gaussian or Laplacian noise to gradients\\nand DP-SGD coupled with additional clipping [2] can form an effective defense against gradient\\nleakage attacks. For example, DP can efficiently mitigate the impact of TAB [102] by reducing the\\nnumber of unique training sequences leaked by Transformer-based language models. TAB depends\\non black-box access to the model, i.e., the model’s top-k predictions at each token position given\\nan input prefix. Ensuring no access to the model’s underlying probability distributions through\\nAPI hardening techniques [102] may potentially mitigate the impact of such attacks. Previous\\nstudies [76] have demonstrated that the success of gradient leakage attacks is highly dependent\\non the attacker’s ability to solve the gradient optimization problem over a loss function under\\nthe condition of non-zero gradients. Moreover, the attack strategy for successfully reconstructing\\nprivate training data may differ between well-trained models and inadequately trained models.\\nAn effective mitigation technique could involve robust model optimization during training so that\\nadversarial perturbations have minimum impact [313].\\nHowever, there is a significant shortcoming of using DP in model training. It may sacrifice\\nthe model’s utility to a certain extent. Trading-off preserving privacy and model utility may rise\\nsignificant challenge for preventing gradient leakage attacks LLMs. Prior works explored various\\ntechniques, such as [286], [205], and [98] to defend against gradient leakage attacks in the language\\ndomain for small NLP models. Further research is required to develop defense mechanisms against\\ngradient leakage attacks on LLMs.\\nDefense Against Membership Inference Attack. In order to mitigate MIA in the language\\ndomain, several mechanisms are proposed, including dropout, auto de-identification [257] model\\nstacking, differential privacy [2], and adversarial regularization [180]. Salem et al. came up with\\nthe first effective defense mechanism against MIA [218]. Their approach included dropout and\\nmodel stacking. In each training iteration of a fully connected neural network model, dropout\\nis defined as the random deletion of a certain proportion of neuron connections. It can mitigate\\noverfitting in DNNs, which is a contributing factor to MIA [218]. However, this technique works\\nonly when a neural network is targeted by the attack model. To work with other target models,\\nthey proposed another defense technique referred to as model stacking. The idea behind this\\ndefense is if distinct parts of the target model undergo training with different subsets of data, the\\noverall model is expected to exhibit a lower tendency of overfitting. It can be achieved through\\nthe application of model stacking, one of the popular ensemble learning techniques. To defend\\nagainst black-box MIA on ML models, Jia et al. proposed MemGuard [109] which is essentially a\\nnoise perturbation mechanism for the predicted confidence score of the target model. It makes\\ndifficult for the attacker to infer whether a sample was part of the training data. However, this\\ntechnique has been evaluated for image and numeric datasets. The performance of this technique\\non LMs/LLMs still remains unexplored, which necessitates further studies to evaluate whether\\nit can effectively defend against MIA for LM/LLMs. Differential privacy (DP) based techniques\\nare also widely used to prevent privacy leakage by MIA [161], [136]. It includes data perturbation\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:29\\nand output perturbation [95]. Models facilitated with differential privacy employed with the sto-\\nchastic gradient descent optimization algorithm [61] can reduce empirical privacy leakages while\\nensuring comparable model utility in the non-DP environment [103]. Model pruning [86], and\\nknowledge distillation [92] are employed to mitigate the impacts of preference-based MIA, such\\nas PREMIA [65]. A recent framework, InferDPT [254], has been proposed to leverage black-box\\nLLMs to facilitate privacy-preserving inference, which effectively integrates DP in text generation\\ntasks. Another defense method against MIAs is to include regularization during the training of the\\nmodel. Regularization refers to a set of techniques used to prevent overfitting and improve the\\ngeneralization performance of an ML model. Label smoothing [250] is one kind of regularization\\nmethod that prevents overfitting of the ML model, which contributes to MIA [308]. Very few defense\\ntechniques have been proposed for LLMs [72], [136]. Most of the existing defense techniques have\\nbeen experimented on relatively small LMs, such as test classifiers [271], which are not evaluated\\nfor LLMs. Moreover, DP-based defenses may impair model utility. We argue that there is a pressing\\nneed for further research studies to develop effective defense techniques against MIA on LLMs.\\nDefenses Against PII Leakage Attacks. To mitigate the leaking of personal information from\\nPLMs due to memorization [23], there are several general techniques. During the pre-processing\\nphase, the process of de-duplication and training data curation [45] has the potential to significantly\\ndecrease the amount of memorized text in PLMs. Consequently, this results in a reduction of stored\\npersonal information within these models [130]. Prompt-tuning can also be a potential mitigation\\nagainst memorization. The basic idea is to optimize prompts in the attack environment to evaluate\\nthe capability to extract the memorized content in a target model. DP can efficiently mitigate the\\nimpact of TAB [102] and KART [177] by minimizing the number of unique training sequences leaked\\nby Transformer-based language models. Both TAB and KART require the back-box access of the\\nmodel, such as top-k predictions; therefore, restricting the model access by imposing API hardening\\ntechnique [102] can also be a potential mitigation technique for these kinds of attacks. DP can also\\nmitigate the attacks that utilize hand-crafted prompts with true prefix [178]. PII masking [157] can\\nalso be a viable technique to defend against PII leakage attacks. Personal information identification\\nand filtering methods, such as [45] and [211], may effectively reduce the number of training data\\nsamples extracted through PII leakage attacks, e.g., ProPILE [122]. However, manually checking\\nthe vast training data for LLMs is challenging and labor-intensive. De-duplication at the document\\nor paragraph level is common but may not eliminate repeated occurrences of sensitive information\\nwithin a single document. Advanced strategies for de-duplication and careful sourcing of training\\ndata are essential. Despite sanitization efforts, complete prevention of privacy leaks is challenging,\\nmaking it a first line of defense rather than a foolproof measure. In training, following the process\\nof Carlini et al. [23] and the implementation by Anil et al. [11], the deferentially private stochastic\\ngradient descent (DP-SGD) algorithm [2] can be employed to ensure privacy of training data during\\nthe training process [23], [101]. However, the DP-SGD-based method might not work efficiently as\\nit has a significant computational cost and decreases the trained model utility [312]. PII scrubbing\\nfilters dataset to eliminate PII from text [56], such as leveraging Named Entity Recognition (NER)\\n[127] to tag PII. Even though PII scrubbing methods can mitigate PII leakage risks, they face two\\ncritical challenges [152]: (1) the effectiveness of PII scrubbing may be reduced to preserve the\\ndataset utility, and (2) there is a risk of PII not being completely or accurately removed from the\\ndataset. In downstream applications like dialogue systems [318] and summarization models [93],\\nLMs undergo fine-tuning on task-specific data. While this process may lead to the LM “forgetting“\\nsome memorized data from pre-training [163], [210], it can still introduce privacy leaks if the\\ntask-specific data contains sensitive information.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:30\\nDas, et al.\\nThe aforementioned defense techniques mostly apply to the LMs. To date, we have not yet\\nidentified specific techniques that are dedicated to the LLMs. While according to some recent\\nstudies ([131], [306]), the existing strategies can be used in the LLM context as well. So far, there\\nis a pressing need for more empirical evaluations to determine their effectiveness for LLMs. On\\ntop of that, there are no efficient defense techniques introduced to defend against a few attack\\nmethods, such as KART [177], ProPILE [122], and the recovery of forgotten PII by fine-tuning due\\nto memorization [33]. Therefore, it requires in-depth studies and understanding to design effective\\ndefense techniques against PII attacks on LLMs.\\n7\\nAPPLICATION-BASED RISKS IN LLMS\\nLLMs are emerging techniques with high potential for many applications. The security and privacy\\nvulnerabilities of LLMs may raise serious concerns and risks in their real-world deployment with\\nvarying impacts on different application domains [255], [176].\\nComplicated Human-Interaction. LLM undergoes training on extensive text corpora and inher-\\nently possesses knowledge across diverse tasks. Carefully crafted prompts can potentially extract\\nvaluable and accurate knowledge from LLMs, which requires exploring and developing effective\\nprompt engineering techniques [245]. Despite the ideal scenario of envisioning automated prompt\\ngeneration through human-machine interaction, it is highly important to study the ethical issues\\nand limitations of this approach [280]. Consequently, a noteworthy concern emerges wherein\\nthe reliance on LLMs may potentially shift the entry barrier from coding and machine learning\\nexpertise to proficiency in prompt engineering.\\nHallucination, misinformation and disinformation dissemination. LLMs are renowned\\nfor generating sound output that may incorporate hallucinated knowledge, falsification [105],\\nmisinterpretation [10], biasness [50] which pose challenges in distinguishing it from facts. This\\ngives rise to concerns regarding potential adverse outcomes in LLM utilization, such as user\\nmisconfigurations leading to minimal run-time allocation or inappropriate decision-making in\\ntasks like selecting search spaces for specific problems [107]. Catastrophic forgetting is a problem\\nwhere a neural network forgets information it previously learned after being trained on a new\\ntask. In LLM fine-tuning, it might deteriorate the performance at significant extent [155]. The\\ndeployment of LLMs also entails risks, including the creation of less informed users and the erosion\\nof trust in shared information [280]. Particularly in sensitive domains like legal or medical advice,\\nmisinformation can have serious consequences, potentially leading users to engage in illegal\\nactions or follow detrimental instructions on medical conditions [190], [281]. Simultaneously, the\\nintentional dissemination of fake news and disinformation carries severe implications, influencing\\npublic perception and decision-making processes, and contributing to societal discord [283]. LLM-\\ngenerated misinformation can cause harm to many important sectors of society such as politics [197],\\nfinance [209], healthcare [195], and so on [29]. The dynamic nature of information dissemination\\nin the digital age magnifies these risks, necessitating the development of robust fact-checking\\nmechanisms, ethical guidelines for content generation, and responsible deployment practices\\nfor LLMs. Hallucination mitigation can be applied at both training (e.g., data curation [299] and\\nknowledge enhancement [94]) and inference stages (e.g., uncertainty measurement [99], knowledge\\nretrieval [66], and self-familiarity [154]). Recent studies have introduced several countermeasures,\\nsuch as the chain of verification (COVE [58]) and self-reflection [108], to detect LLM-generated\\nhallucinations and misinformation [28, 37]. These techniques offer potential remedies for the issues\\nmentioned above; however, further exploration is necessary to fully address these challenges. To\\naddress catastrophic forgetting, learning rate scheduling (LR-Adjust [114, 284, 289]) serves as a\\nviable mitigation technique. Self-Synthesized Rehearsal [97] (SSR) is another approach to mitigate\\nthis issue. SSR employs a base LLM to generate synthetic instances for in-context learning, which\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:31\\nis then refined by the latest LLM, preserving its learning ability. High-quality synthetic outputs are\\nchosen for future rehearsals to mitigate catastrophic forgetting.\\nCybercrime and Social Issues. LLMs can potentially be used in various cybercrime [124], e.g.,\\nphishing (efficiently create targeted scam e-mails), malware, and hacking attacks (hackers have\\nused ChatGPT to write malware codes). LLMs pose risks of perpetuating unfair discrimination and\\ncausing representational harm by reinforcing stereotypes and social biases. Harmful associations\\nof specific traits with social identities may lead to exclusion or marginalization of individuals\\noutside established norms [280]. Additionally, toxic language generated by LLMs may incite hate\\nor violence and cause serious offense. These risks are largely rooted in the selection of training\\ncorpora that include harmful language and disproportionately represent certain social identities.\\nTransportation. In the transportation domain, studies reported that LLM can be biased (while doing\\naccident report analysis), and inefficient for performing tasks in self-driving cars [253]. Furthermore,\\nit might leak personal data from self-driving cars while doing accident report automation, and\\naccident information extraction [325]. A framework has been proposed named VistaGPT to deal\\nwith the problems caused by information barriers from heterogeneity at both system and module\\nlevels in a wide range of heterogeneous vehicle automation systems [252]. It leverages LLMs to\\ncreate an automated composing platform to design end-to-end driving systems. This involves\\nemploying a “dividing and recombining” strategy to enhance the ability to generalize. To alleviate\\nthe issue of the long training time of LLMs with large datasets and high computing resource\\nrequirements, Meta-AI’s Llama focuses on fine-tuning offline pre-trained LLMs to handle the\\ntransportation safety domain tasks. The main objective is to create a specialized LLM capable\\nof generating an accurate, context-sensitive, and safety-aware model, that work effectively in\\ntraffic-related scenarios [328].\\nHealthcare and Medicine. The high risks associated with LLMs in the context of healthcare\\nsuggest that their integration into the healthcare system is presently inadvisable, as proposed by\\nDe et al. [53]. Models trained on extensive Internet data lacking rigorous filtering mechanisms may\\ninadvertently incorporate misinformation, biased content, and harmful materials alongside accurate\\nand fair information, thereby posing significant risks in healthcare applications. The potential\\nconsequences of erroneous treatment or medication recommendations by LLMs are particularly\\nconcerning. Moreover, the probabilistic nature of LLMs introduces variability in responses to the\\nsame task, giving rise to challenges in reliability and reproducibility that necessitate continuous\\nhuman oversight. Privacy concerns, especially regarding sensitive health records, coupled with\\nbroader considerations such as AI ethics principles, safety, transparency, explainability, equity, and\\nsustainability, further emphasize the need for caution in deploying LLMs within the healthcare\\ndomain, as discussed by Harrer et al. [87].\\nEducation. The use of LLMs, e.g., ChatGPT, in education is associated with significant drawbacks,\\nparticularly in fostering inaccurate concept learning and an inappropriate approach to education.\\nChatGPT, being a language model trained on diverse Internet data, may unintentionally propagate\\nmisinformation or present concepts with a lack of precision and educational rigor, for instance,\\nscientific misconduct [176]. The excessive dependence on LLMs by both educators and learners can\\nhave serious adverse effects. Students engaging with ChatGPT may encounter misleading content\\nor promote misconceptions, potentially compromising the quality of their learning experience. The\\nabsence of real-time fact-checking and the model’s susceptibility to biases and errors pose risks,\\npotentially leading learners astray and impeding their overall educational progress. Consequently,\\ncaution is recommended when relying on ChatGPT as an educational tool without appropriate\\nsupervision and verification [226], [170].\\nGovernance. The potential misuse of LLMs in governance for spear phishing presents significant\\ncybersecurity challenges. Using GPT-4 as an illustrative example, personal information of British\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:32\\nDas, et al.\\nmembers of parliament (MP) was extracted from Wikipedia, and GPT-3.5 was utilized to generate\\nbiographies, which were then incorporated into phishing emails sent to official email addresses\\n[88]. This highlights the risks associated with misinformation, biased content, and the utilization of\\nLLMs in AI-based cyberattacks within governance. The leakage of confidential information through\\nsuch attacks can pose severe consequences for national security. The generation of misinformation\\nand hate speech by LLMs further emphasizes the existing challenges, underscoring the imperative\\nneed for robust safeguards and countermeasures to address the risks related to the usage of these\\nmodels in governance settings [88].\\nScience. Hallucinations, biases, and paradigm shifts are pressing concerns of LLMs in the science\\ndomain. There is a risk of LLMs generating non-existent and false content. For instance, Meta\\ndeveloped an LLM named Galactica for reasoning scientific knowledge. That was reported to\\ngenerate major flaws due to reproducing biases and presenting falsehoods [46]. As a result, the model\\nwas shut down just after launching public access [91]. Another concern lies in the involvement\\nof LLMs in the scientific discovery process. It is challenging to interpret and understand LLMs\\ndue to their black-box nature, raising doubts about their reliability and trustworthiness in the\\nscience domain. For example, peer-review reports generated by LLMs may misinterpret research\\narticles, which may impair the peer review quality [3]. Moreover, collaborating with LLMs won’t be\\nfundamentally the same as collaborating with other researchers or experts in a corresponding field\\n[16]. Clear principles of using these LLMs and/or other AI tools in scientific explorations should be\\nestablished to ensure transparency, fairness, and trustworthiness [16].\\n8\\nLIMITATIONS OF EXISTING WORKS AND FUTURE RESEARCH DIRECTION\\nFollowing a comprehensive examination of prevailing security and privacy attacks and defense\\nmechanisms, this section delves into the prospects of advancing secure and privacy-preserving\\nLLMs. In Figure 9, we show an overview of the evolution of attack methods and defense mechanisms\\nin LLMs, their limitations, and future research directions. We then discuss the limitations of current\\nsecurity and privacy attacks and defenses along various promising domains that require further\\nresearch.\\nExisting Attack Methods, their Limitations, and Future Research Direction: In LLMs,\\nvarious categories of security and privacy attacks have emerged, posing significant risks to LLM\\nsystems. Among security attacks, prompt injection is a prominent technique in which attackers\\ncraft malicious prompts, either manually [146] or automatically [238], to mislead the LLM into\\ngenerating inappropriate or harmful outputs. These prompts are designed to bypass the model’s\\nsafety alignments [110], enabling the generation of content as per the attacker’s intent. Jailbreaking\\nattacks involve creating malicious prompts in tricky way [246], such as character role-play [83] and\\nattention shifting [147], so that the LLM generates inappropriate/harmful contents. Data poisoning\\nattacks involve the insertion of malicious data samples during the model’s training/fine-tuning\\nphases [126, 263], introducing biases or vulnerabilities that compromise the model’s functionality.\\nSimilarly, backdoor attacks introduce hidden “backdoors” (via trojaning) during the training/fine-\\ntuning phase [235], which are activated by the presence of backdoor triggering words in the prompts,\\nmaking the LLM system vulnerable. LLM privacy attacks focus on extracting sensitive information\\nsuch as private training/fine-tuning data, personal information, and even model components\\n(e.g., gradients or model architecture). Techniques used in these attacks include gradient leakage\\nattacks [55], MIA [239], and PII leakage attacks [178].\\nRecent studies have shown that existing attack methods have some drawbacks. For instance,\\nthe DAN attack [234] builds on jailbreak prompts gathered over six months, extending from the\\ninception of ChatGPT-related sources to May 2023. It is recognized that adversaries have the\\npotential to persist in refining jailbreak prompts for specific objectives beyond the documented\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:33\\nExisting \\nWorks\\nLimitations of \\nExisting Works\\nFuture \\nDirections\\nManual prompting\\nAutomatic prompting\\nSQL-based prompting\\nTriggering-based Attacks\\nTrojaning\\nGradient leakage attacks\\nMemorization\\nJailbreaking attacks\\nContent filtering\\nDetection and prevention \\nbased defense\\nPerplexity-based \\ndefense\\nDifferential privacy\\nDropout and\\nregularization\\nPruning and \\nfiltering\\nPerturbation\\nAttack methods\\nDefense Mechanisms\\nDoes not work for\\nAll models\\nManual prompts \\nstill work better\\nAdversary requires\\nwhite-box information\\nHigh computational \\ncost for FL clients\\nMostly evaluated on \\nsmall language models\\ne.g., RoBERTa [149]\\nPerformance degrades for \\nDP-SGD and  perturbation-\\nbased methods\\nNo effective defense techniques \\nfor advanced attacks, e.g., \\nBadPrompt [21], ProAttack [320], \\nBToP [294]\\nPerplexity and filtering-based \\ntechniques fails under usages \\nof synonymous triggers\\nExisting mitigation techniques \\ndesigned only for specific NLP\\n tasks. But fails for other ones.\\nTrade-off between the \\nmodel utility and defense\\nDevelopment of adversarial \\nReinforcement learning in LLMs\\nFormulating evaluation metrics\\nChecking attack method’s \\nsusceptibility from other domains \\nDynamic attack methods \\nwith different goals\\nExplore privacy risks and \\nVulnerabilities due to attacks\\nDeveloping effective defense techniques for\\nadvanced attacks, e.g., BadPrompt [21],\\n Proattack [320], BToP [294]\\nReal time privacy monitoring\\nExplore secure multi-party\\ncomputation for defense\\nDesigning a robust detection \\nand prevention-based defense\\nExplainability to the attack methods\\nExplainability to the defense methods\\nFig. 9. Overview of the advancements of the attack methods, defense mechanisms in LLMs, their limitations,\\nand future research directions.\\ncollection time-frame. This demonstrates the dynamic nature of adversarial strategies, with the\\nunderstanding that new, optimized prompts may emerge even after the initial data collection phase.\\nMoreover, most of the existing studies on jailbreaking attacks are primarily focused on ChatGPT\\n[54, 132, 234]. It remains unclear whether potential vulnerabilities exist in other LLMs, such as\\nVicuna [38], Bard [158], and Bing Chat [165]. For example, MasterKey [54] may not achieve\\ncomparable performance on Vicuna as it does on ChatGPT. For prompt-based attacks, manual\\nprompts are often more effective than automated prompt-based attack methods. As automated\\nattack methods are primarily designed for generalized tasks, they may not demonstrate same\\neffectiveness for specific tasks as manually crafted prompts. The unlabeled and imbalanced real-\\nworld data further complicate the development of effective automated prompting-based attack\\nmethods [238]. For backdoor attacks, most of them are focused on classification or similar tasks\\n[21], e.g., sentiment analysis and opinion classification. More attention should be paid to the attacks\\nthat perform other NLP tasks, including question answering, text summarization, and language\\ntranslation.\\nThe underlying philosophy behind privacy attacks lies in the correlation between the level of\\naccessibility an adversary holds and its ability to extract sensitive information or exert control over\\ntarget victim LLMs. More access leads to a broader potential for the adversary to recover sensitive\\ndata or influence the target LLM [131]. For instance, when the adversary has access only to the\\nblack-box model, the attacker might be able to leverage training data extraction attacks to recover\\na limited set of private data. However, if the adversary is granted white-box information such\\nas gradients, the attacker can leverage this extra information to accurately recover more private\\ntraining instances [55]. This expanded access can facilitate various privacy attacks, including\\nattribute inference attacks, embedding inversion, and gradient leakage attacks. In gradient-based\\nattacks, the adversary needs the white-box information of a model, which is sometimes impractical\\nin real-world practice. Moreover, most of the existing privacy attacks are designed for vision models.\\nA limited number of studies have reported gradient leakage attacks specifically on language models,\\ne.g., the LAMP attack [14]. In essence, the increased access empowers adversaries to perform\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:34\\nDas, et al.\\nmore sophisticated and targeted privacy attacks against LLMs, potentially compromising sensitive\\ninformation or gaining access to the internal model architecture. The early MIAs were based on\\nthe white-box access assumption [239], which is sometimes impractical in real-world deployment.\\nThe evaluation dataset for some attacks, e.g., ProPILE was built solely from private information\\navailable in open-source datasets provided by major corporations, ensuring ethical data acquisition\\n[122]. However, it is crucial to note that the heuristic data collection process might potentially lead\\nto instances of bias, disassociation, or noise. This adds uncertainty and potential inaccuracies in\\nthe benchmark dataset, requiring attention when interpreting the results.\\nHowever, most attack methods (e.g., BadGPT, BadPrompt, and Trojaning attacks) described in\\nthe existing studies are designed for only relatively small NLP models. Only a few are tested on\\nLLMs (e.g., ProPILE, DAN, and JAILBREAKER). Also, the high cost of accessing commercialized\\nLLMs, such as GPT-3.5 or upper versions, contributes to the lack of attack evaluations on LLMs.\\nBesides, the in-depth vulnerability analysis in terms of privacy attacks and security issues is still\\nlacking for LLMs. One of the reasons can be attributed to the limited number of performance\\nevaluation metrics (e.g., perplexity) in the language domain to comprehensively evaluate attack\\nand defense effectiveness. Also, in the FL environment, it requires very high computational power\\nto train LLMs with such large datasets [247]. It is an open research challenge to develop effective\\nattack and robust defense methods along with the proper evaluation techniques for LLMs. The\\ncorrectness of LLM-generated content has always been a major concern in this area of research.\\nDue to the knowledge gaps, i.e., missing or outdated information might always be present in LLMs.\\nThe problem of hallucination in LLM has been investigated and evaluated from the knowledge-gap\\nperspective but is yet to be investigated from other perspectives such as safety, i.e., abstaining to\\ngenerate harmful content [25].\\nFor LLMs, comprehensive exploration of their vulnerabilities under security and privacy attacks\\nremains an essential area of study. Future research should examine the applicability of well-\\nestablished attack methods from other domains, such as computer vision, to LLMs. In addition to\\nexisting methods, novel attack techniques should be developed to comprehensively inspect various\\nvulnerabilities of LLMs, potentially targeting multiple objectives with a single attack approach.\\nMoreover, special attention should be given to developing appropriate metrics for evaluating the\\nimpact of vulnerabilities related to security and privacy attacks in LLMs. Explainable AI (XAI) can\\nplay a vital role in this domain by increasing transparency and explainability within attack systems,\\nallowing for a better understanding of LLM vulnerabilities. Developing XAI techniques to enhance\\nthe interpretability of LLM vulnerabilities is another essential research direction for advancing the\\nsecurity and privacy of LLMs.\\nExisting Defense Mechanisms, Challenges, and Future Research Direction: To mitigate vul-\\nnerabilities posed by security and privacy attacks, various defense techniques have been proposed in\\nexisting research. Instruction defense [219], paraphrasing [104], and re-tokenization [104] methods\\nare commonly employed to defend against prompt injection attacks. To defense jailbreaking attacks,\\nSmoothLLM [215], LLM Guard [6], and perplexity filtering [78] are widely used. Data curation\\ntechniques [299], including filtering poisoned content [263] and detecting trigger-embedded in-\\nputs [27], are widely used to mitigate data poisoning attacks and backdoor attacks. On the other\\nhand, DP-SGD [2] is a widely adopted technique to defend against gradient leakage attacks, MIAs,\\nand PII leakage attacks. In particular, noise perturbation [331] is a common strategy to defend\\nagainst gradient leakage attacks. Various techniques are prevalent to mitigate MIA in LLMs, includ-\\ning model pruning [143] and knowledge distillation [92]. To prevent PII leakage attacks, training\\ndata curation [299] and restricting attacker access to training data or model [102] serve as viable\\ncountermeasures.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:35\\nFor defense, studies reported that ChatGPT’s safety protections are good enough to prevent single\\njailbreaking prompts however, it is still vulnerable to multi-step jailbreaking [132]. Moreover, the\\nnew Bing AI chatbot [169] is more vulnerable to these direct prompts. System-mode self-reminder\\ndefense techniques are inspired by the human-like reasoning capabilities of LLMs [291]. The more\\ndiscerning question regarding LLM reasoning processes with or without self-reminder remains\\nunsolved. To acquire a comprehensive understanding of the reasoning processes of large neural\\nnetworks, more in-depth investigation is highly required. Although the side effects of self-reminder\\nhave been explored on typical user queries across various NLP tasks, evaluating its effect on any\\ntype of user query poses a challenge, which makes it difficult to fully understand its impact on user\\nexperience [285].\\nConsidering the shortcomings mentioned, developing more flexible self-reminding systems\\nand expert frameworks that improve safety, trustworthiness, and accountability in LLMs without\\ncompromising effectiveness can be a fundamental research challenge to protect LLMs from jail-\\nbreaking attacks. Furthermore, individuals with malicious intent are highly active in online forums,\\nsharing and discussing new strategies. Frequently, they keep these exchanges private to evade\\ndetection. Consequently, it is essential to conduct further research and studies aims to identify\\nand implement effective defense strategies to mitigate the risks posed by the latest jailbreaking\\nattacks. Efficient strategies for defending against backdoor attacks in a black-box environment are\\nstill lacking [117]. Existing defense mechanisms ([249], [201], [230]), for specific learning tasks\\nin LMs are not evaluated for the other learning tasks like, text summarizing, and prompt-based\\nlearning. Moreover, it is found in the literature that prompt-based PLMs are highly susceptible\\nto textual backdoor attacks [295], [60]. Addressing the challenge of textual backdoor attacks in\\nprompt-based paradigms, particularly in the few-shot learning setting [273], is another unresolved\\nchallenge. MDP (Masking-Differential Prompting) defense [291] faces challenges in various NLP\\ntasks like paraphrasing and sentence similarity [74]. Its performance under few-shot learning\\nremains uncertain due to a lack of practical evaluation. While MDP has demonstrated strength\\nagainst earlier backdoor attacks, it may not be effective against recently introduced attacks like Bad-\\nPrompt [21] and BToP [295]. Perplexity-based methods and filtering-based methods may not work\\nwell when attackers use synonymous trigger keys [96]. Furthermore, developing dynamic defense\\nmethods considering the above factors is a challenging future task. Currently, the predominant\\nfocus of investigation on backdoor attacks revolves around text classification in LLMs. However, a\\nnotable gap exists in the literature concerning investigations into backdoor attacks on various tasks\\nfor which LLMs find widespread application, e.g., text summarization and text generation [300].\\nUnderstanding and addressing backdoor attacks in various tasks for which LLMs are employed\\nis crucial for developing effective defense mechanisms and ensuring secure deployment of LLMs.\\nWhile poisoning attacks on ML models have been investigated in the literature [175], there is not\\nyet an effective solution for several attack methods, including ProAttack[321] and Badprompt[21].\\nFurther research in diverse tasks and models can enhance the knowledge and understanding of\\nthe security impacts of LLMs, as well as facilitate the development of robust and trustworthy LLM\\nsystems. Defense techniques, such as dataset cleaning, and removing near duplicate poisoned\\nsamples and anomalies, sometimes slow down the model development process in order to defend\\nagainst data poisoning attacks. Other defense methods, e.g., stopping training after certain epochs,\\nachieve a moderate defense against poisoning attacks but degrade the model utility [263]. Gradient\\nperturbation [95] and DP-SGD-based methods [2] are frequently used to defend against privacy\\nattacks in LLMs. It can prevent the private training data from being leaked based on the parameter\\nconfigurations at a small cost of model utility. Limiting the accessibility to the model and generating\\nlimited prediction results might be another option [312]. Extensive research studies can obtain\\nproper knowledge of to what extent algorithmic defenses such as differential privacy can prevent\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:36\\nDas, et al.\\nPII disclosure without compromising model utility. In the post-processing phase, for API-access\\nmodels such as GPT-3, it is advisable to integrate a detection module that examines the output text\\nto identify sensitive information. If sensitive content is detected, the system should either decline\\nto provide an answer or apply masks to safeguard the sensitive information [98]. Also, for image\\nmodels, a recent study has demonstrated that adding a standard level of random noise into the\\ngradient update might not always work well to prevent gradient leakage attacks on medical images\\n[52]. Recently, an open language model (OLMo) has been introduced to provide open access to the\\ndata, code, and model [242]. The main purpose of OLMo is to facilitate open research on language\\nmodels. They performed PII filtering to remove it from the data, and they also provided a tool to\\nremove PII data upon request. Though it followed existing practices to obscure the PII exposure\\nand identify and remove toxic content, it did not explicitly discuss the impacts of various attacks\\noutlined in this survey paper and how to mitigate those risks. Secure multi-party computation [47]\\ncan be another way to defend against privacy attacks in LLMs, which can be explored in future\\nresearch endeavors. Considering the above limitations of existing defense techniques in LLMs,\\ndeveloping a defense mechanism for these privacy attacks for LLMs would be an imperative task.\\nFuture research initiatives for enhancing security and privacy in LLMs can be directed toward\\nseveral key areas. An ideal defense method should be able to effectively achieve a balance between\\nmodel utility and security & privacy protection. The development of real-time privacy monitoring\\nsystems is essential to improve the resilience of privacy-preserving LLMs. This necessitates the\\nexploration of robust detection techniques against various security and privacy attacks. Furthermore,\\na thorough evaluation of less-explored defense techniques, such as secure multi-party computation\\n(SMPC), is necessary to assess their effectiveness against LLM vulnerabilities. Finally, leveraging\\nthe capabilities of XAI can improve transparency in LLM defense mechanisms.\\nRole of explainable AI in Enhancing Security and Privacy of LLMs: LLM’s ability to make de-\\ncisions on various learning tasks is often criticized due to its black-box nature, e.g., non-interpretable\\nweights. It makes it more challenging for the new practitioners and developers of this field, as it\\nhinders their ability to clearly interpret and understand its application, particularly in critical cases.\\nExplainable AI (XAI) can help to bridge this gap by developing methods to interpret and explain\\ncomplex LLM systems, the decision-making process, and outputs [22]. Conducting studies and\\nresearch to build explainable methods is highly essential to make the usage of LLMs more reliable\\nand trustworthy. XAI can provide transparent insights into the inference process and the dynamic\\nweight assignment by the attention mechanism of the LLMs, which enhances interpretability by\\nhighlighting the most influential input features that contribute to its predictions. This explainability\\nmakes the model more trustworthy and ensures that its output is explainable. Additionally, it\\nfacilitates error analysis and bias detection. XAI has shown its potential in several real-world AI\\napplications such as anomaly detection [1, 8], cyber-security [216], and enhancing data privacy\\n[63]. In ML, several popular methods exist such as Local Interpretable Model-agnostic Explanations\\n(LIME) [214], and SHapley Additive exPlanations (SHAP) [153], which offer insights into model\\nbehavior without requiring access to internal model components. These methods analyze the\\nassociation between inputs and outputs to identify features that most significantly contribute\\nto model predictions. The explainability of code generation models such as CodeBERT [67] and\\nGraphCodeBERT [82] were proposed to understand code syntax and semantics [304]. Also, the\\nad-hoc explanation further clarifies the model’s decision [142].\\nThe role of XAI in the security and privacy aspect of LLM is crucial. Specifically in its development\\nphase, having prior knowledge about the attackers’ ability (e.g., black-box access to the model,\\ninjecting poisonous instances to the training/fine-tuning data) and the vulnerable components\\n(training/fine-tuning data and the model itself) of LLM systems will assist in developing robust\\ntechniques against security and privacy attacks. However, XAI may expose critical aspects of the\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:37\\nLLMs, including their architecture, components, and the dynamic weight allocation by attention\\nmechanism that contribute to predictions. Such disclosures may increase the LLM’s vulnerability\\nto security and privacy attacks. For example, revealing the white-box nature of the model may\\nfacilitate the attacker’s access to the model, increasing the risk of gradient leakage attacks or MIA\\n[63]. Recent studies have claimed that explainability in LLMs may raise further security concerns,\\nespecially with insidious backdoor attacks [35]. Lin et al. proposed an XAI approach that can\\nidentify the triggers (backdoor attack) that mislead the model to contribute to error classification\\n[141]. Li et al. developed an XAI technique to identify and quantify the influence of raw data features\\non successful MIAs. This approach analyzes the data distribution to identify the influential neurons\\ncontributing to compromising private data and subsequently trains an MIA ensemble model using\\nattack features derived from the selected neurons [141]. XAI techniques in image models (e.g.,\\nGrad-CAM [224]) may compromise model privacy under various attacks, such as gradient leakage\\nattacks. In some cases, XAI enables more accurate reconstruction of private training data compared\\nto models that rely solely on predictions [324]. XAI-aware model extraction attack (XaMEA) was\\nproposed to exploit spatial knowledge from decision explanations [297]. It illustrated that the\\ntransparency provided by XAI may facilitate the attacker’s access to the model, making it easier to\\nexploit it. This increased explainability can make the model more vulnerable to model extraction\\nattacks compared to prediction-only models. However, the vulnerabilities mentioned above are\\nmostly explored for DNNs. In LLMs, the vulnerabilities in the XAI context have been explored\\nto a limited extent. Moreover, there are several unique characteristics of LLMs that are different\\nfrom DNNs, e.g., large-scale data and model parameters, task-agnostic, and semantic language\\nunderstanding, which makes it more challenging to design XAI methods for better interpretations.\\nThe comprehensive interpretation of LLM vulnerabilities under XAI, including backdoor attacks,\\nmembership inference attacks (MIA), and model extraction attacks, has yet to be fully explored.\\nAdditionally, vulnerabilities related to prompt injection and jailbreaking attacks in the context of\\nXAI still remain unexplored. Furthermore, open-sourced LLMs are publicly accessible and may\\nprovide greater explainability than closed-source models. However, this enhanced accessibility can\\nmake them more vulnerable to attacks when XAI techniques are employed. Thus, these LLMs may\\nbe more susceptible to attacks due to XAI. Further research is essential to fully understand the\\nextent and impact of attackers’ capabilities under XAI. Additionally, mitigation techniques should\\nbe developed in accordance with the identified risks and impacts.\\n9\\nCONCLUSION\\nLLMs lend themselves as strong tools for comprehending complex linguistic patterns and generating\\nlogical and contextually coherent responses. However, such powerful models also entail potential\\nprivacy and security risks. In this survey, we first provided a detailed overview of LLMs’ security\\nand privacy challenges. We then discussed and analyzed the LLM vulnerabilities from both security\\nand privacy aspects, existing mitigation and defense strategies against these security attacks and\\nprivacy attacks, as well as highlighted their strengths and limitations. In our investigation, we\\nfound that LLMs are highly vulnerable to the discussed attacks. According to our survey, there are\\na limited number of mitigation techniques to prevent those attacks against LLMs. The existing\\nmitigation techniques that are applicable to relatively small LMs could potentially be used for\\nLLMs. However, extensive research studies should be performed to evaluate and tailor the existing\\nsolutions to LLMs. Based on our analysis, we also outlined future research directions focusing on\\nsecurity and privacy aspects, pointed out key research gaps, and illustrated open research problems.\\nThe overarching goal is to enhance the reliability and utility of LLMs through comprehensive\\nexploration and resolution of these vulnerabilities and offer pathways for future research toward\\nsecure and privacy-preserving LLM systems.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:38\\nDas, et al.\\nACKNOWLEDGEMENTS: This work is partially supported by the U.S. Department of Homeland\\nSecurity Grant Award Number 17STCIN00001-05-00. Further, M. Hadi Amini’s work is partly\\nsupported by the U.S. Department of Homeland Security under Grant Award Number 23STSLA00016-\\n01-00. The views and conclusions contained in this document are those of the authors and should\\nnot be interpreted as necessarily representing the official policies, either expressed or implied, of\\nthe U.S. Department of Homeland Security.\\nREFERENCES\\n[1] 30dascoding. 2024. Unraveling the Mysteries of Anomaly Detection: The Power of Explainable AI. Available Online:\\nhttps://30dayscoding.com/blog/explainable-ai-in-anomaly-detection [Accessed on October 31, 2024].\\n[2] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep\\nlearning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications\\nsecurity. 308–318.\\n[3] David Leslie & Sandra Wachter Abeba Birhane, Atoosa Kasirzadeh. 2023. Science in the age of large language models.\\ndoi.org/10.1038/s42254-023-00581-4 (2023).\\n[4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774\\n(2023).\\n[5] Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant Kharkar, Christopher Kruegel, Giovanni Vigna,\\nDavid Evans, Ben Zorn, and Robert Sim. 2023. TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. arXiv\\npreprint arXiv:2301.02344 (2023).\\n[6] Protect AI. 2024. LLM Guard - The Security Toolkit for LLM Interactions. Available Online: https://llm-guard.com/\\n[Accessed on October 11, 2024].\\n[7] Alex Albert. 2023. Jailbreak Chat. Available Online: https://www.jailbreakchat.com [Accessed on January 28, 2024].\\n[8] Tarek Ali. 2024. Next-generation intrusion detection systems with LLMs: real-time anomaly detection, explainable AI,\\nand adaptive data generation. Master’s thesis. T. Ali.\\n[9] Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. arXiv preprint\\narXiv:2308.14132 (2023).\\n[10] Thimira Amaratunga. 2023. Threats, Opportunities, and Misconception s. In Understanding Large Language Models:\\nLearning Their Underlying Concepts and Technologies. Springer, 131–148.\\n[11] Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. 2021. Large-scale differentially private\\nBERT. arXiv preprint arXiv:2108.01624 (2021).\\n[12] Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. 2017. Privacy-preserving deep learning via\\nadditively homomorphic encryption. IEEE transactions on information forensics and security 13, 5 (2017), 1333–1345.\\n[13] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,\\nDeep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning\\nfrom human feedback. arXiv preprint arXiv:2204.05862 (2022).\\n[14] Mislav Balunovic, Dimitar Dimitrov, Nikola Jovanović, and Martin Vechev. 2022. LAMP: Extracting text from gradients\\nwith language model priors. Advances in Neural Information Processing Systems 35 (2022), 7641–7654.\\n[15] Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and Jaehoon Amir Safavi. 2017. Mitigating poisoning attacks on\\nmachine learning models: A data provenance based approach. In Proceedings of the 10th ACM workshop on artificial\\nintelligence and security. 103–110.\\n[16] Marcel Binz, Stephan Alaniz, Adina Roskies, Balazs Aczel, Carl T Bergstrom, Colin Allen, Daniel Schad, Dirk Wulff,\\nJevin D West, Qiong Zhang, et al. 2023. How should the advent of large language models affect the practice of science?\\narXiv preprint arXiv:2312.03759 (2023).\\n[17] ROB BONTA. 2023. California Consumer Privacy Act (CCPA). Available Online: https://oag.ca.gov/privacy/ccpa.\\n[Accessed on January 28, 2024].\\n[18] Samuel R Bowman. 2023. Eight things to know about large language models. arXiv preprint arXiv:2304.00612 (2023).\\n[19] Hezekiah J Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo\\nIglesias, Ron Heichman, and Ramesh Darwishi. 2022. Evaluating the susceptibility of pre-trained language models\\nvia handcrafted adversarial examples. arXiv preprint arXiv:2209.02128 (2022).\\n[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n[21] Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al. 2022. BadPrompt: Backdoor attacks on continuous prompts.\\nAdvances in Neural Information Processing Systems 35 (2022), 37068–37080.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:39\\n[22] Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Navid Nobani, and Andrea Seveso. 2024. XAI meets LLMs: A Survey\\nof the Relation between Explainable AI and Large Language Models. arXiv preprint arXiv:2407.15248 (2024).\\n[23] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,\\nTom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th\\nUSENIX Security Symposium (USENIX Security 21). 2633–2650.\\n[24] Computer Security Resource Center. 2023. Information Systems Security (INFOSEC). Available Online: https:\\n//csrc.nist.gov/glossary/term/information_systems_security. [Accessed on January 28, 2024].\\n[25] Edward Y Chang. 2023. SocraSynth: Multi-LLM Reasoning with Conditional Statistics. Available Online: https://www.\\nresearchgate.net/publication/373753725_SocraSynth_Multi-LLM_Reasoning_with_Conditional_Statistics [Accessed\\non February 05, 2024].\\n[26] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. 2023. Jailbreaking\\nblack box large language models in twenty queries. arXiv preprint arXiv:2310.08419 (2023).\\n[27] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and\\nBiplav Srivastava. 2018. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint\\narXiv:1811.03728 (2018).\\n[28] Canyu Chen and Kai Shu. 2023. Can llm-generated misinformation be detected? arXiv preprint arXiv:2309.13788\\n(2023).\\n[29] Canyu Chen and Kai Shu. 2023. Combating misinformation in the age of llms: Opportunities and challenges. AI\\nMagazine (2023).\\n[30] Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024. Humans or llms as the judge? a\\nstudy on judgement biases. arXiv preprint arXiv:2402.10669 (2024).\\n[31] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. 2019. DeepInspect: A Black-box Trojan Detection and\\nMitigation Framework for Deep Neural Networks.. In IJCAI, Vol. 2. 8.\\n[32] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[33] Xiaoyi Chen, Siyuan Tang, Rui Zhu, Shijun Yan, Lei Jin, Zihao Wang, Liya Su, XiaoFeng Wang, and Haixu Tang.\\n2023. The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. arXiv preprint\\narXiv:2310.15469 (2023).\\n[34] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. 2024. AgentPoison: Red-teaming LLM Agents via\\nPoisoning Memory or Knowledge Bases. arXiv preprint arXiv:2407.12784 (2024).\\n[35] Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Wei Lu, and Gongshen Liu. 2023. Backdoor attacks and coun-\\ntermeasures in natural language processing models: A comprehensive security review. arXiv preprint arXiv:2309.06055\\n(2023).\\n[36] Vivying SY Cheng et al. 2006. Health insurance portability and accountability act (HIPPA) compliant access control\\nmodel for web services. International Journal of Healthcare Information Systems and Informatics (IJHISI) 1, 1 (2006),\\n22–39.\\n[37] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei\\nLiu, et al. 2023. FacTool: Factuality Detection in Generative AI–A Tool Augmented Framework for Multi-Task and\\nMulti-Domain Scenarios. arXiv preprint arXiv:2307.13528 (2023).\\n[38] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\\nZhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatGPT quality.\\nSee https://vicuna. lmsys. org (accessed 14 April 2023) (2023).\\n[39] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan Boneh. 2018. SentiNet: Detecting physical attacks\\nagainst deep learning systems.(2018). (2018).\\n[40] Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024. Comprehensive assessment\\nof jailbreak attacks against llms. arXiv preprint arXiv:2402.05668 (2024).\\n[41] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa\\nDehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine\\nLearning Research 25, 70 (2024), 1–53.\\n[42] Benjamin Clavié, Alexandru Ciceu, Frederick Naylor, Guillaume Soulié, and Thomas Brightwell. 2023. Large Language\\nModels in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. In International Conference\\non Applications of Natural Language to Information Systems. Springer, 3–17.\\n[43] CLOUDFLARE. 2023. What is data privacy? Available Online: https://www.cloudflare.com/learning/privacy/what-is-\\ndata-privacy/. [Accessed on January 28, 2024].\\n[44] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified adversarial robustness via randomized smoothing. In\\ninternational conference on machine learning. PMLR, 1310–1320.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:40\\nDas, et al.\\n[45] Andrea Continella, Yanick Fratantonio, Martina Lindorfer, Alessandro Puccetti, Ali Zand, Christopher Kruegel,\\nGiovanni Vigna, et al. 2017. Obfuscation-Resilient Privacy Leak Detection for Mobile Apps Through Differential\\nAnalysis.. In NDSS, Vol. 17. 10–14722.\\n[46] The Conversation. 2022. The Galactica AI model was trained on scientific knowledge – but it spat out alarmingly\\nplausible nonsense. Available Online: https://theconversation.com/the-galactica-ai-model-was-trained-on-scientific-\\nknowledge-but-it-spat-out-alarmingly-plausible-nonsense-195445 [Accessed on January 28, 2024].\\n[47] Ronald Cramer, Ivan Bjerre Damgård, et al. 2015. Secure multiparty computation. Cambridge University Press.\\n[48] Evan Crothers, Nathalie Japkowicz, and Herna L Viktor. 2023. Machine-generated text: A comprehensive survey of\\nthreat models and detection methods. IEEE Access (2023).\\n[49] Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun. 2022. A unified evaluation\\nof textual backdoor learning: Frameworks and benchmarks. Advances in Neural Information Processing Systems 35\\n(2022), 5009–5023.\\n[50] Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu. 2024. Bias and unfairness in information\\nretrieval systems: New challenges in the llm era. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining. 6437–6447.\\n[51] Dimitrios Damopoulos, Georgios Kambourakis, Stefanos Gritzalis, and Sang Oh Park. 2014. Exposing mobile malware\\nfrom the inside (or what is your mobile app really doing?). Peer-to-Peer Networking and Applications 7 (2014), 687–697.\\n[52] Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. 2023. Privacy risks analysis and mitigation in federated\\nlearning for medical images. In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE,\\n1870–1873.\\n[53] Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pierpaolo Privitera, Paolo Ferragina, Alberto Eugenio\\nTozzi, and Caterina Rizzo. 2023. ChatGPT and the rise of large language models: the new AI-driven infodemic threat\\nin public health. Frontiers in Public Health 11 (2023), 1166120.\\n[54] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu.\\n2023. MasterKey: Automated jailbreak across multiple large language model chatbots. arXiv preprint arXiv:2307.08715\\n(2023).\\n[55] Jieren Deng, Yijue Wang, Ji Li, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, and Caiwen Ding. 2021. TAG:\\nGradient attack on transformer-based language models. arXiv preprint arXiv:2103.06819 (2021).\\n[56] develop.sentry.dev. 2023. PII and Data Scrubbing. Available Online: https://develop.sentry.dev/pii [Accessed on\\nJanuary 28, 2024].\\n[57] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[58] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.\\nChain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495 (2023).\\n[59] Ben Dickson. 2021. Machine learning: What are membership inference attacks? Available Online: https://bdtechtalks.\\ncom/2021/04/23/machine-learning-membership-inference-attacks/ [Accessed on January 28, 2024].\\n[60] Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and Shilin Wang. 2022. PPT: Backdoor attacks on pre-trained models\\nvia poisoned prompt tuning. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,\\nIJCAI-22. 680–686.\\n[61] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential privacy. Foundations and Trends®\\nin Theoretical Computer Science 9, 3–4 (2014), 211–407.\\n[62] Zekeriya Erkin, Martin Franz, Jorge Guajardo, Stefan Katzenbeisser, Inald Lagendijk, and Tomas Toft. 2009. Privacy-\\npreserving face recognition. In Privacy Enhancing Technologies: 9th International Symposium, PETS 2009, Seattle, WA,\\nUSA, August 5-7, 2009. Proceedings 9. Springer, 235–253.\\n[63] Fatima Ezzeddine. 2024. Privacy Implications of Explainable AI in Data-Driven Systems. arXiv preprint arXiv:2406.15789\\n(2024).\\n[64] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A\\nsurvey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM\\nSIGKDD Conference on Knowledge Discovery and Data Mining. 6491–6501.\\n[65] Qizhang Feng, Siva Rajesh Kasa, Hyokun Yun, Choon Hui Teo, and Sravan Babu Bodapati. 2024. Exposing privacy\\ngaps: Membership inference attack on preference data for LLM alignment. arXiv preprint arXiv:2407.06443 (2024).\\n[66] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. Retrieval-generation synergy\\naugmented large language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP). IEEE, 11661–11665.\\n[67] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\\nDaxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint\\narXiv:2002.08155 (2020).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:41\\n[68] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines\\n30 (2020), 681–694.\\n[69] Jacob Fox. 2023. Data Poisoning Attacks: A New Attack Vector within AI. Available Online: https://www.cobalt.io/\\nblog/data-poisoning-attacks-a-new-attack-vector-within-ai [Accessed on January 28, 2024].\\n[70] JAKE FRANKENFIELD. 2023. What Is Personally Identifiable Information (PII)? Types and Examples. Available\\nOnline: https://www.investopedia.com/terms/p/personally-identifiable-information-pii.asp [Accessed on January 28,\\n2024].\\n[71] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,\\nXiawu Zheng, et al. 2023. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.\\narXiv preprint arXiv:2306.13394 (2023).\\n[72] Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. 2023. Practical Membership Inference\\nAttacks against Fine-tuned Large Language Models via Self-prompt Calibration. arXiv preprint arXiv:2311.06062\\n(2023).\\n[73] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi\\nZhang, and Nesreen K Ahmed. 2024. Bias and fairness in large language models: A survey. Computational Linguistics\\n(2024), 1–79.\\n[74] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv\\npreprint arXiv:2012.15723 (2020).\\n[75] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. 2019. Strip: A defence\\nagainst trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications\\nConference. 113–125.\\n[76] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. 2020. Inverting gradients-how easy is it\\nto break privacy in federated learning? Advances in Neural Information Processing Systems 33 (2020), 16937–16947.\\n[77] Robin C Geyer, Tassilo Klein, and Moin Nabi. 2017. Differentially private federated learning: A client level perspective.\\narXiv preprint arXiv:1712.07557 (2017).\\n[78] Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language\\nmodels via perplexity estimation. arXiv preprint arXiv:2212.04037 (2022).\\n[79] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. Not\\nwhat you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In\\nProceedings of the 16th ACM Workshop on Artificial Intelligence and Security. 79–90.\\n[80] Alexandre Sablayrolles Hervé Jégou Guo, Chuan and Douwe Kiela. 2021. Gradient-based adversarial attacks against\\ntext transformers. arXiv preprint arXiv:2104.13733 (2021).\\n[81] Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. 2021. Gradient-based adversarial attacks against\\ntext transformers. arXiv preprint arXiv:2104.13733 (2021).\\n[82] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\\natkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint\\narXiv:2009.08366 (2020).\\n[83] Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj. 2023. From ChatGPT to\\nThreatGPT: Impact of generative AI in cybersecurity and privacy. IEEE Access (2023).\\n[84] Surbhi Gupta, Abhishek Singhal, and Akanksha Kapoor. 2016. A literature survey on social engineering attacks:\\nPhishing attack. In 2016 international conference on computing, communication and automation (ICCCA). IEEE, 537–540.\\n[85] Siddhant Haldar. 2023. Gradient-based Adversarial Attacks : An Introduction. Available Online: https://medium.com/\\nswlh/gradient-based-adversarial-attacks-an-introduction-526238660dc9s [Accessed on January 28, 2024].\\n[86] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,\\ntrained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).\\n[87] Stefan Harrer. 2023. Attention is not all you need: the complicated case of ethically using large language models in\\nhealthcare and medicine. EBioMedicine 90 (2023).\\n[88] Julian Hazell. 2023. Large language models can be used to effectively scale spear phishing campaigns. arXiv preprint\\narXiv:2305.06972 (2023).\\n[89] Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, and Philip S Yu. 2024. The Emerged Security and Privacy\\nof LLM Agent: A Survey with Case Studies. arXiv preprint arXiv:2407.19354 (2024).\\n[90] Jiaming He, Guanyu Hou, Xinyue Jia, Yangyang Chen, Wenqi Liao, Yinhang Zhou, and Rang Zhou. 2024. Data\\nStealing Attacks against Large Language Models via Backdooring. Electronics 13, 14 (2024), 2858.\\n[91] Will Douglas Heaven. 2022. \"Why Meta’s latest large language model survived only three days online\". MIT Technology\\nReview. Last accessed December 15 (2022), 2022.\\n[92] Geoffrey Hinton. 2015. Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531 (2015).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:42\\nDas, et al.\\n[93] Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, and Yejin Choi. 2019. Efficient adaptation of pretrained trans-\\nformers for abstractive summarization. arXiv preprint arXiv:1906.00138 (2019).\\n[94] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. 2023. A survey of knowledge enhanced\\npre-trained language models. IEEE Transactions on Knowledge and Data Engineering (2023).\\n[95] Li Hu, Anli Yan, Hongyang Yan, Jin Li, Teng Huang, Yingying Zhang, Changyu Dong, and Chunsheng Yang. 2023.\\nDefenses to Membership Inference Attacks: A Survey. Comput. Surveys 56, 4 (2023), 1–34.\\n[96] Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. 2023. Composite Backdoor Attacks Against\\nLarge Language Models. arXiv preprint arXiv:2310.07676 (2023).\\n[97] Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su.\\n2024. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. arXiv preprint\\narXiv:2403.01244 (2024).\\n[98] Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. 2022. Are Large Pre-Trained Language Models Leaking Your\\nPersonal Information? arXiv preprint arXiv:2205.12628 (2022).\\n[99] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2023.\\nLook before you leap: An exploratory study of uncertainty measurement for large language models. arXiv preprint\\narXiv:2307.10236 (2023).\\n[100] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing\\nHu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai\\nconversations. arXiv preprint arXiv:2312.06674 (2023).\\n[101] Huseyin A Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor Rühle, James Withers, and Robert Sim. 2021.\\nPrivacy analysis in language models via training data leakage report. ArXiv, abs/2101.05405 (2021).\\n[102] Huseyin A Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor Rühle, James Withers, and Robert Sim. 2021.\\nTraining data leakage analysis in language models. arXiv preprint arXiv:2101.05405 (2021).\\n[103] Abhyuday Jagannatha, Bhanu Pratap Singh Rawat, and Hong Yu. 2021. Membership inference attack susceptibility of\\nclinical language models. arXiv preprint arXiv:2104.08305 (2021).\\n[104] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum,\\nAniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned\\nlanguage models. arXiv preprint arXiv:2309.00614 (2023).\\n[105] Malin Jansson, Stefan Hrastinski, Stefan Stenbom, and Fredrik Enoksson. 2021. Online question and answer sessions:\\nHow students support their own and other students’ processes of inquiry in a text-based learning environment. The\\nInternet and Higher Education 51 (2021), 100817.\\n[106] Brindha Jeyaraman. 2023. Adversarial Attacks on LLMs: Safeguarding Language Models Against Manipulation.\\nhttps://www.linkedin.com/pulse/adversarial-attacks-llms-safeguarding-language-models-jeyaraman/\\n[107] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and\\nPascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.\\n[108] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating hallucination in\\nlarge language models via self-reflection. arXiv preprint arXiv:2310.06271 (2023).\\n[109] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. 2019. Memguard: Defending\\nagainst black-box membership inference attacks via adversarial examples. In Proceedings of the 2019 ACM SIGSAC\\nconference on computer and communications security. 259–274.\\n[110] Shuyu Jiang, Xingshu Chen, and Rui Tang. 2023. Prompt packer: Deceiving llms through compositional instruction\\nwith hidden attacks. arXiv preprint arXiv:2310.10077 (2023).\\n[111] Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. 2023. Lion: Adversarial Distillation of Closed-Source\\nLarge Language Model. arXiv preprint arXiv:2305.12870 (2023).\\n[112] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. TinyBERT:\\nDistilling BERT for Natural Language Understanding. arXiv preprint arXiv:1909.10351 (2019).\\n[113] Hongpeng Jin, Wenqi Wei, Xuyu Wang, Wenbin Zhang, and Yanzhao Wu. 2023. Rethinking learning rate tuning in\\nthe era of large language models. In 2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI).\\nIEEE, 112–121.\\n[114] Hongpeng Jin, Wenqi Wei, Xuyu Wang, Wenbin Zhang, and Yanzhao Wu. 2023. Rethinking Learning Rate Tuning\\nin the Era of Large Language Models. In 2023 IEEE 5th International Conference on Cognitive Machine Intelligence\\n(CogMI). 112–121. https://doi.org/10.1109/CogMI58952.2023.00025\\n[115] Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang, et al. 2024. Attackeval: How\\nto evaluate the effectiveness of jailbreak attacking on large language models. arXiv preprint arXiv:2401.09002 (2024).\\n[116] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023.\\nChallenges and applications of large language models. arXiv preprint arXiv:2307.10169 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:43\\n[117] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. 2023. Backdoor attacks for in-context\\nlearning with language models. arXiv preprint arXiv:2307.14692 (2023).\\n[118] Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022. Deduplicating training data mitigates privacy risks in language\\nmodels. In International Conference on Machine Learning. PMLR, 10697–10707.\\n[119] Masahiro Kaneko, Youmi Ma, Yuki Wata, and Naoaki Okazaki. 2024. Sampling-based Pseudo-Likelihood for Member-\\nship Inference Attacks. arXiv preprint arXiv:2404.11262 (2024).\\n[120] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. 2023. Exploiting\\nprogrammatic behavior of LLMs: Dual-use through standard security attacks. arXiv preprint arXiv:2302.05733 (2023).\\n[121] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. In Proceedings of naacL-HLT, Vol. 1. 2.\\n[122] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. 2024. Propile: Probing\\nprivacy leakage in large language models. Advances in Neural Information Processing Systems 36 (2024).\\n[123] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models\\nare zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199–22213.\\n[124] Nir Kshetri. 2023. Cybercrime and privacy threats of large language models. IT Professional 25, 3 (2023), 9–13.\\n[125] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. 2023. Certifying LLM safety\\nagainst adversarial prompting. arXiv preprint arXiv:2309.02705 (2023).\\n[126] Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight poisoning attacks on pre-trained models. arXiv preprint\\narXiv:2004.06660 (2020).\\n[127] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural\\narchitectures for named entity recognition. arXiv preprint arXiv:1603.01360 (2016).\\n[128] Raz Lapid, Ron Langberg, and Moshe Sipper. 2023. Open sesame! universal black box jailbreaking of large language\\nmodels. arXiv preprint arXiv:2309.01446 (2023).\\n[129] Isack Lee and Haebin Seong. 2024. Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak\\nVulnerabilities in AI Systems. arXiv preprint arXiv:2410.13334 (2024).\\n[130] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas\\nCarlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).\\n[131] Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, and Yangqiu Song. 2023.\\nPrivacy in large language models: Attacks, defenses and future directions. arXiv preprint arXiv:2310.10383 (2023).\\n[132] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. 2023. Multi-step jailbreaking privacy attacks on\\nChatGPT. arXiv preprint arXiv:2304.05197 (2023).\\n[133] Jiazhao Li, Yijin Yang, Zhuofeng Wu, VG Vydiswaran, and Chaowei Xiao. 2023. ChatGPT as an attack tool: Stealthy\\ntextual backdoor attack via blackbox generative model trigger. arXiv preprint arXiv:2304.14475 (2023).\\n[134] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. 2021. Backdoor attacks on pre-trained\\nmodels by layerwise weight poisoning. arXiv preprint arXiv:2108.13888 (2021).\\n[135] Linyi Li, Tao Xie, and Bo Li. 2023. SoK: Certified robustness for deep neural networks. In 2023 IEEE symposium on\\nsecurity and privacy (SP). IEEE, 1289–1310.\\n[136] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. 2021. Large language models can be strong\\ndifferentially private learners. arXiv preprint arXiv:2110.05679 (2021).\\n[137] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. 2023. Deepinception: Hypnotize\\nlarge language model to be jailbreaker. arXiv preprint arXiv:2311.03191 (2023).\\n[138] Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, and Yang Liu. 2023. Multi-target backdoor\\nattacks for code pre-trained models. arXiv preprint arXiv:2306.08350 (2023).\\n[139] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. 2021. Neural attention distillation: Erasing\\nbackdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930 (2021).\\n[140] Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, and Xiaochun Cao. 2024. Vl-\\ntrojan: Multimodal instruction backdoor attacks against autoregressive visual language models. arXiv preprint\\narXiv:2402.13851 (2024).\\n[141] Yi-Shan Lin, Wen-Chuan Lee, and Z Berkay Celik. 2021. What do you see? Evaluation of explainable artificial\\nintelligence (XAI) interpretability through neural backdoors. In Proceedings of the 27th ACM SIGKDD conference on\\nknowledge discovery & data mining. 1027–1035.\\n[142] Zachary C Lipton. 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is\\nboth important and slippery. Queue 16, 3 (2018), 31–57.\\n[143] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-Pruning: Defending against backdooring attacks on\\ndeep neural networks. In International symposium on research in attacks, intrusions, and defenses. Springer, 273–294.\\n[144] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023. Autodan: Generating stealthy jailbreak prompts on\\naligned large language models. arXiv preprint arXiv:2310.04451 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:44\\nDas, et al.\\n[145] Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. 2024. Automatic and universal prompt\\ninjection attacks against large language models. arXiv preprint arXiv:2403.04957 (2024).\\n[146] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu.\\n2023. Prompt Injection attack against LLM-integrated Applications. arXiv preprint arXiv:2306.05499 (2023).\\n[147] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu.\\n2023. Jailbreaking ChatGPT via prompt engineering: An empirical study. arXiv preprint arXiv:2305.13860 (2023).\\n[148] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang Gong. 2024. Formalizing and benchmarking\\nprompt injection attacks and defenses. In USENIX Security Symposium.\\n[149] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning\\nattack on neural networks. In 25th Annual Network And Distributed System Security Symposium (NDSS 2018). Internet\\nSoc.\\n[150] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\\nmoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint\\narXiv:1907.11692 (2019).\\n[151] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muham-\\nmad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language\\nModels’ Alignment. arXiv preprint arXiv:2308.05374 (2023).\\n[152] Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-Béguelin. 2023. Analyzing\\nleakage of personally identifiable information in language models. arXiv preprint arXiv:2302.00539 (2023).\\n[153] Scott Lundberg. 2017. A unified approach to interpreting model predictions. arXiv preprint arXiv:1705.07874 (2017).\\n[154] Junyu Luo, Cao Xiao, and Fenglong Ma. 2023. Zero-resource hallucination prevention for large language models.\\narXiv preprint arXiv:2309.02654 (2023).\\n[155] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic\\nforgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747 (2023).\\n[156] Malwarebytes. 2023.\\nWhat is iPhone jailbreaking.\\nAvailable Online: https://www.malwarebytes.com/iphone-\\njailbreaking. [Accessed on January 28, 2024].\\n[157] Courtney Mansfield, Amandalynne Paullada, and Kristen Howell. 2022. Behind the Mask: Demographic bias in name\\ndetection for PII masking. arXiv preprint arXiv:2205.04505 (2022).\\n[158] James Manyika and Sissie Hsiao. 2023. An overview of Bard: an early experiment with generative AI. AI. Google\\nStatic Documents 2 (2023).\\n[159] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela\\nJiang, and Lilian Weng. 2023. A holistic approach to undesired content detection in the real world. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence, Vol. 37. 15009–15018.\\n[160] Ariana Martino, Michael Iannelli, and Coleen Truong. 2023. Knowledge injection to counter large language model\\n(LLM) hallucination. In European Semantic Web Conference. Springer, 182–185.\\n[161] Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf, Mrinmaya Sachan, and Taylor Berg-\\nKirkpatrick. 2023. Membership Inference Attacks against Language Models via Neighbourhood Comparison. arXiv\\npreprint arXiv:2305.18462 (2023).\\n[162] Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner. 2023. Adversarial prompting for black box foundation\\nmodels. arXiv preprint arXiv:2302.04237 (2023).\\n[163] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connectionist networks: The sequential\\nlearning problem. In Psychology of learning and motivation. Vol. 24. Elsevier, 109–165.\\n[164] Leland McInnes, John Healy, and Steve Astels. 2017. HDBSCAN: Hierarchical density based clustering. J. Open Source\\nSoftw. 2, 11 (2017), 205.\\n[165] Yusuf Mehdi. 2023. Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the\\nweb. Available Online: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-\\nmicrosoft-bing-and-edge-your-copilot-for-the-web/ [Accessed on January 28, 2024].\\n[166] Meta-Llma. 2023. Llama-2-13b-hf. Available Online: https://huggingface.co/meta-llama/Llama-2-13b-hf [Accessed\\non January 28, 2024].\\n[167] Meta-Llma. 2023. Llama-2-7b. Available Online: https://huggingface.co/huggyllama/llama-7bf [Accessed on January\\n28, 2024].\\n[168] Meta-Llma. 2023. LLaMA-30b. Available Online: https://huggingface.co/huggyllama/llama-30b [Accessed on January\\n28, 2024].\\n[169] Microsoft. 2023. Bing AI Chatbot. https://www.bing.com/chat\\n[170] Silvia Milano, Joshua A McGrane, and Sabina Leonelli. 2023. Large language models challenge the future of higher\\neducation. Nature Machine Intelligence 5, 4 (2023), 333–334.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:45\\n[171] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng\\nGao. 2024. Large language models: A survey. arXiv preprint arXiv:2402.06196 (2024).\\n[172] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. 2022. Quantifying\\nprivacy risks of masked language models using membership inference attacks. arXiv preprint arXiv:2203.03929 (2022).\\n[173] Aayush Mittal. 2023. Prompt Hacking and Misuse of LLMs. \"[Accessed on January 28, 2024]\". https://www.unite.ai/\\nprompt-hacking-and-misuse-of-llm\\n[174] Domenic Molinaro. 2023. What Is Rooting? The Risks of Rooting Your Android Device. Available Online: https:\\n//www.avast.com/c-rooting-android [Accessed on January 28, 2024].\\n[175] Ervin Moore, Ahmed Imteaj, Shabnam Rezapour, and M. Hadi Amini. 2023. A Survey on Secure and Private Federated\\nLearning Using Blockchain: Theory and Application in Resource-Constrained Computing. IEEE Internet of Things\\nJournal 10, 24 (2023), 21942–21958. https://doi.org/10.1109/JIOT.2023.3313055\\n[176] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D Griffin. 2023. Use of LLMs for illicit purposes: Threats,\\nprevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833 (2023).\\n[177] Yuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu Abe, Shuntaro Yada, Shoko Wakamiya,\\nand Eiji Aramaki. 2020. KART: Privacy leakage framework of language models pre-trained with clinical records.\\narXiv preprint arXiv:2101.00036 (2020).\\n[178] Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, and Xuebing Zhou. 2024. PII-Compass: Guiding\\nLLM training data extraction prompts towards the target PII via grounding. arXiv preprint arXiv:2407.02943 (2024).\\n[179] Jomil˙e Nakutaviči¯ut˙e. 2023. Why root Android phones? Available Online: https://nordvpn.com/blog/why-you-\\nshouldnt-root-android [Accessed on January 28, 2024].\\n[180] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine learning with membership privacy using adversarial\\nregularization. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security. 634–646.\\n[181] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick\\nBarnes, and Ajmal Mian. 2023. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435\\n(2023).\\n[182] Seth Neel and Peter Chang. 2023. Privacy Issues in Large Language Models: A Survey. arXiv preprint arXiv:2312.06717\\n(2023).\\n[183] Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H Pham, Khoa D Doan, and Kok-Seng Wong. 2024. Backdoor\\nattacks and defenses in federated learning: Survey, challenges and future research directions. Engineering Applications\\nof Artificial Intelligence 127 (2024), 107166.\\n[184] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. 2022. Diffusion models\\nfor adversarial purification. arXiv preprint arXiv:2205.07460 (2022).\\n[185] OpenAI. 2023. API to Prevent Prompt Injection & Jailbreaks. Available Online: https://community.openai.com/t/api-\\nto-prevent-prompt-injection-jailbreaks/203514 [Accessed on January 28, 2024].\\n[186] OpenAI. 2023. ChatGPT. Available Online: https://chat.openai.com [Accessed on January 28, 2024].\\n[187] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems 35 (2022), 27730–27744.\\n[188] Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh,\\nand Rahul Gupta. 2023. Controlling the extraction of memorized data from large language models via prompt-tuning.\\narXiv preprint arXiv:2305.11759 (2023).\\n[189] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy risks of general-purpose language models. In 2020\\nIEEE Symposium on Security and Privacy (SP). IEEE, 1314–1331.\\n[190] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. 2023. On the Risk\\nof Misinformation Pollution with Large Language Models. arXiv preprint arXiv:2305.13661 (2023).\\n[191] Prachi (Nayyar) Pathak. 2023. How do you protect Machine Learning from attacks? Available Online: https://www.\\nlinkedin.com/advice/1/how-do-you-protect-machine-learning-from-attacks#data-poisoning-attack [Accessed on\\nJanuary 28, 2024].\\n[192] Pearlhawaii.com. 2023. WHAT IS JAILBREAKING, CRACKING, OR ROOTING A MOBILE DEVICE? Available Online:\\nhttps://pearlhawaii.com/what-is-jailbreaking-cracking-or-rooting-a-mobile-device. [Accessed on January 28, 2024].\\n[193] Rodrigo Pedro, Daniel Castro, Paulo Carreira, and Nuno Santos. 2023. From Prompt Injections to SQL Injection\\nAttacks: How Protected is Your LLM-Integrated Web Application? arXiv preprint arXiv:2308.01990 (2023).\\n[194] Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint\\narXiv:2211.09527 (2022).\\n[195] Roy H Perlis, Kristin Lunz Trujillo, Jon Green, Alauna Safarpour, James N Druckman, Mauricio Santillana, Katherine\\nOgnyanova, and David Lazer. 2023. Misinformation, trust, and use of ivermectin and hydroxychloroquine for\\nCOVID-19. In JAMA Health Forum, Vol. 4. American Medical Association, e233257–e233257.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:46\\nDas, et al.\\n[196] Miguel Piedrafita. 2022. Methodologies of Jailbreaking. Available Online: https://learnprompting.org/docs/prompt_\\nhacking/jailbreakin. [Accessed on January 28, 2024].\\n[197] Francesco Pierri, Luca Luceri, Nikhil Jindal, and Emilio Ferrara. 2023. Propaganda and misinformation on Facebook\\nand Twitter during the Russian invasion of Ukraine. In Proceedings of the 15th ACM web science conference 2023.\\n65–74.\\n[198] Promptbase. 2024. Promptbase. Available Online: https://promptbase.com/ [Accessed on August 23, 2024].\\n[199] Learn Prompting. 2023. Jailbreaking. Available Online: https://learnprompting.org/docs/prompt_hacking/jailbreaking\\n[Accessed on January 28, 2024].\\n[200] Learn Prompting. 2023. Your Guide to Generative AI. Available Online: https://learnprompting.org [Accessed on\\nJanuary 28, 2024].\\n[201] Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2020. ONION: A simple and effective\\ndefense against textual backdoor attacks. arXiv preprint arXiv:2011.10369 (2020).\\n[202] Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. 2021. Hidden\\nKiller: Invisible textual backdoor attacks with syntactic trigger. arXiv preprint arXiv:2105.12400 (2021).\\n[203] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. 2023. Visual adversarial examples\\njailbreak aligned large language models. In The Second Workshop on New Frontiers in Adversarial Machine Learning.\\n[204] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\\nunsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.\\n[205] Mohammad Raeini. 2023. Privacy-preserving large language models (PPLLMs). Available at SSRN 4512071 (2023).\\n[206] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\\nMachine Learning Research 21, 1 (2020), 5485–5551.\\n[207] Goutham Ramakrishnan and Aws Albarghouthi. 2022. Backdoors in neural models of source code. In 2022 26th\\nInternational Conference on Pattern Recognition (ICPR). IEEE, 2892–2899.\\n[208] Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, and Anyu Wang. 2024. Jail-\\nbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models. arXiv preprint\\narXiv:2406.09321 (2024).\\n[209] Aman Rangapur, Haoran Wang, and Kai Shu. 2023. Investigating online financial misinformation and its consequences:\\nA computational perspective. arXiv preprint arXiv:2309.12363 (2023).\\n[210] Roger Ratcliff. 1990. Connectionist models of recognition memory: constraints imposed by learning and forgetting\\nfunctions. Psychological review 97, 2 (1990), 285.\\n[211] Jingjing Ren, Ashwin Rao, Martina Lindorfer, Arnaud Legout, and David Choffnes. 2016. ReCon: Revealing and\\ncontrolling PII leaks in mobile network traffic. In Proceedings of the 14th Annual International Conference on Mobile\\nSystems, Applications, and Services. 361–374.\\n[212] Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. 2020. Adversarial attacks and defenses in deep learning. Engineering\\n6, 3 (2020), 346–360.\\n[213] Facebook Resrach. 2024. LAMA: LAnguage Model Analysis. Available Online: https://github.com/facebookresearch/\\nLAMA [Accessed on August 23, 2024].\\n[214] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why should I trust you?\" Explaining the predictions\\nof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data\\nmining. 1135–1144.\\n[215] Alexander Robey, Eric Wong, Hamed Hassani, and George J Pappas. 2023. SmoothLLM: Defending large language\\nmodels against jailbreaking attacks. arXiv preprint arXiv:2310.03684 (2023).\\n[216] Mirka Saarela and Vili Podgorelec. 2024. Recent Applications of Explainable AI (XAI): A Systematic Literature Review.\\nApplied Sciences 14, 19 (2024), 8884.\\n[217] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Hervé Jégou. 2019. White-box vs\\nblack-box: Bayes optimal strategies for membership inference. In International Conference on Machine Learning. PMLR,\\n5558–5567.\\n[218] Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. 2018. ML-leaks:\\nModel and data independent membership inference attacks and defenses on machine learning models. arXiv preprint\\narXiv:1806.01246 (2018).\\n[219] Sander Schulhoff. 2024. Instruction Defense. Available Online: https://learnprompting.org/docs/prompt_hacking/\\ndefensive_measures/instruction [Accessed on October 11, 2024].\\n[220] Sander Schulhoff. 2024. Instruction Defense. Available Online: https://learnprompting.org/docs/prompt_hacking/\\ndefensive_measures/post_prompting [Accessed on October 11, 2024].\\n[221] Sander Schulhoff. 2024. Sandwich Defense. Available Online: https://learnprompting.org/docs/prompt_hacking/\\ndefensive_measures/sandwich_defense [Accessed on October 11, 2024].\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:47\\n[222] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnera-\\nbilities in neural code completion. In 30th USENIX Security Symposium (USENIX Security 21). 1559–1575.\\n[223] SECWRITER. 2023. Prompt Hacking and Misuse of LLMs. Available Online: https://cyberdom.blog/2023/06/17/\\nunderstanding-prompt-injection-genai-risks [Accessed on January 28, 2024].\\n[224] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.\\n2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE\\ninternational conference on computer vision. 618–626.\\n[225] Jose Selvi. 2023. Exploring Prompt Injection Attacks. Available Online: https://research.nccgroup.com/2022/12/05/\\nexploring-prompt-injection-attacks/. [Accessed on January 28, 2024].\\n[226] Jesse Senechal, Eric Ekholm, Samaher Aljudaibi, Mary Strawderman, and Chris Parthemos. 2023. Balancing the\\nBenefits and Risks of Large Language AI Models in K12 Public Schools. (2023).\\n[227] Zeyang Sha, Xinlei He, Pascal Berrang, Mathias Humbert, and Yang Zhang. 2022. Fine-tuning is all you need to\\nmitigate backdoor attacks. arXiv preprint arXiv:2212.09067 (2022).\\n[228] Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T\\nBukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, et al. 2023. Loft: Local proxy fine-tuning for improving\\ntransferability of adversarial attacks against large language model. arXiv preprint arXiv:2310.04445 (2023).\\n[229] Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y Zhao. 2023. Prompt-specific poisoning\\nattacks on text-to-image generative models. arXiv preprint arXiv:2310.13828 (2023).\\n[230] Kun Shao, Junan Yang, Yang Ai, Hui Liu, and Yu Zhang. 2021. BDDR: An effective defense against textual backdoor\\nattacks. Computers & Security 110 (2021), 102433.\\n[231] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023. Plug and Pray: Exploiting off-the-shelf components of\\nMulti-Modal Models. arXiv preprint arXiv:2307.14539 (2023).\\n[232] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of\\nvulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844 (2023).\\n[233] Virat Shejwalkar, Huseyin A Inan, Amir Houmansadr, and Robert Sim. 2021. Membership inference attacks against\\nNLP classification models. In NeurIPS 2021 Workshop Privacy in Machine Learning.\\n[234] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. \"Do Anything Now\": Characterizing\\nand Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. arXiv preprint arXiv:2308.03825 (2023).\\n[235] Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. 2023. BadGPT: Exploring Security Vulnerabilities of ChatGPT via\\nBackdoor Attacks to InstructGPT. arXiv preprint arXiv:2304.12298 (2023).\\n[236] Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. 2024. Optimization-\\nbased Prompt Injection Attack to LLM-as-a-Judge. arXiv preprint arXiv:2403.17710 (2024).\\n[237] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke\\nZettlemoyer. 2023. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 (2023).\\n[238] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting\\nknowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 (2020).\\n[239] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against\\nmachine learning models. In 2017 IEEE symposium on security and privacy (SP). IEEE, 3–18.\\n[240] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. 2023. On the exploitability\\nof instruction tuning. Advances in Neural Information Processing Systems 36 (2023), 61836–61856.\\n[241] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher\\nPotts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the\\n2013 conference on empirical methods in natural language processing. 1631–1642.\\n[242] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Khyathi Chandu,\\nJennifer Dumas, Li Lucy, Xinxi Lyu, et al. 2023. Dolma: An Open Corpus of 3 Trillion Tokens for Language Model\\nPretraining Research. Allen Institute for AI, Tech. Rep (2023).\\n[243] Congzheng Song and Ananth Raghunathan. 2020. Information leakage in embedding models. In Proceedings of the\\n2020 ACM SIGSAC conference on computer and communications security. 377–390.\\n[244] Congzheng Song and Vitaly Shmatikov. 2019. Auditing data provenance in text-generation models. In Proceedings of\\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 196–206.\\n[245] Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers,\\nAlexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach\\nto prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364 (2022).\\n[246] Dirk HR Spennemann. 2023. Exploring Ethical Boundaries: Can ChatGPT Be Prompted to Give Advice on How to\\nCheat in University Assignments? (2023).\\n[247] Ningxin Su, Chenghao Hu, Baochun Li, and Bo Li. 2023. TITANIC: Towards Production Federated Learning with\\nLarge Language Models. Available Online: https://iqua.ece.toronto.edu/papers/ningxinsu-infocom24.pdf [Accessed\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:48\\nDas, et al.\\non February 6, 2024].\\n[248] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan\\nZhang, Xiner Li, et al. 2024. TrustLLM: Trustworthiness in Large Language Models. arXiv preprint arXiv:2401.05561\\n(2024).\\n[249] Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, and\\nSusmit Jha. 2023. TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision. 165–175.\\n[250] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition.\\n2818–2826.\\n[251] Annalisa Szymanski, Noah Ziems, Heather A Eicher-Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald A Metoyer. 2024.\\nLimitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks. arXiv preprint\\narXiv:2410.20266 (2024).\\n[252] Yonglin Tian, Xuan Li, Hui Zhang, Chen Zhao, Bai Li, Xiao Wang, and Fei-Yue Wang. 2023. VistaGPT: Generative\\nparallel transformers for vehicles with intelligent systems for transport automation. IEEE Transactions on Intelligent\\nVehicles (2023).\\n[253] Yonglin Tian, Jiangong Wang, Yutong Wang, Chen Zhao, Fei Yao, and Xiao Wang. 2022. Federated vehicular\\ntransformers and their federations: Privacy-preserving computing and cooperation for autonomous driving. IEEE\\nTransactions on Intelligent Vehicles (2022).\\n[254] Meng Tong, Kejiang Chen, Yuang Qi, Jie Zhang, Weiming Zhang, and Nenghai Yu. 2023. Privinfer: Privacy-preserving\\ninference for black-box large language model. arXiv preprint arXiv:2310.12214 (2023).\\n[255] Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne\\nTheodorakopoulos, Tanja Tornede, Henning Wachsmuth, et al. 2023. AutoML in the Age of Large Language Models:\\nCurrent Challenges, Future Opportunities and Risks. arXiv preprint arXiv:2306.08107 (2023).\\n[256] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. 2019. Demystifying membership inference\\nattacks in machine learning as a service. IEEE Transactions on Services Computing 14, 6 (2019), 2073–2089.\\n[257] Thomas Vakili, Anastasios Lamproudis, Aron Henriksson, and Hercules Dalianis. 2022. Downstream task performance\\nof BERT models pre-trained using automatically de-identified clinical data. In Proceedings of the Thirteenth Language\\nResources and Evaluation Conference. 4245–4252.\\n[258] Apostol Vassilev, Alina Oprea, Alie Fordyce, and Hyrum Andersen. 2024. Adversarial Machine Learning: A Taxonomy\\nand Terminology of Attacks and Mitigations. NIST Trustworthy and Responsible AI NIST AI 100-2e2023 (2024).\\n[259] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\\n[260] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. Matching networks for one shot\\nlearning. Advances in neural information processing systems 29 (2016).\\n[261] Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection regulation (GDPR). A Practical Guide, 1st\\nEd., Cham: Springer International Publishing 10, 3152676 (2017), 10–5555.\\n[262] Eric Wallace, Tony Z Zhao, Shi Feng, and Sameer Singh. 2020. Concealed data poisoning attacks on NLP models.\\narXiv preprint arXiv:2010.12563 (2020).\\n[263] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023. Poisoning Language Models During Instruction\\nTuning. arXiv preprint arXiv:2305.00944 (2023).\\n[264] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019.\\nIdentifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP).\\nIEEE, 707–723.\\n[265] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye,\\nXiubo Geng, et al. 2023. On the robustness of ChatGPT: An adversarial and out-of-distribution perspective. arXiv\\npreprint arXiv:2302.12095 (2023).\\n[266] Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. 2023. Adversarial Demonstration\\nAttacks on Large Language Models. arXiv preprint arXiv:2305.14950 (2023).\\n[267] Shang Wang, Tianqing Zhu, Bo Liu, Ding Ming, Xu Guo, Dayong Ye, and Wanlei Zhou. 2024. Unique Security and\\nPrivacy Threats of Large Language Model: A Comprehensive Survey. arXiv preprint arXiv:2406.07973 (2024).\\n[268] Wei Wang, Vincent W Zheng, Han Yu, and Chunyan Miao. 2019. A survey of zero-shot learning: Settings, methods,\\nand applications. ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 1–37.\\n[269] Yequan Wang, Jiawen Deng, Aixin Sun, and Xuying Meng. 2022. Perplexity from plm is unreliable for evaluating text\\nquality. arXiv preprint arXiv:2210.05892 (2022).\\n[270] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-\\ndecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:49\\n[271] Yijue Wang, Nuo Xu, Shaoyi Huang, Kaleel Mahmood, Dan Guo, Caiwen Ding, Wujie Wen, and Sanguthevar\\nRajasekaran. 2022. Analyzing and Defending against Membership Inference Attacks in Natural Language Processing\\nClassification. In 2022 IEEE International Conference on Big Data (Big Data). IEEE, 5823–5832.\\n[272] Yifei Wang, Dizhan Xue, Shengjie Zhang, and Shengsheng Qian. 2024. BadAgent: Inserting and Activating Backdoor\\nAttacks in LLM Agents. arXiv preprint arXiv:2406.03007 (2024).\\n[273] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing from a few examples: A survey on\\nfew-shot learning. ACM computing Surveys 53, 3 (2020), 1–34.\\n[274] Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, and Enze Wang. 2023. Self-deception: Reverse\\npenetrating the semantic firewall of large language models. arXiv preprint arXiv:2308.11521 (2023).\\n[275] Maurice Weber, Xiaojun Xu, Bojan Karlaš, Ce Zhang, and Bo Li. 2023. Rab: Provable robustness against backdoor\\nattacks. In 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 1311–1328.\\n[276] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does LLM safety training fail? arXiv\\npreprint arXiv:2307.02483 (2023).\\n[277] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\\nDenny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682\\n(2022).\\n[278] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing\\nSystems 35 (2022), 24824–24837.\\n[279] Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Mehmet Emre Gursoy, Stacey Truex, and Yanzhao Wu. 2020.\\nA framework for evaluating client privacy leakages in federated learning. In Computer Security–ESORICS 2020:\\n25th European Symposium on Research in Computer Security, ESORICS 2020, Guildford, UK, September 14–18, 2020,\\nProceedings, Part I 25. Springer, 545–566.\\n[280] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia\\nGlaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv\\npreprint arXiv:2112.04359 (2021).\\n[281] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra\\nCheng, Borja Balle, Atoosa Kasirzadeh, et al. 2022. Taxonomy of risks posed by language models. In Proceedings of\\nthe 2022 ACM Conference on Fairness, Accountability, and Transparency. 214–229.\\n[282] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-\\nSmith, and Douglas C Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with ChatGPT. arXiv\\npreprint arXiv:2302.11382 (2023).\\n[283] Chenxi Whitehouse, Tillman Weyde, Pranava Madhyastha, and Nikos Komninos. 2022. Evaluation of fake news\\ndetection with knowledge-enhanced language models. In Proceedings of the International AAAI Conference on Web\\nand Social Media, Vol. 16. 1425–1429.\\n[284] Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni,\\nand Daniel Preotiuc-Pietro. 2023. Overcoming catastrophic forgetting in massively multilingual continual learning.\\narXiv preprint arXiv:2305.16252 (2023).\\n[285] Fangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, and Xing Xie. 2023.\\nDefending ChatGPT against Jailbreak Attack via Self-Reminder. (2023).\\n[286] Ruihan Wu, Xiangyu Chen, Chuan Guo, and Kilian Q Weinberger. 2023. Learning to Invert: Simple adaptive attacks\\nfor gradient inversion in federated learning. In Uncertainty in Artificial Intelligence. PMLR, 2293–2303.\\n[287] Xiaodong Wu, Ran Duan, and Jianbing Ni. 2023. Unveiling security, privacy, and ethical concerns of ChatGPT. Journal\\nof Information and Intelligence (2023).\\n[288] Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, and Lichao Sun. 2024. Can Large Language Models\\nAutomatically Jailbreak GPT-4V? arXiv preprint arXiv:2407.16686 (2024).\\n[289] Yanzhao Wu and Ling Liu. 2023. Selecting and composing learning rate policies for deep neural networks. ACM\\nTransactions on Intelligent Systems and Technology 14, 2 (2023), 1–25.\\n[290] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin,\\nEnyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. arXiv preprint\\narXiv:2309.07864 (2023).\\n[291] Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, and Ting Wang. 2023.\\nDefending pre-trained language models as few-shot learners against backdoor attacks. arXiv preprint arXiv:2309.13256\\n(2023).\\n[292] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. 2024. Badchain:\\nBackdoor chain-of-thought prompting for large language models. arXiv preprint arXiv:2401.12242 (2024).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n111:50\\nDas, et al.\\n[293] Zhen Xiang, Zidi Xiong, and Bo Li. 2024. CBD: A certified backdoor detector based on local dominant probability.\\nAdvances in Neural Information Processing Systems 36 (2024).\\n[294] Yuan Xin, Zheng Li, Ning Yu, Michael Backes, and Yang Zhang. 2022. Membership Leakage in Pre-trained Language\\nModels. (2022).\\n[295] Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu. 2022. Exploring the universal vulnerability of\\nprompt-based learning paradigm. arXiv preprint arXiv:2204.05239 (2022).\\n[296] Yuancheng Xu, Jiarui Yao, Manli Shu, Yanchao Sun, Zichu Wu, Ning Yu, Tom Goldstein, and Furong Huang. 2024.\\nShadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models. arXiv preprint arXiv:2402.06659\\n(2024).\\n[297] Anli Yan, Teng Huang, Lishan Ke, Xiaozhang Liu, Qi Chen, and Changyu Dong. 2023. Explanation leaks: Explanation-\\nguided model extraction attacks. Information Sciences 632 (2023), 269–284.\\n[298] Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, and Xiangyu Zhang. 2024. ParaFuzz:\\nAn interpretability-driven technique for detecting poisoned samples in nlp. Advances in Neural Information Processing\\nSystems 36 (2024).\\n[299] Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, and Yuan Hong. 2024. An LLM-Assisted\\nEasy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong\\nDetection. arXiv preprint arXiv:2406.06822 (2024).\\n[300] Haomiao Yang, Kunlan Xiang, Hongwei Li, and Rongxing Lu. 2023. A Comprehensive Overview of Backdoor Attacks\\nin Large Language Models within Communication Networks. arXiv preprint arXiv:2308.14367 (2023).\\n[301] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2021. Be careful about poisoned word\\nembeddings: Exploring the vulnerability of the embedding layers in NLP models. arXiv preprint arXiv:2103.15543\\n(2021).\\n[302] Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun. 2021. RAP: Robustness-aware perturbations for defending\\nagainst backdoor attacks on NLP models. arXiv preprint arXiv:2110.07831 (2021).\\n[303] Yang Yang. 2022. Holistic risk assessment of inference attacks in machine learning. arXiv preprint arXiv:2212.10628\\n(2022).\\n[304] Zhou Yang, Zhensu Sun, Terry Zhuo Yue, Premkumar Devanbu, and David Lo. 2024. Robustness, security, privacy,\\nexplainability, efficiency, and usability of large language models for code. arXiv preprint arXiv:2403.07506 (2024).\\n[305] Zhou Yang, Bowen Xu, Jie M Zhang, Hong Jin Kang, Jieke Shi, Junda He, and David Lo. 2024. Stealthy backdoor\\nattack for code models. IEEE Transactions on Software Engineering (2024).\\n[306] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. A survey on large language model\\n(llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing (2024), 100211.\\n[307] Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri. 2022. Enhanced\\nmembership inference attacks against machine learning models. In Proceedings of the 2022 ACM SIGSAC Conference\\non Computer and Communications Security. 3093–3106.\\n[308] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy risk in machine learning: Analyzing\\nthe connection to overfitting. In 2018 IEEE 31st computer security foundations symposium (CSF). IEEE, 268–282.\\n[309] Wencong You, Zayd Hammoudeh, and Daniel Lowd. 2023. Large language models are better adversaries: Exploring\\ngenerative clean-label backdoor attacks against text classifiers. arXiv preprint arXiv:2310.18603 (2023).\\n[310] Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. Gptfuzzer: Red teaming large language models with auto-generated\\njailbreak prompts. arXiv preprint arXiv:2309.10253 (2023).\\n[311] Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, and Ning Zhang. 2024. Don’t Listen To\\nMe: Understanding and Exploring Jailbreak Prompts of Large Language Models. arXiv preprint arXiv:2403.17336\\n(2024).\\n[312] Santiago Zanella-Béguelin, Lukas Wutschitz, Shruti Tople, Victor Rühle, Andrew Paverd, Olga Ohrimenko, Boris Köpf,\\nand Marc Brockschmidt. 2020. Analyzing information leakage of updates to natural language models. In Proceedings\\nof the 2020 ACM SIGSAC conference on computer and communications security. 363–375.\\n[313] Shudong Zhang, Haichang Gao, and Qingxun Rao. 2021. Defense against adversarial attacks by reconstructing images.\\nIEEE Transactions on Image Processing 30 (2021), 6117–6129.\\n[314] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona\\nDiab, Xian Li, Xi Victoria Lin, et al. 2022. OPT: Open pre-trained transformer language models. arXiv preprint\\narXiv:2205.01068 (2022).\\n[315] Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial attacks on deep-learning\\nmodels in natural language processing: A survey. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 3\\n(2020), 1–41.\\n[316] Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. 2021. Trojaning language models for fun and profit. In\\n2021 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 179–197.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\nSecurity and Privacy Challenges of Large Language Models: A Survey\\n111:51\\n[317] Yiming Zhang and Daphne Ippolito. 2023. Prompts should not be seen as secrets: Systematically measuring prompt\\nextraction attack success. arXiv preprint arXiv:2307.06865 (2023).\\n[318] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\\nDolan. 2019. DIALOGPT: Large-scale generative pre-training for conversational response generation. arXiv preprint\\narXiv:1911.00536 (2019).\\n[319] Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, and Xu Sun. 2022. Fine-Mixing: Mitigating backdoors\\nin fine-tuned language models. arXiv preprint arXiv:2210.09545 (2022).\\n[320] Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023. Defending large language models against jailbreaking\\nattacks through goal prioritization. arXiv preprint arXiv:2311.09096 (2023).\\n[321] Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and Jie Fu. 2023. Prompt as Triggers for Backdoor Attack:\\nExamining the Vulnerability in Language Models. arXiv preprint arXiv:2305.01219 (2023).\\n[322] Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K Qiu, and Lili Qiu. 2024. Retrieval Augmented Generation\\n(RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely. arXiv\\npreprint arXiv:2409.14924 (2024).\\n[323] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[324] Xuejun Zhao, Wencan Zhang, Xiaokui Xiao, and Brian Lim. 2021. Exploiting explanations for model inversion attacks.\\nIn Proceedings of the IEEE/CVF international conference on computer vision. 682–692.\\n[325] Abdel-Aty Zheng, Wang Wang, and Ding. 2023. Chat-GPT is on the Horizon: Could a Large Language Model be\\nSuitable for Intelligent Traffic Safety Research and Applications? https://arxiv.org/ftp/arxiv/papers/2303/2303.05382.pdf\\n(2023).\\n[326] Fei Zheng. 2023. Input Reconstruction Attack against Vertical Federated Large Language Models. arXiv preprint\\narXiv:2311.07585 (2023).\\n[327] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\\nDacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with MT-bench and chatbot arena. Advances in Neural\\nInformation Processing Systems 36 (2023), 46595–46623.\\n[328] Ou Zheng, Mohamed Abdel-Aty, Dongdong Wang, Chenzhu Wang, and Shengxuan Ding. 2023. TrafficSafetyGPT:\\nTuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety. arXiv preprint\\narXiv:2307.15311 (2023).\\n[329] Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang,\\nShihan Dou, Zhiheng Xi, et al. 2024. EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models.\\narXiv preprint arXiv:2403.12171 (2024).\\n[330] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022.\\nLarge language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 (2022).\\n[331] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients. Advances in neural information processing\\nsystems 32 (2019).\\n[332] Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao Zhao, Zheng Zhang, Qingyang\\nMao, Shijin Wang, et al. 2023. Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective.\\narXiv preprint arXiv:2306.10512 (2023).\\n[333] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\\nGeoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019).\\n[334] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023. Universal and\\ntransferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 (2023).\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2024.\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n=== Generative AI porn \"communities\" ===\\nPopular sites like civitai.com allow users to create, upload, and download fine-tuned versions of open source models of SDXL, Flux, and that are specifically designed for generating various pornographic scenes or effects.   \\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Fran'), Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.\\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nSince its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundatio')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.\\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nSince inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.\\nT'),\n",
       " Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Francisco filed a landmark lawsuit to shut down \"undress\" apps that allow users to generate non-consensual AI nude images, citing violations of state laws. The case aligns with California\\'s recent legislation—SB 926, SB 942, and SB 981—championed by Senators Aisha Wahab a')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
